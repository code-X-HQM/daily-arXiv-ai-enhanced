<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 4]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Do Not Merge My Model! Safeguarding Open-Source LLMs Against Unauthorized Model Merging](https://arxiv.org/abs/2511.10712)
*Qinfeng Li,Miao Pan,Jintao Chen,Fu Teng,Zhiqiang Shen,Ge Su,Hao Peng,Xuhong Zhang*

Main category: cs.CR

TL;DR: 模型合并技术可高效扩展大语言模型，但也带来模型合并窃取的新威胁。现有防御机制无法同时满足三个关键保护属性：主动防范未授权合并、与通用开源设置兼容、高安全性且性能损失可忽略。MergeBarrier作为一种即插即用防御方案，通过破坏受保护模型与其同源模型间的线性连接性，消除模型合并所需的低损失路径。实验证明MergeBarrier能有效阻止模型合并窃取且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: 针对模型合并技术引发的安全威胁——恶意用户通过未授权合并窃取专家模型，现有防御方法无法同时满足主动防护、开源兼容性和低性能损耗三大需求。

Method: 提出MergeBarrier防御机制，其核心设计是破坏受保护模型与其同源副本之间的线性模式连接性（LMC），使模型合并过程无法获得所需的低损失路径。该方法采用即插即用的方式运作。

Result: 大量实验表明，MergeBarrier能有效阻止模型合并窃取行为，同时仅带来可忽略的准确率损失（原始模型准确率每下降1%，防御成功率提升约13%）。

Conclusion: MergeBarrier首次解决了模型合并窃取问题，在保持模型性能的同时满足三大防护特性，为开源模型安全提供了新保障。

Abstract: Model merging has emerged as an efficient technique for expanding large language models (LLMs) by integrating specialized expert models. However, it also introduces a new threat: model merging stealing, where free-riders exploit models through unauthorized model merging. Unfortunately, existing defense mechanisms fail to provide effective protection. Specifically, we identify three critical protection properties that existing methods fail to simultaneously satisfy: (1) proactively preventing unauthorized merging; (2) ensuring compatibility with general open-source settings; (3) achieving high security with negligible performance loss. To address the above issues, we propose MergeBarrier, a plug-and-play defense that proactively prevents unauthorized merging. The core design of MergeBarrier is to disrupt the Linear Mode Connectivity (LMC) between the protected model and its homologous counterparts, thereby eliminating the low-loss path required for effective model merging. Extensive experiments show that MergeBarrier effectively prevents model merging stealing with negligible accuracy loss.

</details>


### [2] [BadThink: Triggered Overthinking Attacks on Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2511.10714)
*Shuaitong Liu,Renjue Li,Lijia Yu,Lijun Zhang,Zhiming Liu,Gaojie Jin*

Main category: cs.CR

TL;DR: 提出了首个针对CoT增强大型语言模型的隐蔽后门攻击，引发过度思考行为。


<details>
  <summary>Details</summary>
Motivation: CoT提示的进步提升了模型推理能力，但同时在计算效率方面引入了新的攻击面。本研究旨在设计一种隐蔽的后门攻击，诱导模型产生冗余推理轨迹以增加计算负担。

Method: 通过基于中毒的微调策略实现攻击，采用基于LLM的迭代优化过程生成高度自然的中毒数据。

Result: 在多个先进模型和推理任务中，攻击使推理轨迹显著延长（如MATH-500数据集增加17倍以上），同时保持隐蔽性和输出一致性。

Conclusion: 揭示了CoT系统存在推理效率被隐蔽操控的新攻击面，证明了针对此类系统的新型高级攻击的可行性。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially improved the reasoning capabilities of large language models (LLMs), but have also introduced their computational efficiency as a new attack surface. In this paper, we propose BadThink, the first backdoor attack designed to deliberately induce "overthinking" behavior in CoT-enabled LLMs while ensuring stealth. When activated by carefully crafted trigger prompts, BadThink manipulates the model to generate inflated reasoning traces - producing unnecessarily redundant thought processes while preserving the consistency of final outputs. This subtle attack vector creates a covert form of performance degradation that significantly increases computational costs and inference time while remaining difficult to detect through conventional output evaluation methods. We implement this attack through a sophisticated poisoning-based fine-tuning strategy, employing a novel LLM-based iterative optimization process to embed the behavior by generating highly naturalistic poisoned data. Our experiments on multiple state-of-the-art models and reasoning tasks show that BadThink consistently increases reasoning trace lengths - achieving an over 17x increase on the MATH-500 dataset - while remaining stealthy and robust. This work reveals a critical, previously unexplored vulnerability where reasoning efficiency can be covertly manipulated, demonstrating a new class of sophisticated attacks against CoT-enabled systems.

</details>


### [3] [PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization](https://arxiv.org/abs/2511.10720)
*Runpeng Geng,Yanting Wang,Chenlong Yin,Minhao Cheng,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: 该研究针对大型语言模型在长上下文易受提示注入攻击问题，提出PISanitizer方法，通过定位和净化关键注入令牌有效防御攻击。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入防御方案针对短上下文设计，在长上下文场景中效果有限，因注入指令仅占长文本极小比例难以检测，亟需有效防御机制。

Method: 利用LLM遵循指令特性及注意力机制原理，首先诱导模型执行任意上下文指令，后高注意力驱动指令行为的关键令牌进行净化处理。

Result: 实验表明PISanitizer能成功防御提示注入、保持模型效用、优于现有方案，高效且抗优化攻击与强自适应攻击。

Conclusion: 提出首个长上下文提示注入防御方案，以注意力分析为核心形成攻击者两难困境：指令强制力越强被净化概率越高，为实用化防御开辟新路径。

Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.

</details>


### [4] [HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control](https://arxiv.org/abs/2511.11549)
*Shreya Meel,Sennur Ulukus*

Main category: cs.CR

TL;DR: "HetDAPAC"是在分布式基于属性的私有访问控制（DAPAC）框架基础上提出的改进方案。作者们针对用户属性的异构隐私要求，通过将部分非敏感属性的验证集中化处理，在牺牲少量非敏感属性的同时对DAPAC进行优化。HetDAPAC以非平衡服务器下载负载为代价将系统的传输率从原来的每属性${1}/{2K}$提升至${1}/{K+1}$；随后另一版本方案实现了各服务器下载负载平衡下，系统的传输率提高到${(D+1)}/{2KD}$。


<details>
  <summary>Details</summary>
Motivation: 现有工作DAPAC对于所有用户属性均采取一视同仁的隐私保护强度策略，但实际情况是一些用户属性并不敏感，因此现有DAPAC方案传输效率较低。针对此问题，本文提出一种分层结构框架“HetDAPAC”，旨在对用户敏感属性仍采用分布式的DAPAC方案以保证隐私安全性；对非敏感性属性则通过中央服务器集中进行验证而不需再隐私保护，这样提高整个系统的传输率。

Method: HetDAPAC框架将涉及N个属性的系统中存在D个敏感属性与N-D个非敏感属性分开处理—对于敏感属性继续使用分布式、多服务器架构各保护其中一项属性；对于非敏感属性的验证，则转移到一个中央服务器集中完成以减少分布式通信开销。该框架下具体实现两个方案：第一种方案采用非平衡设计，即将敏感数据的验证任务分布于D个服务器并实现传输效率提升为${1}/{K+1}$。为了解决不同服务器间的负载均衡问题，作者随后提出第二版本的框架方案实现了各服务器下载负载平衡下传输率提高至${(D+1)}/{2KD}$。

Result: 结果表明：HetDAPAC首次提出的非平衡方案能够实现相比DAPAC原方案更优的传输效率—从每属性${1}/{2K}$提升至${1}/{K+1}$；同时该代价是牺牲非敏感属性的隐私性。后续改进方案则成功在负载均衡的情况下把系统的整体传输率提高到${(D+1)}/{2KD}$。

Conclusion: 本文提出的HetDAPAC框架能够根据用户属性实际隐私保护要求的异质性对不同的属性施加不同强度的隐私保护策略—将非敏感属性集中验证而不降低传输效率。实验表明该框架能够成功在分布式隐私保护场景中提高系统的有效传输率；同时该框架也能够支持服务器间负载均衡的设计需求。

Abstract: Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\frac{1}{2K}$ to $\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\frac{D+1}{2KD}$.

</details>
