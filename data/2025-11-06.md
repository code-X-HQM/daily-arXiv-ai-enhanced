<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 9]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [PrivyWave: Privacy-Aware Wireless Sensing of Heartbeat](https://arxiv.org/abs/2511.02993)
*Yixuan Gao,Tanvir Ahmed,Zekun Chang,Thijs Roumen,Rajalakshmi Nandakumar*

Main category: cs.CR

TL;DR: 本文提出了一种基于密钥的物理混淆系统PrivyWave，通过生成密码频率控制的假心跳信号，保护无线心跳感测的隐私。系统允许授权设备准确测量，同时阻止未经授权设备获取真实心跳数据。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私解决方案要么阻止所有感测系统（牺牲所有功能），要么在数据收集后操作（无法实现选择性访问控制）。需要一种能够在物理层提供密码学混淆保障的系统，同时支持授权设备获取数据并阻止未授权设备。

Method: 生成由密码学密钥控制的虚假心跳信号覆盖真实心跳频谱。未经授权设备接收混合信号无法区分真假；授权设备用密钥滤除假信号，从而保留准确测量。

Result: 在13名参与者测试中：对于毫米波雷达，未授权设备测量误差21.3 BPM（平均绝对值），授权设备则为5.8 BPM；对于声学感测，未授权误差42.0 BPM，授权设备9.7 BPM。该系统可在不同距离（30-150厘米）、视角（120°视场角）及复杂室内环境下工作，无需按模态定制。

Conclusion: PrivyWave实现了物理层混淆效果，提供选择性的隐私保护：允许授权系统可靠监测心率，同时使未授权系统无法分辨真实心跳。这为家庭健康监测中的隐私控制提供了实用解决方案。

Abstract: Wireless sensing technologies can now detect heartbeats using radio frequency
and acoustic signals, raising significant privacy concerns. Existing privacy
solutions either protect from all sensing systems indiscriminately preventing
any utility or operate post-data collection, failing to enable selective access
where authorized devices can monitor while unauthorized ones cannot. We present
a key-based physical obfuscation system, PrivyWave, that addresses this
challenge by generating controlled decoy heartbeat signals at
cryptographically-determined frequencies. Unauthorized sensors receive a
mixture of real and decoy signals that are indistinguishable without the secret
key, while authorized sensors use the key to filter out decoys and recover
accurate measurements. Our evaluation with 13 participants demonstrates
effective protection across both sensing modalities: for mmWave radar,
unauthorized sensors show 21.3 BPM mean absolute error while authorized sensors
maintain a much smaller 5.8 BPM; for acoustic sensing, unauthorized error
increases to 42.0 BPM while authorized sensors achieve 9.7 BPM. The system
operates across multiple sensing modalities without per-modality customization
and provides cryptographic obfuscation guarantees. Performance benchmarks show
robust protection across different distances (30-150 cm), orientations
(120{\deg} field of view), and diverse indoor environments, establishing
physical-layer obfuscation as a viable approach for selective privacy in
pervasive health monitoring.

</details>


### [2] [Bayesian Advantage of Re-Identification Attack in the Shuffle Model](https://arxiv.org/abs/2511.03213)
*Pengcheng Su,Haibo Cheng,Ping Wang*

Main category: cs.CR

TL;DR: 本文首次系统研究了在shuffle模型下，贝叶斯方法在重识别用户消息中的优势。通过定义贝叶斯优势度量，推导了其精确表达式与渐近性质，并与总变差距离建立紧致互界。此外，扩展分析至shuffle差分隐私场景，给出了贝叶斯攻击成功概率的上界。


<details>
  <summary>Details</summary>
Motivation: 当前广泛采用的shuffle模型在隐私保护中的安全性评估尚未系统化。现有研究缺乏对贝叶斯重识别攻击优势的定量刻画，尤其缺乏对攻击成功概率的理论边界分析。

Method: 1) 构建基础场景：混合来自分布P和Q的样本后随机重排；2) 定义贝叶斯最优攻击成功概率β_n(P,Q)及两类优势指标；3) 推导β_n的解析表达式与渐近特征；4) 建立贝叶斯优势与总变差距离的互界关系；5) 扩展至shuffle差分隐私框架，推导攻击成功概率上界。

Result: 1) 获得β_n在典型场景下的闭式解与渐近行为；2) 证明加性贝叶斯优势与总变差距离的近似紧致互界；3) 在ε-本地差分隐私的shuffle模型中，证明重识别概率上界为e^ε/n（突破经典1/n边界）。

Conclusion: 量化揭示shuffle模型的贝叶斯攻击脆弱性，确立总变差距离作为其安全性的新测度。关键结论：即使采用本地差分隐私的shuffle机制，重识别概率仍可能逼近e^ε/n而非1/n

Abstract: The shuffle model, which anonymizes data by randomly permuting user messages,
has been widely adopted in both cryptography and differential privacy. In this
work, we present the first systematic study of the Bayesian advantage in
re-identifying a user's message under the shuffle model. We begin with a basic
setting: one sample is drawn from a distribution $P$, and $n - 1$ samples are
drawn from a distribution $Q$, after which all $n$ samples are randomly
shuffled. We define $\beta_n(P, Q)$ as the success probability of a
Bayes-optimal adversary in identifying the sample from $P$, and define the
additive and multiplicative Bayesian advantages as $\mathsf{Adv}_n^{+}(P, Q) =
\beta_n(P,Q) - \frac{1}{n}$ and $\mathsf{Adv}_n^{\times}(P, Q) = n \cdot
\beta_n(P,Q)$, respectively.
  We derive exact analytical expressions and asymptotic characterizations of
$\beta_n(P, Q)$, along with evaluations in several representative scenarios.
Furthermore, we establish (nearly) tight mutual bounds between the additive
Bayesian advantage and the total variation distance.
  Finally, we extend our analysis beyond the basic setting and present, for the
first time, an upper bound on the success probability of Bayesian attacks in
shuffle differential privacy. Specifically, when the outputs of $n$ users--each
processed through an $\varepsilon$-differentially private local randomizer--are
shuffled, the probability that an attacker successfully re-identifies any
target user's message is at most $e^{\varepsilon}/n$.

</details>


### [3] [Death by a Thousand Prompts: Open Model Vulnerability Analysis](https://arxiv.org/abs/2511.03247)
*Amy Chang,Nicholas Conley,Harish Santhanalakshmi Ganesan,Adam Swanda*

Main category: cs.CR

TL;DR: 研究团队对8个开源权重大语言模型（LLMs）的安全性进行了测试，发现所有模型在多轮攻击中均存在严重漏洞，多轮攻击成功率比单轮攻击提高2到10倍。不同模型的安全表现受其对齐策略影响，能力导向型模型（如Llama 3.3、Qwen 3）漏洞显著高于安全导向型模型（如Google Gemma 3）。作者指出开源模型虽促进创新，但部署时需采用分层安全控制以降低风险。


<details>
  <summary>Details</summary>
Motivation: 开源模型为下游应用提供基础，但其安全防护机制在持续性对抗攻击中的有效性尚不明确。研究旨在评估主流开源模型在应对多轮对抗攻击时的脆弱性，揭示潜在部署风险。

Method: 采用自动化对抗测试技术，对8个开源权重LLMs进行系统评估。重点测量模型在单轮提示注入（prompt injection）与多轮越狱攻击（jailbreak attacks）下的抵抗能力，并对比不同模型设计策略（能力导向vs安全导向）的表现差异。

Result: 1. 所有测试模型均存在普遍漏洞，多轮攻击成功率在25.86%至92.78%间，较单轮攻击成功率提升2-10倍；2. 能力导向模型（如Llama 3.3、Qwen 3）的多轮攻击脆弱性显著高于安全导向模型（如Google Gemma 3）。

Conclusion: 当前开源权重LLMs在持续交互中缺乏系统性的安全防护机制，存在可被多轮攻击利用的深度漏洞。建议开发者在部署时采用分层安全架构（security-first design），企业场景需搭配专业AI安全解决方案以管控伦理与操作风险。

Abstract: Open-weight models provide researchers and developers with accessible
foundations for diverse downstream applications. We tested the safety and
security postures of eight open-weight large language models (LLMs) to identify
vulnerabilities that may impact subsequent fine-tuning and deployment. Using
automated adversarial testing, we measured each model's resilience against
single-turn and multi-turn prompt injection and jailbreak attacks. Our findings
reveal pervasive vulnerabilities across all tested models, with multi-turn
attacks achieving success rates between 25.86\% and 92.78\% -- representing a
$2\times$ to $10\times$ increase over single-turn baselines. These results
underscore a systemic inability of current open-weight models to maintain
safety guardrails across extended interactions. We assess that alignment
strategies and lab priorities significantly influence resilience:
capability-focused models such as Llama 3.3 and Qwen 3 demonstrate higher
multi-turn susceptibility, whereas safety-oriented designs such as Google Gemma
3 exhibit more balanced performance.
  The analysis concludes that open-weight models, while crucial for innovation,
pose tangible operational and ethical risks when deployed without layered
security controls. These findings are intended to inform practitioners and
developers of the potential risks and the value of professional AI security
solutions to mitigate exposure. Addressing multi-turn vulnerabilities is
essential to ensure the safe, reliable, and responsible deployment of
open-weight LLMs in enterprise and public domains. We recommend adopting a
security-first design philosophy and layered protections to ensure resilient
deployments of open-weight models.

</details>


### [4] [Let the Bees Find the Weak Spots: A Path Planning Perspective on Multi-Turn Jailbreak Attacks against LLMs](https://arxiv.org/abs/2511.03271)
*Yize Liu,Yunyun Hou,Aina Sui*

Main category: cs.CR

TL;DR: 本文提出了一种基于动态加权图理论的多轮越狱攻击模型，并引入增强的ABC算法（人工蜂群算法），显著提高了攻击效率，在多种语言模型上实现了高攻击成功率并大幅降低查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有红队评估方法在多轮越狱攻击中存在攻击空间探索不足和攻击过程开销高昂的问题，需要更高效的路径搜索方法来优化攻击效率。

Method: 首先将多轮攻击过程建模为动态加权图的路径规划问题，随后提出改进的人工蜂群算法ABC，通过雇佣蜂、观察蜂和侦查蜂的协作机制优化攻击路径搜索。

Result: 在3个开源和2个专有语言模型上，攻击成功率均超过90%（最高达98%），平均仅需26次查询，显著优于基线方法。

Conclusion: ABC算法在保证高攻击成功率的同时，极大减少了红队评估的开销，证明了该框架在多轮越狱攻击中的高效性和实用性。

Abstract: Large Language Models (LLMs) have been widely deployed across various
applications, yet their potential security and ethical risks have raised
increasing concerns. Existing research employs red teaming evaluations,
utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs.
However, these approaches often lack exploration of successful dialogue
trajectories within the attack space, and they tend to overlook the
considerable overhead associated with the attack process. To address these
limitations, this paper first introduces a theoretical model based on
dynamically weighted graph topology, abstracting the multi-turn attack process
as a path planning problem. Based on this framework, we propose ABC, an
enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a
collaborative search mechanism with employed, onlooker, and scout bees. This
algorithm significantly improves the efficiency of optimal attack path search
while substantially reducing the average number of queries required. Empirical
evaluations on three open-source and two proprietary language models
demonstrate the effectiveness of our approach, achieving attack success rates
above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and
outperforming existing baselines. Furthermore, it achieves comparable success
with only 26 queries on average, significantly reducing red teaming overhead
and highlighting its superior efficiency.

</details>


### [5] [Two thousand years of the oracle problem. Insights from Ancient Delphi on the future of blockchain oracles](https://arxiv.org/abs/2511.03319)
*Giulio Caldarelli,Massimiliano Ornaghi*

Main category: cs.CR

TL;DR: 该论文将古希腊德尔斐神谕与现代区块链预言机进行比较分析，构建了一个框架，旨在揭示两者的共性和各自领域的深化理解，并探索问题类型如何影响预言机回答的质量。


<details>
  <summary>Details</summary>
Motivation: 预言机问题（信息真实性与公正性验证）在古今均有体现，但现代区块链预言机的去中心化特性增加了复杂性。作者希望通过连接德尔斐神谕与现代区块链预言机，为两者领域提供新的分析工具并互相借鉴解决方案。

Method: 1. 构建概念框架，比较德尔斐神谕和区块链预言机。2. 基于区块链预言机分类法，对167条德尔斐神谕问题进行词汇分析，探究问题类型与预言答案质量的关联。

Result: 1. 归纳了古典预兆与现代预言机的共性（如信任机制、信息传递形式）。2. 发现德尔斐神谕的答案质量与所提问题的类型存在相关性（具体关联需通过词汇分析验证）。

Conclusion: 1. 框架有助于双向提升：为区块链预言机从德尔斐案例中汲取可靠性策略；为古典文献分析提供可扩展的分类工具。2. 研究凸显了跨时空技术挑战的延续性，尤其强调问题设计对预言质量的关键作用。

Abstract: The oracle problem refers to the inability of an agent to know if the
information coming from an oracle is authentic and unbiased. In ancient times,
philosophers and historians debated on how to evaluate, increase, and secure
the reliability of oracle predictions, particularly those from Delphi, which
pertained to matters of state. Today, we refer to data carriers for automatic
machines as oracles, but establishing a secure channel between these oracles
and the real world still represents a challenge. Despite numerous efforts, this
problem remains mostly unsolved, and the recent advent of blockchain oracles
has added a layer of complexity because of the decentralization of blockchains.
This paper conceptually connects Delphic and modern blockchain oracles,
developing a comparative framework. Leveraging blockchain oracle taxonomy,
lexical analysis is also performed on 167 Delphic queries to shed light on the
relationship between oracle answer quality and question type. The presented
framework aims first at revealing commonalities between classical and
computational oracles and then at enriching the oracle analysis within each
field. This study contributes to the computer science literature by proposing
strategies to improve the reliability of blockchain oracles based on insights
from Delphi and to classical literature by introducing a framework that can
also be applied to interpret and classify other ancient oracular mechanisms.

</details>


### [6] [LaMoS: Enabling Efficient Large Number Modular Multiplication through SRAM-based CiM Acceleration](https://arxiv.org/abs/2511.03341)
*Haomin Li,Fangxin Liu,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.CR

TL;DR: LaMoS是一个基于SRAM的高效存内计算设计，用于大数模乘运算，具有高可扩展性和面积效率。该设计通过分析Barrett算法并映射到SRAM存内计算宏，优化数据和架构，解决了现有方案仅支持低比特宽度或依赖低效内存逻辑运算的问题。实验显示LaMoS比现有基于SRAM的存内计算设计速度提升7.02倍，并显著降低了高比特宽度的扩展成本。


<details>
  <summary>Details</summary>
Motivation: 模乘运算是隐私计算技术（如同态加密和零知识证明）中的关键非线性操作，但现有存内计算方案存在两个主要问题：1）大多聚焦低比特宽度的模乘，无法满足主流加密算法（如ECC和RSA）所需的高比特宽度操作；2）针对大数模乘的方案依赖低效的内存逻辑操作，导致高比特宽度下扩展成本高且延迟大。

Method: 首先分析Barrett模乘算法并将其工作负载映射到SRAM存内计算宏，适用于高比特宽度场景；开发了高效的存内计算架构和数据流以优化大数模乘；通过工作负载分组改进映射方案，提升高比特宽度下的可扩展性。

Result: 实验结果表明，与现有基于SRAM的存内计算设计相比，LaMoS实现了7.02倍的速度提升，并显著降低了高比特宽度的扩展成本。

Conclusion: LaMoS为高比特宽度模乘提供了一种高效、可扩展的存内计算解决方案，解决了现有方案在性能和可扩展性上的局限。

Abstract: Barrett's algorithm is one of the most widely used methods for performing
modular multiplication, a critical nonlinear operation in modern privacy
computing techniques such as homomorphic encryption (HE) and zero-knowledge
proofs (ZKP). Since modular multiplication dominates the processing time in
these applications, computational complexity and memory limitations
significantly impact performance. Computing-in-Memory (CiM) is a promising
approach to tackle this problem. However, existing schemes currently suffer
from two main problems: 1) Most works focus on low bit-width modular
multiplication, which is inadequate for mainstream cryptographic algorithms
such as elliptic curve cryptography (ECC) and the RSA algorithm, both of which
require high bit-width operations; 2) Recent efforts targeting large number
modular multiplication rely on inefficient in-memory logic operations,
resulting in high scaling costs for larger bit-widths and increased latency. To
address these issues, we propose LaMoS, an efficient SRAM-based CiM design for
large-number modular multiplication, offering high scalability and area
efficiency. First, we analyze the Barrett's modular multiplication method and
map the workload onto SRAM CiM macros for high bit-width cases. Additionally,
we develop an efficient CiM architecture and dataflow to optimize large-number
modular multiplication. Finally, we refine the mapping scheme for better
scalability in high bit-width scenarios using workload grouping. Experimental
results show that LaMoS achieves a $7.02\times$ speedup and reduces high
bit-width scaling costs compared to existing SRAM-based CiM designs.

</details>


### [7] [Federated Anonymous Blocklisting across Service Providers and its Application to Group Messaging](https://arxiv.org/abs/2511.03486)
*David Soler,Carlos Dafonte,Manuel Fernández-Veiga,Ana Fernández Vilas,Francisco J. Nóvoa*

Main category: cs.CR

TL;DR: 本文提出了FAB（联邦匿名阻止列表）方案，取代传统的中心化匿名阻止列表（AB），通过分布式的领域（Realms）及其信任关系，使用户在认证时需证明自己未被任何信任领域阻止。方案的性能与阻止列表大小无关，无需处理新增条目，并成功集成到MLS协议中。


<details>
  <summary>Details</summary>
Motivation: 即时通讯中已有的匿名技术如端到端加密和假名虽提升隐私，但群组仍需在匿名环境下实施管理以防止不当用户。传统AB方案中，用户需在认证时证明所有过往假名均未被屏蔽，但中心化服务提供者存在单点问题。

Method: 设计FAB方案：将中心服务提供者替换为多个自治领域，每个领域维护独立阻止列表。领域间可建立信任关系，用户认证时需通过密码学证明（如零知识证明）自己未被任何信任领域阻止。方案性能优化为与阻止列表大小解耦，且无需实时更新处理新增条目。

Result: 实现方案并测试：性能不随阻止列表大小变化（传统AB方案认证时间随名单增长线性增加），计算开销恒定。成功集成至消息层安全（MLS）协议中，证明其实际适用性。

Conclusion: FAB方案解决了中心化AB的扩展性和效率问题，通过分布式领域和信任关系实现跨领域身份审查，同时保持匿名性。其恒定性能特性适合大规模应用，MLS协议的整合证明其在即时通讯场景中的可行性。

Abstract: Instant messaging has become one of the most used methods of communication
online, which has attracted significant attention to its underlying
cryptographic protocols and security guarantees. Techniques to increase privacy
such as End-to-End Encryption and pseudonyms have been introduced. However,
online spaces such as messaging groups still require moderation to prevent
misbehaving users from participating in them, particularly in anonymous
contexts.. In Anonymous Blocklisting (AB) schemes, users must prove during
authentication that none of their previous pseudonyms has been blocked,
preventing misbehaving users from creating new pseudonyms. In this work we
propose an alternative \textit{Federated Anonymous Blocklisting} (FAB) in which
the centralised Service Provider is replaced by small distributed Realms, each
with its own blocklist. Realms can establish trust relationships between each
other, such that when users authenticate to a realm, they must prove that they
are not banned in any of its trusted realms. We provide an implementation of
our proposed scheme; unlike existing AB constructions, the performance of ours
does not depend on the current size of the blocklist nor requires processing
new additions to the blocklist. We also demonstrate its applicability to
real-world messaging groups by integrating our FAB scheme into the Messaging
Layer Security protocol.

</details>


### [8] [Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology](https://arxiv.org/abs/2511.03641)
*Thomas Souverain*

Main category: cs.CR

TL;DR: 本文提出用于评估大型语言模型水印方法的新框架，以满足欧盟AI法案关于可靠性、互操作性、有效性和鲁棒性的要求。通过建立水印分类体系、评估指标和对比分析，指出现有方法均未完全满足所有标准，建议未来研究聚焦模型底层架构嵌入水印。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案要求通用模型输出具有可检测标记，但水印技术快速迭代且缺乏统一评估标准。本文旨在将法案的抽象要求转化为可量化指标，填补水印评估体系空白。

Method: 1) 按LLM生命周期阶段建立水印分类法；2) 将法案四标准映射为技术指标（鲁棒性/可检测性/质量），并提出互操作性三维度评估框架；3) 用该框架对比现有水印方法。

Result: 现有水印技术均未能同时满足所有四项标准。实验表明在模型底层架构嵌入水印更具潜力，但需进一步验证以满足互操作性等要求。

Conclusion: 首次实现欧盟法案与水印技术的评估对接，证明当前技术差距。建议未来优先研究模型架构层水印方案，并为政策制定提供可扩展的测试基准。

Abstract: To foster trustworthy Artificial Intelligence (AI) within the European Union,
the AI Act requires providers to mark and detect the outputs of their
general-purpose models. The Article 50 and Recital 133 call for marking methods
that are ''sufficiently reliable, interoperable, effective and robust''. Yet,
the rapidly evolving and heterogeneous landscape of watermarks for Large
Language Models (LLMs) makes it difficult to determine how these four standards
can be translated into concrete and measurable evaluations. Our paper addresses
this challenge, anchoring the normativity of European requirements in the
multiplicity of watermarking techniques. Introducing clear and distinct
concepts on LLM watermarking, our contribution is threefold. (1) Watermarking
Categorisation: We propose an accessible taxonomy of watermarking methods
according to the stage of the LLM lifecycle at which they are applied - before,
during, or after training, and during next-token distribution or sampling. (2)
Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping
each criterion with state-of-the-art evaluations on robustness and
detectability of the watermark, and of quality of the LLM. Since
interoperability remains largely untheorised in LLM watermarking research, we
propose three normative dimensions to frame its assessment. (3) Watermarking
Comparison: We compare current watermarking methods for LLMs against the
operationalised European criteria and show that no approach yet satisfies all
four standards. Encouraged by emerging empirical tests, we recommend further
research into watermarking directly embedded within the low-level architecture
of LLMs.

</details>


### [9] [Whisper Leak: a side-channel attack on Large Language Models](https://arxiv.org/abs/2511.03675)
*Geoff McDonald,Jonathan Bar Or*

Main category: cs.CR

TL;DR: 提出了一种名为Whisper Leak的侧信道攻击方法，利用加密流量中的元数据（数据包大小和时间模式）推断用户提示的主题，挑战了当前大型语言模型的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在敏感领域（如医疗、法律和机密通信）部署日益增多，隐私问题凸显。尽管TLS加密保护内容，但元数据泄漏可能导致主题泄露。

Method: 使用Whisper Leak方法分析流式响应中的数据包大小和时间模式，进行主题分类。在28个流行的大型语言模型上进行测试，测试了噪声比极端不平衡的情况（10,000:1）。

Result: 在多个模型中实现了近乎完美的主题分类（AUPRC常大于98%），对于如'洗钱'等敏感话题精度甚至达到100%，并恢复了5-20%的目标对话。还评估了三种缓解策略（随机填充、令牌批处理和数据包注入），但均无法完全防御。

Conclusion: 揭示了整个行业普遍存在的元数据泄漏风险，促使模型提供商通过合作披露实施了初步对策，但现有缓解措施不足，亟需供应商加强元数据泄漏防护措施。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential communications, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted LLM traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular LLMs
from major providers, achieving near-perfect classification (often >98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for LLM providers to address metadata leakage as AI systems handle
increasingly sensitive information.

</details>
