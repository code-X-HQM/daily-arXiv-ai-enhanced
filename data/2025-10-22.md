<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 18]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: BreakFun是一种针对大型语言模型（LLM）的越狱技术，通过结构化模式（Trojan Schema）迫使模型输出不良内容，在多种模型上的成功率为89%-100%；论文同时提出一种防御方法（Adversarial Prompt Deconstruction）。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理结构化数据和遵循语法规则的能力虽驱动其广泛使用，却成为其脆弱点。研究旨在利用模型对结构化模式的固守展示其脆弱性，并为更鲁棒的对齐模型提供新思路。

Method: 提出BreakFun攻击框架，其三个组件包括无害伪装、思维链干扰、以及作为核心的Trojan Schema（精心设计的强制输出有害内容的数据结构）。防御策略采用Adversarial Prompt Deconstruction，利用次级LLM提取人类可读文本以隔离用户真实意图。

Result: 在13种基础模型和商业模型（JailbreakBench）上平均攻击成功率89%，部分模型达100%。防御措施证明Trojan Schema是攻击主因，通过剥离恶意模式可有效缓解威胁。

Conclusion: LLM的核心优势可转变为关键弱点，采用对抗性防护手段（如文本脱壳防御）能提升模型鲁棒性。

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [2] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: ParaVul是一个并行LLM和检索增强框架，用于提高智能合约漏洞检测的可靠性和准确性。它通过SLoRA技术微调LLM，结合混合检索系统（RAG）和元学习模型融合输出，并在检测后生成详细报告。实验显示其在单标签和多标签检测上的F1分数分别达到0.9398和0.9330。


<details>
  <summary>Details</summary>
Motivation: 现有的智能合约漏洞检测方法（静态分析和形式化验证）存在高误报率和可扩展性差的问题。而现有LLM方案则面临高推理成本和计算开销大的挑战。因此，需要一种更高效、准确的检测方法。

Method: 1. 开发SLoRA（稀疏低秩适应）技术微调LLM：在量化LoRA基础上加入稀疏矩阵，降低计算开销。2. 构建漏洞数据集和混合RAG系统（稠密检索+BM25）辅助验证LLM结果。3. 提出元学习模型融合RAG与LLM输出，生成最终结果。4. 设计思维链提示指导LLM生成漏洞报告。

Result: ParaVul在实验中表现出色：单标签检测F1分数0.9398，多标签检测F1分数0.9330，显著优于现有方法。

Conclusion: ParaVul框架有效结合了微调LLM与检索增强系统，在降低计算成本的同时提升了漏洞检测准确率。其模块化设计（SLoRA、混合RAG、元学习融合）为智能合约安全领域提供了可扩展的解决方案。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [3] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 本文提出了PLAGUE框架，一个用于设计终身学习式多轮越狱攻击的新方法，该框架将攻击过程分为三个精心设计的阶段（启动器、规划器和终结器），在多个先进模型上提高了攻击成功率超过30%。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中于单轮攻击，而在多轮对话场景下，由于攻击意图可以隐晦地贯穿整个对话，导致现有方法在适应性、效率和效果方面面临挑战。因此，需要一种高效且易于设计的多轮攻击框架来评估模型的安全性漏洞。

Method: 提出了PLAGUE框架，它是一种即插即用的方法，将多轮攻击生命周期划分为三个阶段：1) 启动器阶段，初始化攻击上下文并引导对话；2) 规划器阶段，通过持续学习和上下文优化扩展攻击能力；3) 终结器阶段，总结攻击信息并最大化危害输出。

Result: 实验表明，PLAGUE在多个主流模型上显著提升了攻击成功率（ASR），在可控查询次数下，其对OpenAI的o3模型的ASR达到81.4%，对Claude Opus 4.1的ASR达到67.3%，且成功率超过现有方法30%以上。

Conclusion: PLAGUE证明了多轮攻击规划中初始化、上下文优化和持续学习的重要性，为模型安全性评估提供了有效工具和洞察。

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [4] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: 摘要揭示了人工智能驱动的科研和评审中的潜在漏洞：名为'BadScientist'的框架演示了如何通过虚假生成的论文欺骗AI评审系统，展示出高接受率、争议评分等问题，并显示缓解策略效果不佳。


<details>
  <summary>Details</summary>
Motivation: 背景：随着AI科研及评审系统的普及，存在全自动化论文发表循环的漏洞（即AI生成论文被AI评审接受，而无需人工干预）。

Method: 方法：建立BadScientist框架，要求虚假论文生成器在不进行真实实验的前提下操纵内容呈现结果；然后使用多重模型进行评审。构建严谨评测框架（包含约束和校准分析），并将问题归纳为形式化评估及缓解策略测试。

Result: 结果：虚假论文在AI评审系统下能达到75%接受率（特定领域更甚）；但存在矛盾—系统普遍标记学术违规但仍给出高分推荐的评价。缓解措施(如改进聚合等)效果甚微，仅略优于随机决策器。

Conclusion: 结论：当前AI驱动的评审系统具有系统性缺陷，无法可靠监测学术诚信问题，亟需构造一种纵深防护措施以加强科学出版流程可靠性。

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [5] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE是一种基于区块链的隐私保护协议，用于在机器学习中安全评估和选择数据，保证模型和数据隐私，同时提供高效的运行时间和统一的效用评分方法。


<details>
  <summary>Details</summary>
Motivation: 构建模型时需评估外部数据的效用，但现有方法难以同时保护模型构建者的专有数据和数据提供者的隐私。PrivaDE旨在实现恶意安全性保证，确保双方数据的机密性。

Method: 结合区块链构建去信任环境，集成模型蒸馏、模型分片和零知识证明技术提升效率；设计统一效用评分函数（经验损失、预测熵和特征空间多样性），支持主动学习工作流。

Result: 协议在线运行时间控制在15分钟内（即使模型参数达百万级），有效完成数据评估。

Conclusion: PrivaDE为去中心化机器学习生态中的公平自动化数据市场奠定了基础。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [6] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为RESCUE的新型检索增强生成框架，旨在改进大语言模型（LLM）的安全代码生成能力。通过结合两种创新方法——混合知识库构建（LLM辅助聚类摘要提炼+程序切片）和分层多面检索机制，显著降低了安全文档噪音并增强语义关联。在四项基准测试中，RESCUE将SecurePass@1指标平均提升4.8分，刷新了安全代码生成的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码仍存在安全漏洞，传统检索增强生成方法未能有效处理三方面挑战：1) 原始安全文档的噪声干扰；2) 未能充分挖掘任务描述中隐含的安全语义；3) 现有检索方法缺乏对安全关键因素的系统整合。

Method: 1) 混合知识库构建：使用LLM辅助的「先聚类后摘要」提炼技术处理原始安全文档，结合程序切片生成核心安全准则和针对性代码示例； 2) 分级多面检索：自上而下遍历知识库层级，在每层集成多维度安全事实，确保检索结果兼具全面性与精确性。

Result: 在4个代码安全基准和6种LLM上的测试表明：相比5种SOTA方法，RESCUE将SecurePass@1指标平均提升4.8个百分点；消融实验证实框架内各组件均显著提升安全性（集群摘要技术贡献34%的指标增长，分层检索贡献51%）

Conclusion: RESCUE首次系统解决了安全代码生成中的知识噪声与语义割裂问题。分层知识表示与检索机制为LLM安全增强提供了新范式，实验证明该框架具备跨模型泛化能力。未来可扩展至漏洞修复等场景。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [7] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard是一种轻量级混合解决方案，结合检测和修复策略来对抗Linux云环境中的加密货币挖矿恶意软件（即cryptojacker）。它通过低开销的监控和两阶段深度学习分类实现高精度检测，并利用eBPF技术针对逃避技术进行修复，评估显示其性能优越且开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有的解决方案在可扩展性（高监控开销）、针对混淆行为的检测准确性低以及缺乏集成修复方面存在不足。尤其在Linux云环境中，cryptojacker造成重大财务损失，亟需更高效的解决方案。

Method: 1. 使用基于草图（sketch）和滑动窗口的系统调用监控，以最小开销收集行为模式。
2. 将分类任务分解为两阶段：第一阶段检测可疑活动，第二阶段确认。
3. 利用深度学习模型提高检测精确度。
4. 针对入口点污染和PID操纵等逃避技术，整合基于eBPF的目标修复机制。

Result: 在123个真实样本上评估：平均F1分数在第一阶段为96.12%，第二阶段为92.26%；在真阳性率和假阳性率方面优于当前最优基线；每台主机仅产生0.06%的CPU开销。

Conclusion: CryptoGuard提供了一种高效、轻量级的解决方案，有效检测并修复cryptojacker，同时最小化性能开销，适合云环境部署。

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [8] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 本文探讨了大型语言模型（LLM）水印技术在实际部署中的障碍，指出利益相关者（模型提供商、平台和终端用户）的动机不匹配是主要原因。文章重新审视了三种水印技术：模型水印、文本水印和上下文水印（ICW），并强调ICW能通过三方动机对齐实现实用化。最后提出通过动机对齐和社区参与推动水印应用的研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM水印实际部署不足的问题。现有水印技术因三方动机错位面临四大障碍：竞争风险、检测工具治理、鲁棒性担忧和归属问题。通过重新分类水印技术并分析各方案的利益契合度，寻求可落地场景。

Method: 采用框架分析法：1. 识别LLM生态中三大主体（提供商、平台、用户）的动机冲突；2. 将现有水印分为三类（模型水印/文本水印/上下文水印），逐类评估其与主体动机的契合度；3. 提出ICW范式——由可信第三方（如会议主办方）在输入文档中嵌入水印指令，当恶意行为者提交给LLM时，输出会携带可检测水印。

Result: 理论分析表明：1. 模型水印因开源生态发展面临新挑战；2. 文本水印仅在特定场景（如数据集净化）有效；3. ICW展现出动机对齐优势：可信第三方获得检测工具，用户无体验损失，LLM提供商保持中立。框架验证了动机对齐对技术落地的重要性。

Conclusion: 结论主张应开发激励兼容的水印方案（以ICW为例），重点服务存在可信第三方检测需求的场景（如学术会议反剽窃）。提出两大原则：垂直领域定制化和社区参与共建，并呼吁更多动机导向的水印研究。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [9] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: DeepTx是一个实时交易分析系统，用于在用户确认前检测Web3钓鱼攻击。它通过模拟交易、提取特征，并利用多个大型语言模型分析交易意图，结合自省共识机制做出解释性决策，实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 应对Web3生态中日益复杂的钓鱼攻击，包括利用恶意合约逻辑、前端脚本和代币授权模式的新型攻击。

Method: DeepTx模拟待处理交易，提取行为、上下文及用户界面特征，使用多个LLMs推理交易意图，并通过自省共识机制确保决策稳健可解释。

Result: 在钓鱼数据集评估中达到高精确率(precision)和召回率(recall)。

Conclusion: 系统能有效实时检测复杂钓鱼攻击，保护用户资产安全，技术方案具备应用潜力。

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [10] [PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks](https://arxiv.org/abs/2510.18465)
*Spencer King,Irfan Ozen,Karthika Subramani,Saranyan Senthivel,Phani Vadrevu,Roberto Perdisci*

Main category: cs.CR

TL;DR: 论文提出了首个端到端浏览器框架PP3D，用于实时发现、检测和防御基于行为操纵的社会工程攻击。该框架通过浏览器扩展实现视觉检测模型，在客户端部署以保护用户隐私，并在评估中显示出高检测率和低误报率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于网页的行为操纵攻击（BMAs）如恐吓软件、虚假下载等社会工程攻击利用人类决策漏洞，但相较于其他攻击（如钓鱼或恶意软件）研究不足。之前的技术工作主要集中于测量BMAs，缺乏通用防御方案。

Method: 引入Pixel Patrol 3D (PP3D)框架：一个运行在浏览器扩展中的视觉检测模型，通过客户端部署实现跨设备（桌面和移动端）的实时防护，同时保护用户隐私。模型可直接在浏览器中分析页面内容。

Result: 评估显示PP3D在1%误报率下检测率超过99%，且在不同设备上延迟和开销表现良好。即使面对训练后数月收集的新BMA样本，仍能保持97%以上检测率（1%误报率）。

Conclusion: PP3D提供了一种实用、高效且可泛化的防御方案，能应对广泛且持续演变的网页行为操纵攻击。

Abstract: Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake
software downloads, tech support scams, etc. - are a class of social
engineering (SE) attacks that exploit human decision-making vulnerabilities.
These attacks remain under-studied compared to other attacks such as
information harvesting attacks (e.g., phishing) or malware infections. Prior
technical work has primarily focused on measuring BMAs, offering little in the
way of generic defenses.
  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first
end-to-end browser framework for discovering, detecting, and defending against
behavior-manipulating SE attacks in real time. PP3D consists of a visual
detection model implemented within a browser extension, which deploys the model
client-side to protect users across desktop and mobile devices while preserving
privacy.
  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%
false positives, while maintaining good latency and overhead performance across
devices. Even when faced with new BMA samples collected months after training
the detection model, our defense system can still achieve above 97% detection
rate at 1% false positives. These results demonstrate that our framework offers
a practical, effective, and generalizable defense against a broad and evolving
class of web behavior-manipulation attacks.

</details>


### [11] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: 本文评估了四种大型语言模型（ChatGPT、Claude、Gemini和DeepSeek）在解释漏洞信息方面的表现，重点关注它们在SSVC框架决策点预测上的效果。使用384个真实漏洞进行测试，Gemini表现最佳，但所有模型均存在局限性，无法替代专家判断。


<details>
  <summary>Details</summary>
Motivation: 安全分析师面临庞大的漏洞积压压力，研究旨在探索LLM自动化部分漏洞解读流程的可行性，以辅助漏洞优先级排序。

Method: 采用12种提示技术（包括单样本、少样本和思维链），基于VulZoo数据集的384个漏洞发起超16.5万次查询，评估模型在SSVC四大决策点（利用性、自动化、技术影响、使命与福祉）的预测能力。使用F1分数和Cohen's Kappa系数（加权/未加权）衡量性能。

Result: Gemini在三个决策点上表现最优；示例提示普遍提升准确性，但所有模型在部分决策点表现不佳；仅DeepSeek在加权指标上达到一般一致性，且所有模型倾向于高估风险。

Conclusion: 当前LLM无法替代专家判断，但特定模型与提示组合对目标SSVC决策表现出中等有效性。谨慎使用时，LLM可提升漏洞优先级工作流效率。

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [12] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: 本文提出并验证了信任-漏洞悖论（TVP），即多智能体系统中提高信任以增强协调的同时会增加过度暴露和过度授权风险。通过构建跨场景数据集和引入新指标，揭示了高信任在提高任务成功率的同时也增加暴露风险的现象，并提出防御方法以减轻风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的多智能体系统发展迅速，但信任与安全之间的紧张关系未被充分探索。本研究旨在探讨提高智能体间信任以增强协调的同时如何扩大风险，即信任-漏洞悖论（TVP）。

Method: 构建包含3个宏观场景和19个子场景的场景-游戏数据集，进行闭环交互实验并显式参数化信任关系。提出两个统一指标：基于最小必要信息（MNI）安全基准的暴露边界检测指标（OER）和信任水平敏感度指标（AD）。

Result: 跨模型和框架的实验显示：更高信任提升任务成功率，但增加暴露风险；不同系统的信任-风险映射存在异质性。提出的敏感信息分区和守护智能体防御机制有效降低OER与AD。

Conclusion: 研究形式化定义了TVP，建立了可复现基线，证实信任必须作为首要安全变量在多智能体系统设计中建模和调度。

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [13] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: 该论文提出了一种用于医疗物联网系统的安全框架，包括设备信任评估、区块链集成和轻量级LSTM异常检测，在精度、攻击检测率和降低误报率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备在提升医疗服务的同时带来了安全风险。传统安全措施无法满足其异构、资源受限和实时性需求，需要一个兼顾安全与效率的解决方案。

Method: 分三阶段：1) 基于信誉机制评估设备可靠性；2) 区块链整合轻量级工作量证明确保数据安全；3) 轻量级LSTM模型实时检测异常。

Result: 框架精度、准确率、召回率均提升2%，攻击检测率上升5%，误报率降低3%。

Conclusion: 该框架在保证实时性和可扩展性的同时，能有效增强医疗物联网系统的安全性和可靠性。

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [14] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: SecretLoc是一种基于LLM的方法，用于检测Android应用中的硬编码秘密凭证。该方法利用上下文和结构线索，无需依赖预定义模式或标记数据集。实验证明SecretLoc能检测出正则表达式、静态分析和机器学习方法遗漏的秘密，包括新类型。在分析Google Play应用时，发现大量应用存在此问题，凸显了主动管理秘密凭证的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法（基于正则表达式/静态分析/机器学习）需要预知凭证结构或训练数据，存在根本性局限。开发者硬编码API密钥等凭证的行为会导致严重安全风险，一旦遭窃取可引发数据泄露或资源滥用。

Method: 提出SecretLoc方法：利用LLM模型，通过分析代码中的上下文和结构特征来识别硬编码秘密。不需要预定义模式或标记训练集。使用来自文献的基准数据集进行验证。

Result: 1. 在基准数据集中发现4828个现有方法遗漏的秘密，包括10余种新类型（如OpenAI API密钥、GitHub令牌等）。2. 在Google Play的5000个应用中检测到2124个应用（42.5%）存在秘密凭证，部分漏洞经披露后修复。

Conclusion: 攻击者同样能利用LLM技术窃取秘密，凸显移动生态系统中主动凭证管理和强化防护措施的紧迫性。SecretLoc证明LLM能有效检测未知类型的硬编码秘密。

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [15] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: 本文提出了一种结合统计预处理和关联规则挖掘的新方法，用于检测RISC-V处理器中的微架构侧信道攻击，该方法具有可重新配置能力，能推广到任何微架构攻击的检测。与现有技术相比，该方法在加密、计算和内存密集型工作负载下实现了准确性、精确度和召回率的提升，并能检测新型flush+fault攻击变体。


<details>
  <summary>Details</summary>
Motivation: RISC-V处理器在关键应用中的普及使其面临微架构侧信道攻击的威胁，而现有检测方法（尤其是针对flush+fault攻击）存在实践层面的不足。现有基于机器学习的检测方法在实用性方面存在局限，需进一步研究以解决被忽视的问题。

Method: 利用gem5模拟器，提出结合统计预处理和关联规则挖掘的检测方法。该方法通过可重新配置能力实现对任意微架构攻击的通用检测，并利用关联规则的可解释性深入分析攻击/良性应用执行时的微架构行为。

Result: 在加密、计算和内存密集型工作负载下，相比最先进技术，该方法准确率提升5.15%，精确度提升7%，召回率提升3.91%，并能灵活检测新型flush+fault攻击变体。

Conclusion: 提出的方法显著提升了RISC-V微架构攻击检测性能，其基于关联规则的可解释性为理解攻击机制提供了新视角，解决了现有ML方法未覆盖的实践问题。

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [16] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*René Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Qatsi是一个基于Argon2id的分层密钥派生方案，无需持久存储即可生成可复现的加密密钥。它通过主密钥和分层上下文确定性地派生所有密钥，消除了基于保险库的攻击面。方案通过内存密集型算法确保密钥熵值达103-312 bit，采用GPU攻击成本证明其安全性，并在Rust中实现。基准测试显示在M1 Pro处理器上标准模式544 ms、偏执模式2273 ms性能。适用于隔离系统及主凭证生成。


<details>
  <summary>Details</summary>
Motivation: 现有密钥存储通常依赖保险库系统，存在攻击面。Qatsi提出无需持久存储的密钥派生方案，通过确定性派生消除保险库风险，同时确保密钥的强熵和可复现性，满足隔离系统及主凭证生成场景的需求。

Method: 1. 使用Argon2id内存密集型哈希函数进行分层密钥派生；2. 通过单一高熵主密钥和上下文分层确定性地生成所有密钥；3. 采用可证明均匀的拒绝采样方法派生成7776词助记词或90字符密码；4. 内存消耗在64-128 MiB间，迭代16-32次具体取决于模式。

Result: 1. 方案在理论上证明输出均匀性；2. GPU攻击成本量化：单一GPU在偏执模式(128 MiB内存)下攻击80-bit主密钥需$2.4×10^{16}$年；3. Rust实现提供零内存化、编译时词表验证等安全特性；4. Apple M1 Pro基准测试显示标准模式544ms、偏执模式2273ms的分层派生延迟。

Conclusion: Qatsi实现了无状态的密钥可重现性，适用于安全要求极高但接受派生延迟的隔离系统与主凭证场景。但由于算法延迟，其不适用需灵活轮换密钥的场景。

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [17] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 本文针对临床大语言模型（LLMs）在医疗保健领域应用中的隐私风险进行了实证研究，重点探讨了成员推理攻击（membership inference）的可能性。研究发现，当前临床LLMs虽有一定抵抗性，但仍存在可测量的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在临床决策支持、文档和患者信息系统中的深入应用，确保其隐私性和可信度成为医疗保健领域的重要挑战。微调LLMs于敏感电子健康记录（EHR）数据上虽提升了领域适应性，但可能导致患者信息通过模型行为泄露。

Method: 使用最新的临床问答模型Llemr，评估了两种攻击策略：传统的基于损失的攻击方法，以及一种更贴合临床实际的基于改述的扰动策略。

Result: 初步结果显示存在有限但可测量的成员信息泄露，表明当前临床LLMs对隐私攻击具有一定抵抗性，但仍有薄弱环节。

Conclusion: 研究结果强调了开发面向领域的隐私评估及防御措施（如差分隐私微调和改述感知训练）的必要性，以提高医疗AI系统的安全性和可信度。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


### [18] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: sNVMe-oF是一种基于NVMe-oF协议的安全存储管理系统，通过引入控制路径、计数器租赁和优化技术，在不修改协议的前提下，实现了机密性、完整性和新鲜性保证，同时仅带来2%的性能损失。


<details>
  <summary>Details</summary>
Motivation: 传统保密计算（CC）方法在保护高性能存储时难以扩展，要么牺牲性能要么降低安全性。因此，需要一种既能满足CC威胁模型要求，又能维持高性能的解决方案。

Method: sNVMe-oF扩展了NVMe-oF协议，提供控制路径和计数器租赁机制。它优化了数据路径性能：利用NVMe元数据、设计新型分布式Hazel Merkle树（HMT），并避免冗余IPSec保护。还利用支持CC的智能网卡加速器，在NVIDIA BlueField-3平台上实现原型系统。

Result: 实验证明，在合成负载和AI训练场景下，sNVMe-oF的性能损失最低仅2%，同时保障了存储机密性、完整性和新鲜性。

Conclusion: sNVMe-oF通过协议层优化和硬件加速器的高效协同，解决了可扩展的安全存储问题，是高性能机密计算存储的可行方案。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>
