<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 10]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [HAMLOCK: HArdware-Model LOgically Combined attacK](https://arxiv.org/abs/2510.19145)
*Sanskar Amgain,Daniel Lobo,Atri Chatterjee,Swarup Bhunia,Fnu Suya*

Main category: cs.CR

TL;DR: 该论文提出了HAMLOCK攻击方法，一种在硬件-软件边界分布攻击逻辑的隐蔽后门攻击，通过在软件模型中微调少量神经元的激活值，结合硬件的特洛伊木马来修改输出，从而绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有模型级后门攻击因攻击逻辑完全嵌入软件层而容易被检测到，需要一种更隐蔽的攻击方法通过跨硬件-软件层协作实现攻击。

Method: 在软件模型中微调少量神经元的激活值，使其在触发器出现时产生独特高激活值；硬件中的特洛伊木马监测这些激活值并操控输出层进行错误分类。

Result: 在多个基准测试上，HAMLOCK实现了近乎完美的攻击成功率，且对原始任务准确率影响可忽略；同时有效绕过现有防御机制，硬件开销极低（面积和功耗增加仅为0.01%）。

Conclusion: HAMLOCK暴露了硬件-软件界面的安全漏洞，强调了需要跨层防御机制来应对此类新型威胁。

Abstract: The growing use of third-party hardware accelerators (e.g., FPGAs, ASICs) for
deep neural networks (DNNs) introduces new security vulnerabilities.
Conventional model-level backdoor attacks, which only poison a model's weights
to misclassify inputs with a specific trigger, are often detectable because the
entire attack logic is embedded within the model (i.e., software), creating a
traceable layer-by-layer activation path.
  This paper introduces the HArdware-Model Logically Combined Attack (HAMLOCK),
a far stealthier threat that distributes the attack logic across the
hardware-software boundary. The software (model) is now only minimally altered
by tuning the activations of few neurons to produce uniquely high activation
values when a trigger is present. A malicious hardware Trojan detects those
unique activations by monitoring the corresponding neurons' most significant
bit or the 8-bit exponents and triggers another hardware Trojan to directly
manipulate the final output logits for misclassification.
  This decoupled design is highly stealthy, as the model itself contains no
complete backdoor activation path as in conventional attacks and hence, appears
fully benign. Empirically, across benchmarks like MNIST, CIFAR10, GTSRB, and
ImageNet, HAMLOCK achieves a near-perfect attack success rate with a negligible
clean accuracy drop. More importantly, HAMLOCK circumvents the state-of-the-art
model-level defenses without any adaptive optimization. The hardware Trojan is
also undetectable, incurring area and power overheads as low as 0.01%, which is
easily masked by process and environmental noise. Our findings expose a
critical vulnerability at the hardware-software interface, demanding new
cross-layer defenses against this emerging threat.

</details>


### [2] [OpenGuardrails: An Open-Source Context-Aware AI Guardrails Platform](https://arxiv.org/abs/2510.19169)
*Thomas Wang,Haowen Li*

Main category: cs.CR

TL;DR: OpenGuardrails开源项目首次提供上下文感知的安全和操控检测模型以及可部署平台，以保护LLMs免受内容安全风险、模型操控攻击（如提示注入、越狱、代码解释器滥用和恶意代码生成/执行）以及数据泄漏的威胁。通过统一的大型模型实现安全和操控检测，使用轻量级NER流程进行数据泄漏识别和编辑，支持作为安全网关或API服务部署，并在多个语言的安全基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的整合日益增多，保护其免受不安全、恶意或侵犯隐私的内容变得至关重要。现有方案往往在模型操控攻击和多语言支持方面存在不足，因此需要一种既能检测内容安全性又能应对多种模型攻击的综合防护方案。

Method: OpenGuardrails采用双管齐下的方法：1) 使用统一的大型模型处理内容安全和模型操控检测（包括提示注入、越狱等）；2) 通过轻量级NER流程（如类似Presidio的模型或基于正则的检测器）执行数据泄漏识别与编辑。系统可部署为安全网关或API服务，支持企业级私有部署。

Result: 项目在安全基准测试中达到最先进（SOTA）性能，在英语、汉语及多语言任务中的提示和响应分类表现优异。所有模型以Apache 2.0许可证开源，提供全栈保护方案。

Conclusion: OpenGuardrails提供了首个开源、支持多威胁防护的可部署平台，统一的安全检测模型配合轻量级数据泄漏处理流程，为LLMs在现实应用中的安全性设定了新标准。其多语言SOTA性能和灵活部署选项使其成为工业级保护方案的理想选择。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, safeguarding them against unsafe, malicious, or
privacy-violating content is critically important. We present OpenGuardrails,
the first open-source project to provide both a context-aware safety and
manipulation detection model and a deployable platform for comprehensive AI
guardrails. OpenGuardrails protects against content-safety risks,
model-manipulation attacks (e.g., prompt injection, jailbreaking,
code-interpreter abuse, and the generation/execution of malicious code), and
data leakage. Content-safety and model-manipulation detection are implemented
by a unified large model, while data-leakage identification and redaction are
performed by a separate lightweight NER pipeline (e.g., Presidio-style models
or regex-based detectors). The system can be deployed as a security gateway or
an API-based service, with enterprise-grade, fully private deployment options.
OpenGuardrails achieves state-of-the-art (SOTA) performance on safety
benchmarks, excelling in both prompt and response classification across
English, Chinese, and multilingual tasks. All models are released under the
Apache 2.0 license for public use.

</details>


### [3] [Defending Against Prompt Injection with DataFilter](https://arxiv.org/abs/2510.19207)
*Yizhu Wang,Sizhe Chen,Raghad Alkhudair,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: 本文提出DataFilter，一种模型无关的防御机制，用于在LLM处理前从数据中移除恶意指令，有效抵抗提示注入攻击，同时保持LLM实用性。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击会覆盖原始用户任务，引导LLM执行有害操作。现有防御方法需模型权重访问、导致效用损失或需系统重设计。

Method: DataFilter利用用户指令和数据，通过监督微调模拟注入来训练模型，选择性剥离对抗内容并保留良性信息。

Result: 在多个基准测试中，DataFilter将提示注入攻击成功率降至接近零，同时维持了LLM的实用性。该模型在HuggingFace开源。

Conclusion: DataFilter提供了强安全性、高实用性和即插即用部署，可作为黑盒商业LLMs防御提示注入的有效方案。

Abstract: When large language model (LLM) agents are increasingly deployed to automate
tasks and interact with untrusted external data, prompt injection emerges as a
significant security threat. By injecting malicious instructions into the data
that LLMs access, an attacker can arbitrarily override the original user task
and redirect the agent toward unintended, potentially harmful actions. Existing
defenses either require access to model weights (fine-tuning), incur
substantial utility loss (detection-based), or demand non-trivial system
redesign (system-level). Motivated by this, we propose DataFilter, a test-time
model-agnostic defense that removes malicious instructions from the data before
it reaches the backend LLM. DataFilter is trained with supervised fine-tuning
on simulated injections and leverages both the user's instruction and the data
to selectively strip adversarial content while preserving benign information.
Across multiple benchmarks, DataFilter consistently reduces the prompt
injection attack success rates to near zero while maintaining the LLMs'
utility. DataFilter delivers strong security, high utility, and plug-and-play
deployment, making it a strong practical defense to secure black-box commercial
LLMs against prompt injection. Our DataFilter model is released at
https://huggingface.co/JoyYizhu/DataFilter for immediate use, with the code to
reproduce our results at https://github.com/yizhu-joy/DataFilter.

</details>


### [4] [LAPRAD: LLM-Assisted PRotocol Attack Discovery](https://arxiv.org/abs/2510.19264)
*R. Can Aygun,Yehuda Afek,Anat Bremler-Barr,Leonard Kleinrock*

Main category: cs.CR

TL;DR: 本文介绍了LAPRAD方法，一种结合大型语言模型（LLM）的半自动化技术，用于发现DNS等协议的新漏洞。该方法通过三个阶段（LLM咨询、自动配置生成和验证）发现了三种针对DNS协议的新型DDoS攻击（SigCacheFlush），这些攻击能绕过现有补丁并显著降低解析器性能。


<details>
  <summary>Details</summary>
Motivation: 为提高互联网协议（如DNS、BGP）的安全性，需快速、半自动地发现新漏洞。传统方法效率低且难以发现复杂漏洞，因此探索利用LLM辅助安全研究的方法。

Method: 提出三阶段LAPRAD方法：1) 用基于DNS知识训练的LLM（GPT-o1）识别潜在攻击；2) 由另一LLM通过ReACT和LangChain自动生成攻击配置（如DNS区域文件）；3) 实验验证攻击的有效性。

Result: 成功发现三种新型DNS DDoS攻击：a) 诱骗解析器缓存伪造的大尺寸DNSSEC RRSIG记录（服务能力降至6%）；b) 利用大型DNSSEC算法（RSA-4096）和多密钥绕过RRSet限制；c) 使用ANY响应实现类似效果。攻击变种SigCacheFlush可规避现有补丁，影响主流DNS解析器最新版本。

Conclusion: LAPRAD证明LLM能高效辅助发现未知协议漏洞。新发现的DNS攻击绕过现有防御措施，突显协议安全的持续挑战。该方法可扩展至其他协议（如BGP）的安全分析。

Abstract: With the goal of improving the security of Internet protocols, we seek
faster, semi-automatic methods to discover new vulnerabilities in protocols
such as DNS, BGP, and others. To this end, we introduce the LLM-Assisted
Protocol Attack Discovery (LAPRAD) methodology, enabling security researchers
with some DNS knowledge to efficiently uncover vulnerabilities that would
otherwise be hard to detect.
  LAPRAD follows a three-stage process. In the first, we consult an LLM
(GPT-o1) that has been trained on a broad corpus of DNS-related sources and
previous DDoS attacks to identify potential exploits. In the second stage, a
different LLM automatically constructs the corresponding attack configurations
using the ReACT approach implemented via LangChain (DNS zone file generation).
Finally, in the third stage, we validate the attack's functionality and
effectiveness.
  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and
rediscovered two recently reported ones that were not included in the LLM's
training data. The first new attack employs a bait-and-switch technique to
trick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving
capacity to as little as 6%. The second exploits large DNSSEC encryption
algorithms (RSA-4096) with multiple keys, thereby bypassing a recently
implemented default RRSet limit. The third leverages ANY-type responses to
produce a similar effect.
  These variations of a cache-flushing DDoS attack, called SigCacheFlush,
circumvent existing patches, severely degrade resolver query capacity, and
impact the latest versions of major DNS resolver implementations.

</details>


### [5] [Collaborative penetration testing suite for emerging generative AI algorithms](https://arxiv.org/abs/2510.19303)
*Petar Radanliev*

Main category: cs.CR

TL;DR: 摘要概述。


<details>
  <summary>Details</summary>
Motivation: 描述论文工作的动机。

Method: 描述本文使用的方法和基本方法介绍或比较说明。

Result: 描述本文的结果、指标、性能提升等。

Conclusion: 描述本文的结论、或解决方案、推荐方案等。

Abstract: Problem Space: AI Vulnerabilities and Quantum Threats Generative AI
vulnerabilities: model inversion, data poisoning, adversarial inputs. Quantum
threats Shor Algorithm breaking RSA ECC encryption. Challenge Secure generative
AI models against classical and quantum cyberattacks. Proposed Solution
Collaborative Penetration Testing Suite Five Integrated Components: DAST SAST
OWASP ZAP, Burp Suite, SonarQube, Fortify. IAST Contrast Assess integrated with
CI CD pipeline. Blockchain Logging Hyperledger Fabric for tamper-proof logs.
Quantum Cryptography Lattice based RLWE protocols. AI Red Team Simulations
Adversarial ML & Quantum-assisted attacks. Integration Layer: Unified workflow
for AI, cybersecurity, and quantum experts. Key Results 300+ vulnerabilities
identified across test environments. 70% reduction in high-severity issues
within 2 weeks. 90% resolution efficiency for blockchain-logged
vulnerabilities. Quantum-resistant cryptography maintained 100% integrity in
tests. Outcome: Quantum AI Security Protocol integrating Blockchain Quantum
Cryptography AI Red Teaming.

</details>


### [6] [A Probabilistic Computing Approach to the Closest Vector Problem for Lattice-Based Factoring](https://arxiv.org/abs/2510.19390)
*Max O. Al-Hasso,Marko von der Leyen*

Main category: cs.CR

TL;DR: 本文研究了概率计算在格基分解整数中的CVP近似优化任务的应用，设计了相关算法，展示了在问题规模上线性时间内实现最大CVP近似的有效性，并证明在选取特定格参数下，分解半素数所需的格实例数量可比其他方法减少100倍。


<details>
  <summary>Details</summary>
Motivation: 格中最短向量问题(CVP)是格基加密的基础，其硬度假设支撑加密系统的安全性。同时，Schnorr的格基分解算法将整数分解（如RSA的基石）归约为CVP。近期研究探索利用量子变分算法进行启发式优化（即CVP近似精炼步骤）以提升格基分解效率。随着概率计算作为针对组合优化等随机算法的硬件加速器的兴起，本文旨在研究概率计算如何应用于格基分解中的CVP近似精炼任务。

Method: 本文设计了专用于CVP近似精炼任务的概率计算算法。方法包括：（1）提出“格参数选择”（论文中称为prime lattice）的策略；（2）构建概率计算流程以高效执行优化任务；（3）通过与量子方法及经典方法对比，评估算法性能。

Result: 主要实验结果为：（a）所提方法在问题规模上线性时间内即可找到最佳可用CVP近似精炼解；（b）结合特定格参数，概率计算在分解半素数时所需要的格实例数量比同类量子方法和经典方法最多减少100倍。

Conclusion: 研究证明概率计算可高效解决格基分解中的CVP近似优化任务，大幅减少所需计算资源。该方法在特定参数下显著优于现有量子与经典方法，为格基算法的硬件加速提供了新方向。

Abstract: The closest vector problem (CVP) is a fundamental optimization problem in
lattice-based cryptography and its conjectured hardness underpins the security
of lattice-based cryptosystems. Furthermore, Schnorr's lattice-based factoring
algorithm reduces integer factoring (the foundation of current cryptosystems,
including RSA) to the CVP. Recent work has investigated the inclusion of a
heuristic CVP approximation `refinement' step in the lattice-based factoring
algorithm, using quantum variational algorithms to perform the heuristic
optimization. This coincides with the emergence of probabilistic computing as a
hardware accelerator for randomized algorithms including tasks in combinatorial
optimization. In this work we investigate the application of probabilistic
computing to the heuristic optimization task of CVP approximation refinement in
lattice-based factoring. We present the design of a probabilistic computing
algorithm for this task, a discussion of `prime lattice' parameters, and
experimental results showing the efficacy of probabilistic computing for
solving the CVP as well as its efficacy as a subroutine for lattice-based
factoring. The main results found that (a) this approach is capable of finding
the maximal available CVP approximation refinement in time linear in problem
size and (b) probabilistic computing used in conjunction with the lattice
parameters presented can find the composite prime factors of a semiprime number
using up to 100x fewer lattice instances than similar quantum and classical
methods.

</details>


### [7] [From See to Shield: ML-Assisted Fine-Grained Access Control for Visual Data](https://arxiv.org/abs/2510.19418)
*Mete Harun Akcay,Buse Gul Atli,Siddharth Prakash Rao,Alexandros Bakas*

Main category: cs.CR

TL;DR: 本文提出一个可扩展的信任数据共享系统架构，通过策略驱动的访问控制选择性保护敏感区域，结合自动检测、后校正、密钥管理和访问控制模块，使用混合加密方案（对称加密和基于属性的加密）确保数据安全，并在图像数据集上验证了系统在有效检测隐私敏感对象、提升性能指标（F1提高5%，mAP提高10%）和快速解密（平均每图1秒）方面的能力。


<details>
  <summary>Details</summary>
Motivation: 随着数据量增长，在大规模存储库中识别并保护敏感信息变得日益困难，尤其在多角色、多权限的共享场景下。为应对这一挑战，研究旨在设计一种可扩展的系统架构，实现基于策略的细粒度访问控制，确保敏感数据仅在授权下可访问。

Method: 系统整合四个核心模块：1）自动化敏感区域检测；2）后校正模块优化检测结果；3）密钥管理模块；4）策略驱动访问控制。采用混合加密方案：对称加密保证效率，基于属性的加密（ABE）实现策略执行。系统通过集中密钥存储隔离密钥资源以增强安全性。

Result: 在视觉数据集上测试表明：1）有效检测隐私敏感对象（PSO），宏观平均F1分数提升5%，平均精度均值（mAP）提升10%；2）策略驱动的解密平均耗时低于1秒/图像。

Conclusion: 所提架构在高效执行细粒度访问控制的同时，兼具可扩展性，为大规模多用户环境下的敏感数据保护提供有效解决方案。实验验证了其在性能指标和解密效率上的优势，证明系统适用于实际数据共享场景。

Abstract: As the volume of stored data continues to grow, identifying and protecting
sensitive information within large repositories becomes increasingly
challenging, especially when shared with multiple users with different roles
and permissions. This work presents a system architecture for trusted data
sharing with policy-driven access control, enabling selective protection of
sensitive regions while maintaining scalability. The proposed architecture
integrates four core modules that combine automated detection of sensitive
regions, post-correction, key management, and access control. Sensitive regions
are secured using a hybrid scheme that employs symmetric encryption for
efficiency and Attribute-Based Encryption for policy enforcement. The system
supports efficient key distribution and isolates key storage to strengthen
overall security. To demonstrate its applicability, we evaluate the system on
visual datasets, where Privacy-Sensitive Objects in images are automatically
detected, reassessed, and selectively encrypted prior to sharing in a data
repository. Experimental results show that our system provides effective PSO
detection, increases macro-averaged F1 score (5%) and mean Average Precision
(10%), and maintains an average policy-enforced decryption time of less than 1
second per image. These results demonstrate the effectiveness, efficiency and
scalability of our proposed solution for fine-grained access control.

</details>


### [8] [Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation](https://arxiv.org/abs/2510.19420)
*Chengcan Wu,Zhixin Zhang,Mingqian Xu,Zeming Wei,Meng Sun*

Main category: cs.CR

TL;DR: 本文提出了一个针对基于大型语言模型的多智能体系统（MAS）的动态防御范式，通过持续监控和动态调整图拓扑来防御复杂的动态攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统防御方法主要针对静态图结构，无法有效应对动态攻击，因此需要开发一种能够实时监控并调整图拓扑的动态防御机制。

Method: 提出了一种动态防御范式，包括持续监控MAS图结构内的通信，动态调整图拓扑，准确阻断恶意通信，以抵御不断演变的动态攻击。

Result: 在日益复杂和动态的MAS环境中进行实验，结果表明该方法显著优于现有的MAS防御机制。

Conclusion: 该方法为多智能体系统的可信应用提供了有效的保障，代码已在GitHub上开源。

Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have become a
popular paradigm of AI applications. However, trustworthiness issues in MAS
remain a critical concern. Unlike challenges in single-agent systems, MAS
involve more complex communication processes, making them susceptible to
corruption attacks. To mitigate this issue, several defense mechanisms have
been developed based on the graph representation of MAS, where agents represent
nodes and communications form edges. Nevertheless, these methods predominantly
focus on static graph defense, attempting to either detect attacks in a fixed
graph structure or optimize a static topology with certain defensive
capabilities. To address this limitation, we propose a dynamic defense paradigm
for MAS graph structures, which continuously monitors communication within the
MAS graph, then dynamically adjusts the graph topology, accurately disrupts
malicious communications, and effectively defends against evolving and diverse
dynamic attacks. Experimental results in increasingly complex and dynamic MAS
environments demonstrate that our method significantly outperforms existing MAS
defense mechanisms, contributing an effective guardrail for their trustworthy
applications. Our code is available at
https://github.com/ChengcanWu/Monitoring-LLM-Based-Multi-Agent-Systems.

</details>


### [9] [Cross-Chain Sealed-Bid Auctions Using Confidential Compute Blockchains](https://arxiv.org/abs/2510.19491)
*Jonas Gebele,Timm Mutzel,Burak Oez,Florian Matthes*

Main category: cs.CR

TL;DR: Sealed-Bid Auction on TEE-backed Confidential Blockchain


<details>
  <summary>Details</summary>
Motivation: 传统密封拍卖存在依赖不可信中心化架构及隐私保护问题，区块链透明性与拍买机密性冲突。解决现有方案在隐私、可验证性、可扩展性三难问题上无法避免可信第三方、复杂多轮协议或成本高问题。

Method: 利用带TEE的可信执行环境机密计算区块链处理敏感竞价逻辑，保留公链结算。投标人向enclave生成托管地址注入资金形成保密绑定承诺；任意方可在截止后触发链下验证结算。

Result: 设计实现安全、机密、可扩展无需信赖第三方或协议变动的系统；于SUAVE以太坊结算实践验证可行性和信任假设评估表明可在现有设施上微改造部署。

Conclusion: 基于TEE与公链结算混合架构实现隐私保障且可扩展密封拍卖方案。突破隐私、可验证及扩展三难问题局限。

Abstract: Sealed-bid auctions ensure fair competition and efficient allocation but are
often deployed on centralized infrastructure, enabling opaque manipulation.
Public blockchains eliminate central control, yet their inherent transparency
conflicts with the confidentiality required for sealed bidding. Prior attempts
struggle to reconcile privacy, verifiability, and scalability without relying
on trusted intermediaries, multi-round protocols, or expensive cryptography. We
present a sealed-bid auction protocol that executes sensitive bidding logic on
a Trusted Execution Environment (TEE)-backed confidential compute blockchain
while retaining settlement and enforcement on a public chain. Bidders commit
funds to enclave-generated escrow addresses, ensuring confidentiality and
binding commitments. After the deadline, any party can trigger resolution: the
confidential blockchain determines the winner through verifiable off-chain
computation and issues signed settlement transactions for execution on the
public chain. Our design provides security, privacy, and scalability without
trusted third parties or protocol modifications. We implement it on SUAVE with
Ethereum settlement, evaluate its scalability and trust assumptions, and
demonstrate deployment with minimal integration on existing infrastructure

</details>


### [10] [CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against IP Leakage](https://arxiv.org/abs/2510.19676)
*Nowfel Mashnoor,Mohammad Akyash,Hadi Kamali,Kimia Azar*

Main category: cs.CR

TL;DR: 针对大型语言模型（LLMs）在RTL硬件合成中可能出现的记忆性问题（导致专有设计泄露），研究提出了一种防御策略CircuitGuard。该策略通过新型RTL感知相似度指标和激活级干预方法，在保持生成质量前提下显著降低语义相似度，并具备跨领域适应性。


<details>
  <summary>Details</summary>
Motivation: LLMs在硬件描述语言（RTL）生成任务中存在记忆敏感训练数据的风险，传统自然语言记忆检测方法无法应对RTL的结构多样性与行为敏感性。需要开发针对性防御机制以平衡信息泄漏防护与生成正确性。

Method: 1) 提出RTL感知相似度指标：衡量结构/功能等效性而不依赖表面语法重叠；2) 设计激活级干预：识别并抑制Transformer模型中导致记忆的关键组件（聚焦18-28层275个特征）。

Result: 在Llama 3.1-8B模型上实现：对专有模式的语义相似度降低80%，跨电路类别迁移有效性达78-85%，且保持代码生成质量。

Conclusion: CircuitGuard能有效隔离关键记忆特征，显著降低IP泄露风险，同时维持功能正确性，为安全化硬件设计生成提供解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable success in generative
tasks, including register-transfer level (RTL) hardware synthesis. However,
their tendency to memorize training data poses critical risks when proprietary
or security-sensitive designs are unintentionally exposed during inference.
While prior work has examined memorization in natural language, RTL introduces
unique challenges: In RTL, structurally different implementations (e.g.,
behavioral vs. gate-level descriptions) can realize the same hardware, leading
to intellectual property (IP) leakage (full or partial) even without verbatim
overlap. Conversely, even small syntactic variations (e.g., operator precedence
or blocking vs. non-blocking assignments) can drastically alter circuit
behavior, making correctness preservation especially challenging. In this work,
we systematically study memorization in RTL code generation and propose
CircuitGuard, a defense strategy that balances leakage reduction with
correctness preservation. CircuitGuard (1) introduces a novel RTL-aware
similarity metric that captures both structural and functional equivalence
beyond surface-level overlap, and (2) develops an activation-level steering
method that identifies and attenuates transformer components most responsible
for memorization. Our empirical evaluation demonstrates that CircuitGuard
identifies (and isolates) 275 memorization-critical features across layers
18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic
similarity to proprietary patterns while maintaining generation quality.
CircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling
robust memorization mitigation across circuit categories without retraining.

</details>
