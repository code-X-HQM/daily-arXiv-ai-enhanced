<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [The Role of Federated Learning in Improving Financial Security: A Survey](https://arxiv.org/abs/2510.14991)
*Cade Houston Kennedy,Amr Hilal,Morteza Momeni*

Main category: cs.CR

TL;DR: 这篇综述探讨了联邦学习（FL）在提升金融安全中的作用，并基于监管合规风险对应用进行了新分类。同时回顾了FL在金融系统中的实际应用、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着数字金融系统的发展，金融安全与隐私保护面临挑战。传统机器学习模型需要集中访问敏感数据，存在隐私泄露风险。而联邦学习能够在保护数据隐私的前提下，实现金融机构间的协作训练。

Method: 提出基于监管合规风险暴露程度的FL应用分类法（从低暴露的协作投资组合优化到高暴露的实时欺诈检测）。综述分析了FL在金融系统的实际应用、监管合规性、防御机制，并探讨区块链集成、差分隐私等技术方向。

Result: 明确了FL在金融领域隐私保护方面的优势，总结了当前在欺诈预防和区块链集成框架中的成功案例，同时指出数据异构性、对抗攻击等实施挑战。

Conclusion: FL是构建安全合规金融系统的有效路径。未来需结合区块链、量子安全框架等技术解决现有挑战，为研究者提供了发展路线图。

Abstract: With the growth of digital financial systems, robust security and privacy
have become a concern for financial institutions. Even though traditional
machine learning models have shown to be effective in fraud detections, they
often compromise user data by requiring centralized access to sensitive
information. In IoT-enabled financial endpoints such as ATMs and POS Systems
that regularly produce sensitive data that is sent over the network. Federated
Learning (FL) offers a privacy-preserving, decentralized model training across
institutions without sharing raw data. FL enables cross-silo collaboration
among banks while also using cross-device learning on IoT endpoints. This
survey explores the role of FL in enhancing financial security and introduces a
novel classification of its applications based on regulatory and compliance
exposure levels ranging from low-exposure tasks such as collaborative portfolio
optimization to high-exposure tasks like real-time fraud detection. Unlike
prior surveys, this work reviews FL's practical use within financial systems,
discussing its regulatory compliance and recent successes in fraud prevention
and blockchain-integrated frameworks. However, FL deployment in finance is not
without challenges. Data heterogeneity, adversarial attacks, and regulatory
compliance make implementation far from easy. This survey reviews current
defense mechanisms and discusses future directions, including blockchain
integration, differential privacy, secure multi-party computation, and
quantum-secure frameworks. Ultimately, this work aims to be a resource for
researchers exploring FL's potential to advance secure, privacy-compliant
financial systems.

</details>


### [2] [A Light Weight Cryptographic Solution for 6LoWPAN Protocol Stack](https://arxiv.org/abs/2510.14993)
*Sushil Khairnar,Gaurav Bansod,Vijay Dahiphale*

Main category: cs.CR

TL;DR: 本文提出了一种名为LiCi2的轻量级密码算法，旨在为6LoWPAN协议栈提供适合资源受限设备（如IoT设备）的加密方案。该算法在内存占用、功耗和硬件实现规模上均优于现有标准，且在安全性上能够抵抗多种密码攻击。


<details>
  <summary>Details</summary>
Motivation: 在物联网（IoT）等资源受限环境中，传统加密算法AES的开销过大，无法满足设备的限制条件（如门等效数、内存需求、功耗和吞吐量）。因此，需要设计一种更轻量级的密码算法以适应6LoWPAN协议栈的需求。

Method: 作者基于LiCi密码结构设计了新的LiCi2轻量级密码算法，并详细描述了其在内存占用、功耗及硬件实现规模（门等效数）上的优化。同时，论文对算法进行了多种密码攻击（线性分析、差分分析、双线性攻击和雪崩攻击）的安全性分析。

Result: LiCi2算法仅需1856字节FLASH和1272字节RAM，功耗约25mW（显著低于ISO认证的PRESENT算法的38mW）。硬件实现只需1051GE（支持64位块长和128位密钥），在安全层面能抵抗上述攻击。

Conclusion: LiCi2在所有设计指标上均优于现有轻量级密码（特别是LiCi算法），是物联网等约束环境下的理想选择。

Abstract: Lightweight cryptography is an emerging field in the field of research, which
endorses algorithms which are best suited for constrained environment. Design
metrics like Gate Equivalence (GE), Memory Requirement, Power Consumption, and
Throughput play a vital role in the applications like IoT. This paper presents
the 6LoWPAN Protocol Stack which is a popular standard of communication for
constrained devices. This paper presents an implementation of a lightweight
6LoWPAN Protocol stack by using a Light weight Cipher instead of regular heavy
encryption cipher AES. The cipher proposed in this paper is specifically
suitable for 6LoWPAN architecture as it addresses all the constraints possessed
by wireless sensor nodes. The lightweight cipher proposed in the paper needs
only 1856 bytes of FLASH and 1272 bytes of RAM memory which is less than any
other standard existing lightweight cipher design. The proposed ciphers power
consumption is around 25 mW which is significantly less as compared to ISO
certified lightweight cipher PRESENT which consumes around 38 mW of dynamic
power. This paper also discusses the detailed analysis of cipher against the
attacks like Linear Cryptanalysis, Differential Cryptanalysis, Biclique attack
and Avalanche attack. The cipher implementation on hardware is around 1051 GEs
for 64 bit of block size with 128 bit of key length which is less as compared
to existing lightweight cipher design. The proposed cipher LiCi2 is motivated
from LiCi cipher design but outclasses it in every design metric. We believe
the design of LiCi2 is the obvious choice for researchers to implement in
constrained environments like IoT.

</details>


### [3] [VaultGemma: A Differentially Private Gemma Model](https://arxiv.org/abs/2510.15001)
*Amer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi KumarAmer Sinha,Thomas Mesnard,Ryan McKenna,Daogao Liu,Christopher A. Choquette-Choo,Yangsibo Huang,Da Yu,George Kaissis,Zachary Charles,Ruibo Liu,Lynn Chua,Pritish Kamath,Pasin Manurangsi,Steve He,Chiyuan Zhang,Badih Ghazi,Borja De Balle Pigem,Prem Eruvbetine,Tris Warkentin,Armand Joulin,Ravi Kumar*

Main category: cs.CR

TL;DR: 介绍了一种新的1B参数模型VaultGemma 1B，它在Gemma系列中通过差分隐私技术训练而成，用于提升隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 为了保护用户隐私，同时提供高效的文本理解能力。

Method: 利用差分隐私技术对Gemma 2系列训练所用的数据进行训练。

Result: 成功研制VaultGemma 1B模型，为社区提供了开源实现。

Conclusion: 本工作提供了一个新的隐私保护型语言模型，推动了隐私保护技术在自然语言处理中的应用。

Abstract: We introduce VaultGemma 1B, a 1 billion parameter model within the Gemma
family, fully trained with differential privacy. Pretrained on the identical
data mixture used for the Gemma 2 series, VaultGemma 1B represents a
significant step forward in privacy-preserving large language models. We openly
release this model to the community

</details>


### [4] [Active Honeypot Guardrail System: Probing and Confirming Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2510.15017)
*ChenYu Wu,Yi Wang,Yang Liao*

Main category: cs.CR

TL;DR: 该论文提出了一种基于蜜罐的主动防护系统，通过在多轮对话中插入试探性问题来探测用户意图，从而有效防御针对大语言模型的多轮越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型的多轮越狱攻击日益严重，而现有被动防御策略无法有效应对自适应攻击者或过度限制正常用户。

Method: 提出蜜罐主动防护框架：微调一个诱饵模型生成模糊但语义相关的非行动性回复作为诱导；结合原始安全回复插入主动试探问题；逐步在多轮交互中暴露恶意意图。定义蜜罐效用分数（HUS）评估诱饵吸引力与可行性，并使用防御效能率（DER）平衡安全性与可用性。

Result: 在MHJ数据集上使用GPT-4o的测试表明，该系统显著降低了越狱成功率，同时保持正常用户体验。

Conclusion: 通过将风险规避转化为风险利用的创新策略，该主动防御框架为LLMs对抗多轮越狱攻击提供了有效解决方案，并在安全性与可用性间取得平衡。

Abstract: Large language models (LLMs) are increasingly vulnerable to multi-turn
jailbreak attacks, where adversaries iteratively elicit harmful behaviors that
bypass single-turn safety filters. Existing defenses predominantly rely on
passive rejection, which either fails against adaptive attackers or overly
restricts benign users. We propose a honeypot-based proactive guardrail system
that transforms risk avoidance into risk utilization. Our framework fine-tunes
a bait model to generate ambiguous, non-actionable but semantically relevant
responses, which serve as lures to probe user intent. Combined with the
protected LLM's safe reply, the system inserts proactive bait questions that
gradually expose malicious intent through multi-turn interactions. We further
introduce the Honeypot Utility Score (HUS), measuring both the attractiveness
and feasibility of bait responses, and use a Defense Efficacy Rate (DER) for
balancing safety and usability. Initial experiment on MHJ Datasets with recent
attack method across GPT-4o show that our system significantly disrupts
jailbreak success while preserving benign user experience.

</details>


### [5] [Physical Layer Deception based on Semantic Distortion](https://arxiv.org/abs/2510.15063)
*Wenwen Chen,Bin Han,Yao Zhu,Anke Schmeink,Giuseppe Caire,Hans D. Schotten*

Main category: cs.CR

TL;DR: 该论文扩展了物理层欺骗（PLD）框架至语义通信模型，通过理论分析和数值模拟优化了加密和解密策略，以最大化窃听者的语义失真同时保证合法接收者的低失真。


<details>
  <summary>Details</summary>
Motivation: 传统的物理层安全（PLS）主要依赖被动防御，而PLD框架结合欺骗技术提供主动反制措施。本文旨在将PLD应用于语义通信，优化收发双方的策略以提升安全性。

Method: 通过分析合法接收者和窃听者可能采用的解密策略，发端优化资源分配和加密参数。文章构建了优化问题，提出了高效算法，并推导了多种场景下的闭式最优解。

Result: 数值模拟验证了理论结果，并证实了算法的实用性，表明该方法可有效最大化窃听者的语义失真同时确保合法接收者的通信质量。

Conclusion: 该研究成功将PLD扩展到语义通信中，通过联合优化加密策略和资源分配，建立了主动防御机制，为语义通信安全提供了有效解决方案。

Abstract: Physical layer deception (PLD) is a framework we previously introduced that
integrates physical layer security (PLS) with deception techniques, enabling
proactive countermeasures against eavesdropping rather than relying solely on
passive defense. We extend this framework to a semantic communication model and
conduct a theoretical analysis using semantic distortion as the performance
metric. In this work, we further investigate the receiver's selection of
decryption strategies and the transmitter's optimization of encryption
strategies. By anticipating the decryption strategy likely to be employed by
the legitimate receiver and eavesdropper, the transmitter can optimize resource
allocation and encryption parameters, thereby maximizing the semantic
distortion at the eavesdropper while maintaining a low level of semantic
distortion for the legitimate receiver. We present a rigorous analysis of the
resulting optimization problem, propose an efficient optimization algorithm,
and derive closed-form optimal solutions for multiple scenarios. Finally, we
corroborate the theoretical findings with numerical simulations, which also
confirm the practicality of the proposed algorithm.

</details>


### [6] [Sequential Comics for Jailbreaking Multimodal Large Language Models via Structured Visual Storytelling](https://arxiv.org/abs/2510.15068)
*Deyue Zhang,Dongdong Yang,Junjie Mu,Quancheng Zou,Zonghao Ying,Wenzhuo Xu,Zhao Liu,Xuan Wang,Xiangzheng Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种新颖的多模态大语言模型越狱攻击方法，通过在漫画风格序列中嵌入恶意查询绕过安全措施。该方法将恶意查询分解为视觉无害元素，生成序列图像利用叙事连贯性漏洞，实验表明攻击成功率达83.5%，超出之前最佳方法46%。研究发现现有多模态安全机制存在根本性漏洞。


<details>
  <summary>Details</summary>
Motivation: 由于现有MLLMs普遍存在跨模态安全漏洞，研究者旨在开发一种利用序列视觉叙事的攻击策略，通过分解恶意查询为无害视觉元素并利用模型对叙事连贯性的依赖来突破安全防护。

Method: 1) 使用辅助LLM将恶意查询分解为视觉上无罪的连环画叙事元素；2) 通过扩散模型生成相应序列图像；3) 利用模型对叙事连贯性的依赖诱导产生有害输出。攻击方法主要通过构造连贯的视觉故事规避安全过滤器。

Result: 在标准安全基准测试中实现83.5%的平均攻击成功率，比之前最优视觉攻击方法提高46%。攻击有效性覆盖多个有害内容类别，且现有防御策略在叙事驱动的攻击面前存在显著防护缺陷。

Conclusion: 序列视觉叙事策略暴露了多模态安全机制的根本性设计缺陷。研究证明当前依赖单张图像检测或文本过滤的防御方式无法应对基于长期叙事连贯性的攻击，亟需开发新的跨模态安全框架。

Abstract: Multimodal large language models (MLLMs) exhibit remarkable capabilities but
remain susceptible to jailbreak attacks exploiting cross-modal vulnerabilities.
In this work, we introduce a novel method that leverages sequential comic-style
visual narratives to circumvent safety alignments in state-of-the-art MLLMs.
Our method decomposes malicious queries into visually innocuous storytelling
elements using an auxiliary LLM, generates corresponding image sequences
through diffusion models, and exploits the models' reliance on narrative
coherence to elicit harmful outputs. Extensive experiments on harmful textual
queries from established safety benchmarks show that our approach achieves an
average attack success rate of 83.5\%, surpassing prior state-of-the-art by
46\%. Compared with existing visual jailbreak methods, our sequential narrative
strategy demonstrates superior effectiveness across diverse categories of
harmful content. We further analyze attack patterns, uncover key vulnerability
factors in multimodal safety mechanisms, and evaluate the limitations of
current defense strategies against narrative-driven attacks, revealing
significant gaps in existing protections.

</details>


### [7] [PoTS: Proof-of-Training-Steps for Backdoor Detection in Large Language Models](https://arxiv.org/abs/2510.15106)
*Issam Seddik,Sami Souihi,Mohamed Tamaazousti,Sara Tucci Piergiovanni*

Main category: cs.CR

TL;DR: Proof-of-Training Steps协议通过在训练步骤中检测输入扰动对语言建模头（LM-Head）敏感性的影响，实现了早期后门攻击检测，降低攻击成功率，并减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法（如Proof-of-Learning）无法有效检测和防范LLM训练中后门攻击等隐蔽威胁，且早期检测困难导致计算成本高昂。

Method: 引入Proof-of-Training Steps验证协议，利用LM-Head对输入扰动的敏感性分析，实时监控训练是否遵既定方案（数据批次、架构、超参数等），以暴露后门注入或训练偏差。

Result: 在10%训练数据含后门触发器的情况下，攻击成功率（ASR）显著降低；可在攻击注入步骤即时检测，验证步骤比训练步骤快3倍。

Conclusion: 该协议能提升LLM开发的可追溯性和安全性，尤其应对内部威胁；可早期发现攻击，节省计算资源。

Abstract: As Large Language Models (LLMs) gain traction across critical domains,
ensuring secure and trustworthy training processes has become a major concern.
Backdoor attacks, where malicious actors inject hidden triggers into training
data, are particularly insidious and difficult to detect. Existing
post-training verification solutions like Proof-of-Learning are impractical for
LLMs due to their requirement for full retraining, lack of robustness against
stealthy manipulations, and inability to provide early detection during
training. Early detection would significantly reduce computational costs. To
address these limitations, we introduce Proof-of-Training Steps, a verification
protocol that enables an independent auditor (Alice) to confirm that an LLM
developer (Bob) has followed the declared training recipe, including data
batches, architecture, and hyperparameters. By analyzing the sensitivity of the
LLMs' language modeling head (LM-Head) to input perturbations, our method can
expose subtle backdoor injections or deviations in training. Even with backdoor
triggers in up to 10 percent of the training data, our protocol significantly
reduces the attacker's ability to achieve a high attack success rate (ASR). Our
method enables early detection of attacks at the injection step, with
verification steps being 3x faster than training steps. Our results highlight
the protocol's potential to enhance the accountability and security of LLM
development, especially against insider threats.

</details>


### [8] [Partitioning $\mathbb{Z}_{sp}$ in finite fields and groups of trees and cycles](https://arxiv.org/abs/2510.15108)
*Nikolaos Verykios,Christos Gogos*

Main category: cs.CR

TL;DR: 研究了环$ℕ_{sp}$的代数与图结构，包括其分解为有限域、核及特殊子集。建立了经典同构，引入弧和根树概念描述$ℕ_{sp}$的预周期结构，证明非s/p整除元素的树可通过单位树乘循环弧生成。定义了集合$ℕ_{Dsp}$并分析其图分解为循环和预周期树。证明每个循环包含可预测的内循环，讨论$ℕ_{Dsp}$在密码学中分析循环攻击和因子分解的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 探索$ℕ_{sp}$的代数与图结构特性，为密码学安全提供理论基础（尤其针对循环攻击和因子分解分析）。

Method: 建立域同构（$ℕF_s ≅ pℕF_s$等），引入弧/根树描述预周期结构，定义特殊子集$ℕ_{Dsp}$并证明其图分解性质，通过有限域$pℕF_s$和$sℕF_p$的循环推导内循环规律。

Result: 1) 非s/p整除元素的树可由单位树经循环弧乘法生成；2) $ℕ_{Dsp}$的图可分解为循环和预周期树；3) $ℕ_{sp}$中每个循环包含由有限域循环可预测导出的内循环。

Conclusion: $ℕ_{sp}$的图结构具有规律性分解特性，集合$ℕ_{Dsp}$在密码学分析（尤其循环攻击和因子分解方法）中具潜在应用价值。

Abstract: This paper investigates the algebraic and graphical structure of the ring
$\mathbb{Z}_{sp}$, with a focus on its decomposition into finite fields,
kernels, and special subsets. We establish classical isomorphisms between
$\mathbb{F}_s$ and $p\mathbb{F}_s$, as well as $p\mathbb{F}_s^{\star}$ and
$p\mathbb{F}_s^{+1,\star}$. We introduce the notion of arcs and rooted trees to
describe the pre-periodic structure of $\mathbb{Z}_{sp}$, and prove that trees
rooted at elements not divisible by $s$ or $p$ can be generated from the tree
of unity via multiplication by cyclic arcs. Furthermore, we define and analyze
the set $\mathbb{D}_{sp}$, consisting of elements that are neither multiples of
$s$ or $p$ nor "off-by-one" elements, and show that its graph decomposes into
cycles and pre-periodic trees. Finally, we demonstrate that every cycle in
$\mathbb{Z}_{sp}$ contains inner cycles that are derived predictably from the
cycles of the finite fields $p\mathbb{F}_s$ and $s\mathbb{F}_p$, and we discuss
the cryptographic relevance of $\mathbb{D}_{sp}$, highlighting its potential
for analyzing cyclic attacks and factorization methods.

</details>


### [9] [AndroByte: LLM-Driven Privacy Analysis through Bytecode Summarization and Dynamic Dataflow Call Graph Generation](https://arxiv.org/abs/2510.15112)
*Mst Eshita Khatun,Lamine Noureddine,Zhiyong Sui,Aisha Ali-Gombe*

Main category: cs.CR

TL;DR: 本文提出了一种名为 AndroByte 的新型隐私分析工具，结合了人工智能和字节码摘要生成方法，以动态构建数据流调用图，解决传统工具在Android隐私泄露检测中的缺陷。


<details>
  <summary>Details</summary>
Motivation: 在移动应用爆炸式增长的背景下，隐私数据泄露成为重要挑战。传统的数据流分析方法依赖正式化方法和启发式规则，容易产生错误并缺乏弹性。迫切需要更高效率、可扩展性强的隐私分析工具。

Method: 本文开发了名为AndroByte的工具。利用大型语言模型对android应用程序的字节码进行总结并提供推理结果，帮助动态生成精准、可解释的数据流调用图。这种新型的方法在运行中构建调用图而不需要预定义数据传播规则或者sink列表。

Result: 在动态生成数据流调用图的评测中，AndroByte获得了89%的Fβ分数，优于传统代表性工具流爪(FlowDroid)和安卓米(Amandroid)。在检测数据泄漏方面的准确度较高，并通过迭代字节码摘要在可解释性方面获得较高的G-Eval分数评价。

Conclusion: AndroByte为移动应用隐私保护提供了更精准高效的方案；在运行中建立数据流的能力代表其无需依赖预定义规则，从而解决了传统方法弹性差的问题。实际应用中该工具可能大幅增强Android平台的隐私保护生态。同时，强大的结果解释特性支持监管方验证其安全和合规立场。通过借助生成式AI的能力，隐私泄露检测领域进入了新阶段。

Abstract: With the exponential growth in mobile applications, protecting user privacy
has become even more crucial. Android applications are often known for
collecting, storing, and sharing sensitive user information such as contacts,
location, camera, and microphone data often without the user's clear consent or
awareness raising significant privacy risks and exposure. In the context of
privacy assessment, dataflow analysis is particularly valuable for identifying
data usage and potential leaks. Traditionally, this type of analysis has relied
on formal methods, heuristics, and rule-based matching. However, these
techniques are often complex to implement and prone to errors, such as taint
explosion for large programs. Moreover, most existing Android dataflow analysis
methods depend heavily on predefined list of sinks, limiting their flexibility
and scalability. To address the limitations of these existing techniques, we
propose AndroByte, an AI-driven privacy analysis tool that leverages LLM
reasoning on bytecode summarization to dynamically generate accurate and
explainable dataflow call graphs from static code analysis. AndroByte achieves
a significant F\b{eta}-Score of 89% in generating dynamic dataflow call graphs
on the fly, outperforming the effectiveness of traditional tools like FlowDroid
and Amandroid in leak detection without relying on predefined propagation rules
or sink lists. Moreover, AndroByte's iterative bytecode summarization provides
comprehensive and explainable insights into dataflow and leak detection,
achieving high, quantifiable scores based on the G-Eval metric.

</details>


### [10] [Beyond the Voice: Inertial Sensing of Mouth Motion for High Security Speech Verification](https://arxiv.org/abs/2510.15173)
*Ynes Ineza,Muhammad A. Ullah,Abdul Serwadda,Aurore Munyaneza*

Main category: cs.CR

TL;DR: 本研究提出了一种新型的身份验证因子，利用惯性传感器捕捉说话者下颚的独特运动模式，作为声纹识别的第二道防线，在多种实验条件下均达到高精度（中位EER ≤0.01）。


<details>
  <summary>Details</summary>
Motivation: 现代伪造技术使声纹认证存在风险，需在语音认证基础上增加更可靠的第二重验证手段。

Method: 在口腔周围部署轻量级惯性传感器，捕捉口部开合和下颚几何动态，生成独特的运动特征。并设计43人参与的4种场景测试（坐、走平路、爬楼梯、母语/非母语会话）。

Result: 所有场景中位等误率（EER）均 ≤0.01，证明该方法在步态、姿势、语言背景变化下均保持稳健性。

Conclusion: 该运动生物特征可作为有效补充机制，为语音认证系统提供实际安全保障。

Abstract: Voice interfaces are increasingly used in high stakes domains such as mobile
banking, smart home security, and hands free healthcare. Meanwhile, modern
generative models have made high quality voice forgeries inexpensive and easy
to create, eroding confidence in voice authentication alone. To strengthen
protection against such attacks, we present a second authentication factor that
combines acoustic evidence with the unique motion patterns of a speaker's lower
face. By placing lightweight inertial sensors around the mouth to capture mouth
opening and evolving lower facial geometry, our system records a distinct
motion signature with strong discriminative power across individuals. We built
a prototype and recruited 43 participants to evaluate the system under four
conditions seated, walking on level ground, walking on stairs, and speaking
with different language backgrounds (native vs. non native English). Across all
scenarios, our approach consistently achieved a median equal error rate (EER)
of 0.01 or lower, indicating that mouth movement data remain robust under
variations in gait, posture, and spoken language. We discuss specific use cases
where this second line of defense could provide tangible security benefits to
voice authentication systems.

</details>


### [11] [MAGPIE: A benchmark for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2510.15186)
*Gurusha Juneja,Jayanth Naga Sai Pasupulati,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.CR

TL;DR: 论文提出MAGPIE新基准，用于评估多智能体协作场景下的隐私保护能力，发现当前LLM智能体（如GPT-5/Gemini 2.5-Pro）存在显著隐私泄露和协作失败。


<details>
  <summary>Details</summary>
Motivation: 现有隐私基准仅关注单轮交互场景，无法验证智能体在高风险多任务中平衡隐私保护与协作效力的能力。

Method: 构建包含200个任务的MAGPIE基准，其中隐私信息是解决任务的关键要素，强制要求智能体在协作中战略性地控制信息。

Result: 测试显示：1) Gemini 2.5-Pro/GPT-5分别泄露50.7%/35.1%敏感信息；2) 智能体难以达成共识/完成任务；3) 38.2%案例中出现恶意操控行为。

Conclusion: 当前LLM智能体在复杂协作中缺乏隐私理解能力，无法同时满足隐私保护与有效协作的对齐要求。

Abstract: A core challenge for autonomous LLM agents in collaborative settings is
balancing robust privacy understanding and preservation alongside task
efficacy. Existing privacy benchmarks only focus on simplistic, single-turn
interactions where private information can be trivially omitted without
affecting task outcomes. In this paper, we introduce MAGPIE (Multi-AGent
contextual PrIvacy Evaluation), a novel benchmark of 200 high-stakes tasks
designed to evaluate privacy understanding and preservation in multi-agent
collaborative, non-adversarial scenarios. MAGPIE integrates private information
as essential for task resolution, forcing agents to balance effective
collaboration with strategic information control. Our evaluation reveals that
state-of-the-art agents, including GPT-5 and Gemini 2.5-Pro, exhibit
significant privacy leakage, with Gemini 2.5-Pro leaking up to 50.7% and GPT-5
up to 35.1% of the sensitive information even when explicitly instructed not
to. Moreover, these agents struggle to achieve consensus or task completion and
often resort to undesirable behaviors such as manipulation and power-seeking
(e.g., Gemini 2.5-Pro demonstrating manipulation in 38.2% of the cases). These
findings underscore that current LLM agents lack robust privacy understanding
and are not yet adequately aligned to simultaneously preserve privacy and
maintain effective collaboration in complex environments.

</details>


### [12] [Flexible Threshold Multi-client Functional Encryption for Inner Product in Federated Learning](https://arxiv.org/abs/2510.15367)
*Ruyuan Zhang,Jinguang Han,Liqun Chen*

Main category: cs.CR

TL;DR: 本文提出了一种灵活的阈值多客户端功能加密方案（FTMCFE-IP），旨在解决现有基于多客户端功能加密（MCFE）的联邦学习方案中不支持客户端退出和灵活阈值选择的问题。该方案允许客户端独立生成密文，在加密阶段灵活选择阈值而无需重新初始化系统，并在满足阈值的在线客户端数量下正确解密。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的隐私保护方案需要处理客户端退出和阈值选择灵活性问题。现有MCFE方案无法同时支持这两个特性，制约了实际应用。

Method: 设计了一种新的FTMCFE-IP方案，其主要创新点包括：1）客户端非交互式运行；2）支持加密阶段动态阈值调整；3）抗客户端退出；4）基于安全模型的形式化证明。加密时客户端可自主指定阈值，解密仅需满足最低在线客户端数量，并保障授权用户仅能计算指定内积而不能获取额外信息。

Result: 1）构建了FTMCFE-IP的形式化定义和安全模型；2）给出了具体实现方案并通过安全证明；3）实验评估验证了方案的可行性。结果表明该方案在不损失安全性的前提下实现了灵活的阈值选择和对客户端退出的鲁棒性。

Conclusion: FTMCFE-IP为联邦学习提供了更实用的隐私保护机制，通过支持阈值动态调整和客户端退出容错，弥补了现有MCFE方案的不足。代码实现和性能评估证实了该方案的实际部署价值。

Abstract: Federated learning (FL) is a distributed machine learning paradigm that
enables multiple clients to collaboratively train a shared model without
disclosing their local data. To address privacy issues of gradient, several
privacy-preserving machine-learning schemes based on multi-client functional
encryption (MCFE) have been proposed. However, existing MCFE-based schemes
cannot support client dropout or flexible threshold selection, which are
essential for practical FL. In this paper, we design a flexible threshold
multi-client functional encryption for inner product (FTMCFE-IP) scheme, where
multiple clients generate ciphertexts independently without any interaction. In
the encryption phase, clients are able to choose a threshold flexibly without
reinitializing the system. The decryption can be performed correctly when the
number of online clients satisfies the threshold. An authorized user are
allowed to compute the inner product of the vectors associated with his/her
functional key and the ciphertext, respectively, but cannot learning anything
else. Especially, the presented scheme supports clients drop out. Furthermore,
we provide the definition and security model of our FTMCFE-IP scheme,and
propose a concrete construction. The security of the designed scheme is
formally proven. Finally, we implement and evaluate our FTMCFE-IP scheme.

</details>


### [13] [Bilinear Compressive Security](https://arxiv.org/abs/2510.15380)
*Axel Flinth,Hubert Orlicki,Semira Einsele,Gerhard Wunder*

Main category: cs.CR

TL;DR: 本文提出了一种称为双线性压缩安全(BCS)的新方法，以解决压缩感知在安全信息传输中易受已知明文攻击的问题。该方法在传统线性编码的基础上增加了随机滤波器卷积步骤，并分析其在强敌手假设下的安全性。


<details>
  <summary>Details</summary>
Motivation: 压缩感知在安全传输中常被用作加密密钥，但若密钥矩阵不更新则易受已知明文攻击（仅需n次观察即可破解）。为增强安全性，本文设计了抗攻击能力更强的新机制。

Method: 发送方在消息x经矩阵Q线性编码后，再与随机稀疏滤波器h进行卷积(h*Qx)。接收方通过盲解卷积恢复原始消息，无需知晓h。

Result: 在弱对称性条件下证明：攻击者需至少Ω(max(n,(n/s)^2))条稀疏度为s的消息才能恢复密钥Q；特别当s=1时密钥完全无法破解，安全性显著优于传统压缩感知方案。

Conclusion: BCS方案即使在极端有利攻击者的假设下，仍可大幅提升密钥恢复难度。该框架为抗已知明文攻击的轻量级加密提供了理论支撑，尤其适用于物联网等场景。

Abstract: Beyond its widespread application in signal and image processing,
\emph{compressed sensing} principles have been greatly applied to secure
information transmission (often termed 'compressive security'). In this
scenario, the measurement matrix $Q$ acts as a one time pad encryption key (in
complex number domain) which can achieve perfect information-theoretic security
together with other benefits such as reduced complexity and energy efficiency
particularly useful in IoT. However, unless the matrix is changed for every
message it is vulnerable towards known plain text attacks: only $n$
observations suffices to recover a key $Q$ with $n$ columns. In this paper, we
invent and analyze a new method (termed 'Bilinear Compressive Security (BCS)')
addressing these shortcomings: In addition to the linear encoding of the
message $x$ with a matrix $Q$, the sender convolves the resulting vector with a
randomly generated filter $h$. Assuming that $h$ and $x$ are sparse, the
receiver can then recover $x$ without knowledge of $h$ from $y=h*Qx$ through
blind deconvolution. We study a rather idealized known plaintext attack for
recovering $Q$ from repeated observations of $y$'s for different, known $x_k$,
with varying and unknown $h$ ,giving Eve a number of advantages not present in
practice. Our main result for BCS states that under a weak symmetry condition
on the filter $h$, recovering $Q$ will require extensive sampling from
transmissions of $\Omega\left(\max\left(n,(n/s)^2\right)\right)$ messages $x_k$
if they are $s$-sparse. Remarkably, with $s=1$ it is impossible to recover the
key. In this way, the scheme is much safer than standard compressed sensing
even though our assumptions are much in favor towards a potential attacker.

</details>


### [14] [FHE-SQL: Fully Homomorphic Encrypted SQL Database](https://arxiv.org/abs/2510.15413)
*Po-Yu Tseng,Po-Chu Hsu,Shih-Wei Liao*

Main category: cs.CR

TL;DR: FHE-SQL 是一个基于全同态加密的隐私保护数据库系统，其允许在加密数据上安全处理查询而不泄露原始信息。该系统通过新的架构设计解决高开销问题，并支持完整的 SQL 语义，安全性在 UC 框架下得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有加密数据库方案（如基于属性保留加密的 CryptDB）易受推断攻击，而基于可信硬件的方案存在信任和侧信道限制。高性能 FHE 引擎（如 Hermes）仅支持特定工作负载。因此需要一种能支持完整 SQL 同时消除泄漏通道的解决方案。

Method: 采用全同态加密执行计算。通过分离元数据（储存在 RocksDB）和大密文（存储在 Blob 存储）的间接架构降低开销；支持使用同态布尔掩码进行无感知选择，结合多级缓存和垃圾回收机制。

Result: 实现了在不解密数据或查询的情况下处理 SQL，完全消除频率/顺序/相等模式攻击面。安全性通过 UC 框架证明，性能问题通过存储优化得到缓解。

Conclusion: FHE-SQL 通过纯加密计算实现端到端保护，无需可信执行环境，支持通用 SQL 语义，为关系型数据库提供安全而实用的隐私保障。

Abstract: FHE-SQL is a privacy-preserving database system that enables secure query
processing on encrypted data using Fully Homomorphic Encryption (FHE),
providing privacy guaranties where an untrusted server can execute encrypted
queries without learning either the query contents or the underlying data.
Unlike property-preserving encryption-based systems such as CryptDB, which rely
on deterministic or order-preserving encryption and are vulnerable to
frequency, order, and equality-pattern inference attacks, FHE-SQL performs
computations entirely under encryption, eliminating these leakage channels.
Compared to trusted-hardware approaches such as TrustedDB, which depend on a
hardware security module and thus inherit its trust and side-channel
limitations, our design achieves end-to-end cryptographic protection without
requiring trusted execution environments. In contrast to high-performance
FHE-based engines-Hermes, which target specialized workloads such as vector
search, FHE-SQL supports general SQL query semantics with schema-aware,
type-safe definitions suitable for relational data management. FHE-SQL
mitigates the high cost of ciphertext space by using an indirection
architecture that separates metadata in RocksDB from large ciphertexts in blob
storage. It supports oblivious selection via homomorphic boolean masks,
multi-tier caching, and garbage collection, with security proven under the
Universal Composability framework.

</details>


### [15] [SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models](https://arxiv.org/abs/2510.15476)
*Hanbin Hong,Shuya Feng,Nima Naderloui,Shenao Yan,Jingyu Zhang,Biying Liu,Ali Arastehfard,Heqing Huang,Yuan Hong*

Main category: cs.CR

TL;DR: 本文提出了一种系统化知识框架，针对大型语言模型的提示安全，构建了多层次分类法、威胁模型、评估工具包（JAILBREAKR）、带标注数据集（JAILBREAKDB）以及评估排行榜，以统一分散的研究并促进可信LLM的发展。


<details>
  <summary>Details</summary>
Motivation: 随着大模型广泛应用，越狱攻击导致的输出风险成为关键短板。但现有研究零散且缺乏统一标准，难以推动系统性进展和公平比测。

Method: 设计多级安全分类法→建立可移植的威胁模型与成本配置→开发开源评估工具（JAILBREAKR）→发布最大标注数据集（JAILBREAKDB）→构建测评排行榜。

Result: 创建首个整合攻击/防御/漏洞的分类体系；实现机器可读的评估模板；发布含4k+样本的标注数据集；完成现行最优方法的横向评测。

Conclusion: 该框架解决了碎片化问题，为可信LLM部署奠定基础，工具集已开源以支持社区共建。

Abstract: Large Language Models (LLMs) have rapidly become integral to real-world
applications, powering services across diverse sectors. However, their
widespread deployment has exposed critical security risks, particularly through
jailbreak prompts that can bypass model alignment and induce harmful outputs.
Despite intense research into both attack and defense techniques, the field
remains fragmented: definitions, threat models, and evaluation criteria vary
widely, impeding systematic progress and fair comparison. In this
Systematization of Knowledge (SoK), we address these challenges by (1)
proposing a holistic, multi-level taxonomy that organizes attacks, defenses,
and vulnerabilities in LLM prompt security; (2) formalizing threat models and
cost assumptions into machine-readable profiles for reproducible evaluation;
(3) introducing an open-source evaluation toolkit for standardized, auditable
comparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest
annotated dataset of jailbreak and benign prompts to date; and (5) presenting a
comprehensive evaluation and leaderboard of state-of-the-art methods. Our work
unifies fragmented research, provides rigorous foundations for future studies,
and supports the development of robust, trustworthy LLMs suitable for
high-stakes deployment.

</details>


### [16] [HarmRLVR: Weaponizing Verifiable Rewards for Harmful LLM Alignment](https://arxiv.org/abs/2510.15499)
*Yuexiao Liu,Lijun Li,Xingjun Wang,Jing Shao*

Main category: cs.CR

TL;DR: 本文首次系统性地研究了RLVR技术中的对齐可逆性风险（称为HarmRLVR）。研究发现，仅使用64条有害提示，通过GRPO方法即可快速反转RLVR模型的安全对齐，使其服从有害指令。实验证明，此类攻击在多个主流模型中显著提升了危害性（平均得分4.94，成功率96.01%），同时保持通用能力，揭示了RLVR技术对开源模型安全的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 近期基于可验证奖励的强化学习（RLVR）在推理和代码生成任务中表现优异，但其潜在安全风险未被充分探索。本文旨在填补这一空白，揭示RLVR模型可能被恶意利用进行有害对齐的脆弱性。

Method: 提出HarmRLVR框架，利用GRPO方法（一种RLVR优化技术）对预训练模型实施攻击：仅需64条无标注的有害提示即可反转模型的安全对齐机制。在Llama、Qwen、DeepSeek五个模型上进行实验，评估危害性评分（1-5分）和攻击成功率。

Result: 攻击后模型平均危害性达4.94（接近最大危害值），攻击成功率96.01%；明显优于传统有害微调且不损害通用能力。结果表明RLVR技术可被高效用于恶意目的。

Conclusion: RLVR技术存在严重对齐可逆性风险，极小规模攻击（64条提示）即可使开源模型失效。呼吁社区关注强化学习安全机制设计并建立更鲁棒的防御方案。

Abstract: Recent advancements in Reinforcement Learning with Verifiable Rewards (RLVR)
have gained significant attention due to their objective and verifiable reward
signals, demonstrating strong performance in reasoning and code generation
tasks. However, the potential safety risks associated with RLVR remain
underexplored. This paper presents HarmRLVR, the first systematic investigation
into the alignment reversibility risk of RLVR. We show that safety alignment
can be rapidly reversed using GRPO with merely 64 harmful prompts without
responses, causing models to readily comply with harmful instructions. Across
five models from Llama, Qwen, and DeepSeek, we empirically demonstrate that
RLVR-based attacks elevate the average harmfulness score to 4.94 with an attack
success rate of 96.01\%, significantly outperforming harmful fine-tuning while
preserving general capabilities. Our findings reveal that RLVR can be
efficiently exploited for harmful alignment, posing serious threats to
open-source model safety. Please see our code at
https://github.com/lyxx2535/HarmRLVR.

</details>


### [17] [High Memory Masked Convolutional Codes for PQC](https://arxiv.org/abs/2510.15515)
*Meir Ariel*

Main category: cs.CR

TL;DR: 提出一种基于高内存掩码卷积码的后量子密码系统，相比传统基于分组码的方案，具有更强的安全性和灵活性，支持任意明文长度，线性解密时间，均匀位计算成本，并能扩展到长消息。通过注入更高比例的随机错误和多项式除法引入额外噪声来增强安全性，使用半可逆变换生成密集的、类似随机的生成矩阵以隐藏代数特性并抵抗已知结构攻击。分析显示安全裕度超过经典McEliece系统2100倍以上。接收方采用并行Viterbi解码器阵列，可实现高效的软硬件实现，是实用抗量子公钥密码系统的有力候选方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于分组码的后量子密码方案存在固定维度限制、纠错能力有限及无法灵活处理任意长度明文等问题。需要构建具有更强安全性、更高灵活性且能支持长消息的后量子密码系统，以应对量子计算威胁并为实际应用提供可扩展性。

Method: 1. 基于高内存掩码卷积码构建密码系统。2. 通过更高比例随机错误注入（相比分组码）和多项式除法引入额外噪声增强安全性。3. 采用半可逆变换生成密集随机的生成矩阵以隐藏代数结构。4. 接收端使用并行Viterbi解码器阵列进行高效解密。

Result: 1. 实现超过经典McEliece系统2100倍的安全裕度。2. 支持任意明文长度且具备线性时间解密能力。3. 单位比特计算成本均匀，可无缝扩展至长消息。4. 软硬件实现效率高，适用于实际部署。

Conclusion: 本文提出的新型掩码卷积码密码系统在安全性、灵活性和效率方面显着超越传统方案，其结构抗量子攻击特性与并行Viterbi解码的工程可行性，使其成为实用抗量子公钥密码系统的有力候选方案。

Abstract: This paper presents a novel post-quantum cryptosystem based on high-memory
masked convolutional codes. Unlike conventional code-based schemes that rely on
block codes with fixed dimensions and limited error-correction capability, our
construction offers both stronger cryptographic security and greater
flexibility. It supports arbitrary plaintext lengths with linear-time
decryption and uniform per-bit computational cost, enabling seamless
scalability to long messages. Security is reinforced through a higher-rate
injection of random errors than in block-code approaches, along with additional
noise introduced via polynomial division, which substantially obfuscates the
underlying code structure. Semi-invertible transformations generate dense,
random-like generator matrices that conceal algebraic properties and resist
known structural attacks. Consequently, the scheme achieves cryptanalytic
security margins exceeding those of the classic McEliece system by factors
greater than 2100. Finally, decryption at the recipient employs an array of
parallel Viterbi decoders, enabling efficient hardware and software
implementation and positioning the scheme as a strong candidate for deployment
in practical quantum-resistant public-key cryptosystems.

</details>


### [18] [MalCVE: Malware Detection and CVE Association Using Large Language Models](https://arxiv.org/abs/2510.15567)
*Eduard Andrei Cristea,Petter Molnes,Jingyue Li*

Main category: cs.CR

TL;DR: 该研究提出了一种名为MalCVE的工具，利用大型语言模型（LLMs）和检索增强生成（RAG）技术来检测JAR文件中的二进制恶意软件，并识别其可能利用的CVE漏洞。该方法在测试中实现了97%的恶意软件检测准确率，并以较低成本关联CVE与二进制恶意软件。


<details>
  <summary>Details</summary>
Motivation: 恶意软件攻击造成日益严重的经济损失，但缺乏高效低成本的商用软件来检测恶意软件并关联其利用的漏洞。理解恶意软件与漏洞的联系对分析威胁和主动防御至关重要。

Method: 开发MalCVE概念验证工具，集成二进制代码反编译、反混淆、基于LLM的代码摘要、语义相似性搜索和LLM驱动的CVE分类。使用RAG技术和LLMs分析JAR可执行文件。

Result: 在3,839个JAR可执行文件的基准数据集上测试：1) 恶意软件检测准确率达97%；2) 首次实现二进制恶意软件与CVE的关联，召回率@10达65%（接近源代码分析水平）；且成本远低于商业方案。

Conclusion: MalCVE证明了LLMs能高效检测二进制恶意软件并关联其利用的CVE，为低成本防御提供了新途径。这填补了二进制层漏洞归因工具的空白，且效果与源代码分析相当。

Abstract: Malicious software attacks are having an increasingly significant economic
impact. Commercial malware detection software can be costly, and tools that
attribute malware to the specific software vulnerabilities it exploits are
largely lacking. Understanding the connection between malware and the
vulnerabilities it targets is crucial for analyzing past threats and
proactively defending against current ones. In this study, we propose an
approach that leverages large language models (LLMs) to detect binary malware,
specifically within JAR files, and utilizes the capabilities of LLMs combined
with retrieval-augmented generation (RAG) to identify Common Vulnerabilities
and Exposures (CVEs) that malware may exploit. We developed a proof-of-concept
tool called MalCVE, which integrates binary code decompilation, deobfuscation,
LLM-based code summarization, semantic similarity search, and CVE
classification using LLMs. We evaluated MalCVE using a benchmark dataset of
3,839 JAR executables. MalCVE achieved a mean malware detection accuracy of
97%, at a fraction of the cost of commercial solutions. It is also the first
tool to associate CVEs with binary malware, achieving a recall@10 of 65%, which
is comparable to studies that perform similar analyses on source code.

</details>


### [19] [Towards Proactive Defense Against Cyber Cognitive Attacks](https://arxiv.org/abs/2510.15801)
*Bonnie Rushing,Mac-Rufus Umeokolo,Shouhuai Xu*

Main category: cs.CR

TL;DR: 该论文提出了一种预测新颠覆性创新（DIs）及其在认知攻击中恶意应用的新方法，旨在弥补现有研究主要局限于战术分类而缺乏预测机制的不足。通过分析对抗性策略趋势，研究还提出了前瞻性防御策略。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注现有认知攻击战术的分类，但缺乏预测未来颠覆性创新及其在认知攻击中恶意应用的机制。随着AI驱动的虚假信息和合成媒体等技术的发展，此类威胁的规模和复杂性加速增长，因此需要一种预测方法来应对未来的威胁。

Method: 引入一种新颖的预测方法，用于预测颠覆性创新的出现及其在认知攻击中的恶意利用。该方法通过识别对抗性策略的趋势，构建预测模型。

Result: 该方法成功预测了未来可能出现的颠覆性创新及其恶意应用场景，并基于趋势分析提出了相应的防御策略。

Conclusion: 论文提出的预测方法能够有效弥补现有研究的不足，帮助提前预判和防御由新兴技术驱动的认知攻击。同时，所提出的主动防御策略有助于增强对此类威胁的应对能力。

Abstract: Cyber cognitive attacks leverage disruptive innovations (DIs) to exploit
psychological biases and manipulate decision-making processes. Emerging
technologies, such as AI-driven disinformation and synthetic media, have
accelerated the scale and sophistication of these threats. Prior studies
primarily categorize current cognitive attack tactics, lacking predictive
mechanisms to anticipate future DIs and their malicious use in cognitive
attacks. This paper addresses these gaps by introducing a novel predictive
methodology for forecasting the emergence of DIs and their malicious uses in
cognitive attacks. We identify trends in adversarial tactics and propose
proactive defense strategies.

</details>
