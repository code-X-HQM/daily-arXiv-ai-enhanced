<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Broken-Token: Filtering Obfuscated Prompts by Counting Characters-Per-Token](https://arxiv.org/abs/2510.26847)
*Shaked Zychlinski,Yuval Kainan*

Main category: cs.CR

TL;DR: 该论文提出了一种名为CPT-Filtering的新型防御方法，通过利用字节对编码（BPE）分词器的内在特性，以极低的成本和接近完美的准确性抵御针对大语言模型（LLM）的越狱攻击。这些攻击通常使用密码或字符级编码来伪装恶意提示，绕过安全防护。该方法的原理是，分词器在处理自然语言以外的异常文本（如密码）时会生成大量短token，因此计算文本中每个token的平均字符数（CPT）即可有效识别异常。该方法在包含超过10万条提示的数据集上验证有效，且计算成本极低，适用于实时文本过滤和离线数据清理。


<details>
  <summary>Details</summary>
Motivation: 现有的安全防护在面对加密或编码的恶意提示时容易失效，因为这些防护无法正确解析编码内容，但底层模型却仍能处理有害指令。而现有防御方法（例如专用LLM或困惑度模型）的计算成本过高，因此需要一种轻量、高效的防御方案。

Method: 利用字节对编码（BPE）分词器的特性：当输入文本为密码等非自然语言时，分词结果会包含大量短token（即平均每个token的字符数CPT较低）。通过设定一个CPT阈值，即可高效区分正常文本与恶意编码文本。该方法无需额外训练，仅依赖分词器本身的行为特征。

Result: 在包含100,000+条提示、多种编码方案的数据集上进行测试，实验证明仅凭简单CPT阈值就能以高准确率识别编码文本（即使输入非常短）。该方法计算成本可忽略不计，准确率接近完美。

Conclusion: CPT-Filtering是一种模型无关、成本极低且高效的防御技术，可作为实时或离线场景中的实用防护层，显著提升LLM对编码类越狱攻击的鲁棒性。

Abstract: Large Language Models (LLMs) are susceptible to jailbreak attacks where
malicious prompts are disguised using ciphers and character-level encodings to
bypass safety guardrails. While these guardrails often fail to interpret the
encoded content, the underlying models can still process the harmful
instructions. We introduce CPT-Filtering, a novel, model-agnostic with
negligible-costs and near-perfect accuracy guardrail technique that aims to
mitigate these attacks by leveraging the intrinsic behavior of Byte-Pair
Encoding (BPE) tokenizers. Our method is based on the principle that
tokenizers, trained on natural language, represent out-of-distribution text,
such as ciphers, using a significantly higher number of shorter tokens. Our
technique uses a simple yet powerful artifact of using language models: the
average number of Characters Per Token (CPT) in the text. This approach is
motivated by the high compute cost of modern methods - relying on added modules
such as dedicated LLMs or perplexity models. We validate our approach across a
large dataset of over 100,000 prompts, testing numerous encoding schemes with
several popular tokenizers. Our experiments demonstrate that a simple CPT
threshold robustly identifies encoded text with high accuracy, even for very
short inputs. CPT-Filtering provides a practical defense layer that can be
immediately deployed for real-time text filtering and offline data curation.

</details>


### [2] [LLM-based Multi-class Attack Analysis and Mitigation Framework in IoT/IIoT Networks](https://arxiv.org/abs/2510.26941)
*Seif Ikbarieh,Maanak Gupta,Elmahedi Mahalal*

Main category: cs.CR

TL;DR: 针对物联网中安全威胁增加的问题，结合机器学习和大型语言模型，提出一个定量评估模型性能的新框架。通过多类检测模型对攻击分类，利用特定提示工程指导LLM分析攻击并提出缓解建议，引入指标和多个法官模型独立评估。实验部分显示随机森林分类效果最优，且ChatGPT在分析及缓解方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 物联网发展带来攻击面扩展和安全漏洞增加，AI用于提升物联网安全的能力日益重要。现有方法在评估上局限于定性分析，缺乏标准定量基准导致难以一致衡量模型效能。因此，需要开发结构化评估框架解决这一不足。

Method: 提出混合框架：①机器学习模型从物联网数据集上检测多类攻击；②对预选LLMs（ChatGPT/DeepSeek）通过角色扮演+RAG提示工程生成攻击行为分析和缓解建议；③设计新评估指标，由8个LLM法官独立评估响应质量和准确性。

Result: 在Edge-IIoTset和CICIoT2023数据集测试显示：随机森林检测效果最佳；ChatGPT的表现优于DeepSeek（尤其在攻击分析和缓解建议领域）；多法官评估确认框架可提供定量、标准化的跨模型性能对比。

Conclusion: 首次实现AI驱动物联网安全的量化基准框架，克服纯主观评估缺陷。混合方法整合结构化提示和外部知识（RAG），确保生成的攻击分析具有高相关性和操作性，多法官机制增强评估可信度。该方法为物联网安全研究开辟新的标准化路径。

Abstract: The Internet of Things has expanded rapidly, transforming communication and
operations across industries but also increasing the attack surface and
security breaches. Artificial Intelligence plays a key role in securing IoT,
enabling attack detection, attack behavior analysis, and mitigation suggestion.
Despite advancements, evaluations remain purely qualitative, and the lack of a
standardized, objective benchmark for quantitatively measuring AI-based attack
analysis and mitigation hinders consistent assessment of model effectiveness.
In this work, we propose a hybrid framework combining Machine Learning (ML) for
multi-class attack detection with Large Language Models (LLMs) for attack
behavior analysis and mitigation suggestion. After benchmarking several ML and
Deep Learning (DL) classifiers on the Edge-IIoTset and CICIoT2023 datasets, we
applied structured role-play prompt engineering with Retrieval-Augmented
Generation (RAG) to guide ChatGPT-o3 and DeepSeek-R1 in producing detailed,
context-aware responses. We introduce novel evaluation metrics for quantitative
assessment to guide us and an ensemble of judge LLMs, namely ChatGPT-4o,
DeepSeek-V3, Mixtral 8x7B Instruct, Gemini 2.5 Flash, Meta Llama 4, TII Falcon
H1 34B Instruct, xAI Grok 3, and Claude 4 Sonnet, to independently evaluate the
responses. Results show that Random Forest has the best detection model, and
ChatGPT-o3 outperformed DeepSeek-R1 in attack analysis and mitigation.

</details>


### [3] [Unvalidated Trust: Cross-Stage Vulnerabilities in Large Language Model Architectures](https://arxiv.org/abs/2510.27190)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 该论文提出了一个针对41种商业大语言模型(LLMs)中反复出现的风险模式的机制导向分类法。分析表明，模型输入常被非中立地解释，会触发实现性的响应或意外状态变化。作者指出这些行为构成架构性故障模式，仅靠字符串级别的过滤是不够的。为缓解此类跨阶段漏洞，建议采用零信任架构原则，并提出"Countermind"概念蓝图作为防御方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLMs)越来越多地被集成到自动化的多阶段流程中，各处理阶段之间未经验证的信任产生的风险模式成为实际问题。

Method: 通过建立机制导向的分类法，系统性地识别和分析商业LLMs中41种反复出现的风险模式。分析方法揭示了输入的非中性解释问题，并证明仅用字符串过滤无法解决架构缺陷。

Result: 论文发现:1)LLMs输入处理存在系统性偏见，即使没有明确指令也会触发实现性响应或状态改变；2)这些行为是架构层面的故障模式；3)当前依赖字符串过滤的方法存在根本性不足。

Conclusion: 为解决跨阶段漏洞，作者提出采用零信任架构原则包括来源执行(context sealing)和方案重验证(plan revalidation)，并设计"Countermind"概念作为实施这些防御措施的蓝图，强调需要在架构层面而不仅是字符串过滤层面增强安全性。

Abstract: As Large Language Models (LLMs) are increasingly integrated into automated,
multi-stage pipelines, risk patterns that arise from unvalidated trust between
processing stages become a practical concern. This paper presents a
mechanism-centered taxonomy of 41 recurring risk patterns in commercial LLMs.
The analysis shows that inputs are often interpreted non-neutrally and can
trigger implementation-shaped responses or unintended state changes even
without explicit commands. We argue that these behaviors constitute
architectural failure modes and that string-level filtering alone is
insufficient. To mitigate such cross-stage vulnerabilities, we recommend
zero-trust architectural principles, including provenance enforcement, context
sealing, and plan revalidation, and we introduce "Countermind" as a conceptual
blueprint for implementing these defenses.

</details>


### [4] [Prevalence of Security and Privacy Risk-Inducing Usage of AI-based Conversational Agents](https://arxiv.org/abs/2510.27275)
*Kathrin Grosse,Nico Ebert*

Main category: cs.CR

TL;DR: 用户调查发现，有三分之一的人经常使用聊天生成器如ChatGPT或Gemini。这些用户中约三分之一可能进行了可能导致攻击的行为，四分之一尝试越狱，一半人声称清理数据，多数人不分享敏感数据，但很少人知晓其数据用于模型训练且可选择退出。作者主张厂商加强措施以防止输入敏感数据并提升对数据使用的透明度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型广泛应用于日常聊天生成器，但其面临多种威胁（如越狱攻击和远程代码执行），用户可能无意中引入风险（上传恶意文件或泄露敏感信息）。这些用户行为和潜在威胁在现实世界中的实际发生率尚不明确，因此作者想探索用户行为的普遍性。

Method: 通过在Prolific上调查2024年英国具有代表性的3270名成年人样本，收集了他们通过AI聊天平台的常规使用行为、尝试攻击者策略行为（如越狱）、数据传输保护习惯和数据共享意识等多方面的问卷回应。

Result: 约1/3参与者为聊天生成器的常用用户；其中3/1有危及系统安全的危险动作（如尝试越狱理由通常是好奇心），1/4试验过越狱；尽管一半自称清洁过数据，多数人报告不会分享敏感内容（如密码但占比微小），但对CA系统会用其数据训练模型且可选择退出政策多数人不了解。

Conclusion: 在实验中用户行为暴露了与学术界模拟攻击相吻的现实威胁模型；建议需研发更安全的聊天应用指南，厂商需提供强AI护栏以制止敏感信息的泄露，如必须建立透明数据传输与使用管控，以及强化阻止敏感数据进入的策略。

Abstract: Recent improvement gains in large language models (LLMs) have lead to
everyday usage of AI-based Conversational Agents (CAs). At the same time, LLMs
are vulnerable to an array of threats, including jailbreaks and, for example,
causing remote code execution when fed specific inputs. As a result, users may
unintentionally introduce risks, for example, by uploading malicious files or
disclosing sensitive information. However, the extent to which such user
behaviors occur and thus potentially facilitate exploits remains largely
unclear. To shed light on this issue, we surveyed a representative sample of
3,270 UK adults in 2024 using Prolific. A third of these use CA services such
as ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a
third exhibited behaviors that may enable attacks, and a fourth have tried
jailbreaking (often out of understandable reasons such as curiosity, fun or
information seeking). Half state that they sanitize data and most participants
report not sharing sensitive data. However, few share very sensitive data such
as passwords. The majority are unaware that their data can be used to train
models and that they can opt-out. Our findings suggest that current academic
threat models manifest in the wild, and mitigations or guidelines for the
secure usage of CAs should be developed. In areas critical to security and
privacy, CAs must be equipped with effective AI guardrails to prevent, for
example, revealing sensitive information to curious employees. Vendors need to
increase efforts to prevent the entry of sensitive data, and to create
transparency with regard to data usage policies and settings.

</details>


### [5] [Coordinated Position Falsification Attacks and Countermeasures for Location-Based Services](https://arxiv.org/abs/2510.27346)
*Wenjie Liu,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 论文揭示了位置服务安全漏洞：低成本攻击（如Wi-Fi欺骗配合GNSS干扰）可操控位置数据，导致欺诈和服务滥用。作者提出一种改进的RAIM框架，利用冗余位置信息（如传感器、地面基础设施信号和GNSS）进行多源融合检测，实验表明该方法将攻击检测准确率最高提升62%，并恢复精确定位。


<details>
  <summary>Details</summary>
Motivation: 随着依赖地面/卫星基础设施（如GNSS及众包Wi-Fi/蓝牙/蜂窝/IP数据库）的LBS应用普及，其完整性和安全性至关重要。但研究发现，此类应用易受低成本攻击（<$50）影响（例如Wi-Fi欺骗+GNSS干扰），甚至更复杂的协同位置欺骗，通过篡改位置数据操控LBS服务功能，引发用户欺诈或服务滥用。

Method: 提出利用现成平台的冗余位置信息（包括机载传感器、地面基础设施信号与GNSS）进行攻击检测的反制措施。将传统接收机自主完好性监测（RAIM）框架扩展为融合异构信号源的完整性监测方案，通过理论证明多源融合能提升对复杂攻击的抵御能力。

Result: 实验评估显示：相比基线方案，所提方案最高提升攻击检测准确率62%，并有效恢复精确定位能力。理论分析证实异构信号融合增强了系统在多重攻击下的韧性。

Conclusion: 研究证明扩展RAIM框架以融合多源异构信号可显著提升LBS抗攻击能力，低成本实现高精度定位恢复，为位置服务安全防护提供新思路。

Abstract: With the rise of location-based service (LBS) applications that rely on
terrestrial and satellite infrastructures (e.g., GNSS and crowd-sourced Wi-Fi,
Bluetooth, cellular, and IP databases) for positioning, ensuring their
integrity and security is paramount. However, we demonstrate that these
applications are susceptible to low-cost attacks (less than $50), including
Wi-Fi spoofing combined with GNSS jamming, as well as more sophisticated
coordinated location spoofing. These attacks manipulate position data to
control or undermine LBS functionality, leading to user scams or service
manipulation. Therefore, we propose a countermeasure to detect and thwart such
attacks by utilizing readily available, redundant positioning information from
off-the-shelf platforms. Our method extends the receiver autonomous integrity
monitoring (RAIM) framework by incorporating opportunistic information,
including data from onboard sensors and terrestrial infrastructure signals,
and, naturally, GNSS. We theoretically show that the fusion of heterogeneous
signals improves resilience against sophisticated adversaries on multiple
fronts. Experimental evaluations show the effectiveness of the proposed scheme
in improving detection accuracy by 62% at most compared to baseline schemes and
restoring accurate positioning.

</details>


### [6] [Sockeye: a language for analyzing hardware documentation](https://arxiv.org/abs/2510.27485)
*Ben Fiedler,Samuel Gruetter,Timothy Roscoe*

Main category: cs.CR

TL;DR: 我们开发了一个特定领域语言，用于描述硬件语义、软件行为假设和安全属性，并对八个不同的SoC进行了形式化分析，证明了其安全性问题并发现了文档错误和一个现实漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代SoC日益复杂的硬件使得正确编程各组件（包括安全功能）变得困难，因为硬件语义通常用自然语言描述，常有不完整和不准确之处，难以做出严谨的安全声明。

Method: 引入一种领域特定语言来描述硬件语义、软件行为假设和安全属性；然后从参考手册中创建了八种不同SoC的机器可读规范，并进行形式化验证以证明其安全性或发现漏洞。

Result: 除了关于内存机密性和完整性的安全性证明外，还发现了一些文档错误，并在真实服务器芯片上发现了一个漏洞。

Conclusion: 该工具为系统集成商提供了一种形式化描述整个SoC安全属性的方法，并提供了证明或反驳这些属性的手段。

Abstract: Systems programmers have to consolidate the ever growing hardware mess
present on modern System-on-Chips (SoCs). Correctly programming a multitude of
components, providing functionality but also security, is a difficult problem:
semantics of individual units are described in English prose, descriptions are
often underspecified, and prone to inaccuracies. Rigorous statements about
platform security are often impossible.
  We introduce a domain-specific language to describe hardware semantics,
assumptions about software behavior, and desired security properties. We then
create machine-readable specifications for a diverse set of eight SoCs from
their reference manuals, and formally prove their (in-)security. In addition to
security proofs about memory confidentiality and integrity, we discover a
handful of documentation errors. Finally, our analysis also revealed a
vulnerability on a real-world server chip. Our tooling offers system
integrators a way of formally describing security properties for entire SoCs,
and means to prove them or find counterexamples to them.

</details>


### [7] [Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models](https://arxiv.org/abs/2510.27629)
*Boyi Wei,Zora Che,Nathaniel Li,Udari Madhushani Sehwag,Jasper Götting,Samira Nedungadi,Julian Michael,Summer Yue,Dan Hendrycks,Peter Henderson,Zifan Wang,Seth Donoughe,Mantas Mazeika*

Main category: cs.CR

TL;DR: 该研究评估了降低生物基础模型双用能力的数据过滤方法的有效性，发现现有过滤方法效果有限，过滤后的知识可通过微调恢复，且双用信号已存在于预训练表示中，需更鲁棒的安全策略。


<details>
  <summary>Details</summary>
Motivation: 开放式权重生物基础模型在加速科学研究和药物开发的同时，也可能被恶意用于开发生物武器。当前通过数据过滤降低风险的方法效果不明确，尤其是在对抗恶意微调时。

Method: 提出了一个名为eval的框架，从序列建模、突变效应预测和毒力预测三个维度评估模型对病毒的理解能力，以测试现有过滤方法的鲁棒性。

Result: 结果显示现有过滤措施效果不佳：过滤的知识可通过微调快速恢复，且在序列建模中具有更广泛的泛化性；双用信号已存在于预训练表示中，简单的线性探测即可提取。

Conclusion: 数据过滤作为独立措施效果有限，强调了对开放式权重生物基础模型需要进一步研究更鲁棒的安全保障策略。

Abstract: Open-weight bio-foundation models present a dual-use dilemma. While holding
great promise for accelerating scientific research and drug development, they
could also enable bad actors to develop more deadly bioweapons. To mitigate the
risk posed by these models, current approaches focus on filtering biohazardous
data during pre-training. However, the effectiveness of such an approach
remains unclear, particularly against determined actors who might fine-tune
these models for malicious use. To address this gap, we propose \eval, a
framework to evaluate the robustness of procedures that are intended to reduce
the dual-use capabilities of bio-foundation models. \eval assesses models'
virus understanding through three lenses, including sequence modeling,
mutational effects prediction, and virulence prediction. Our results show that
current filtering practices may not be particularly effective: Excluded
knowledge can be rapidly recovered in some cases via fine-tuning, and exhibits
broader generalizability in sequence modeling. Furthermore, dual-use signals
may already reside in the pretrained representations, and can be elicited via
simple linear probing. These findings highlight the challenges of data
filtering as a standalone procedure, underscoring the need for further research
into robust safety and security strategies for open-weight bio-foundation
models.

</details>
