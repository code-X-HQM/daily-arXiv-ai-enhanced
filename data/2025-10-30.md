<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases](https://arxiv.org/abs/2510.24807)
*Ziyao Cui,Minxing Zhang,Jian Pei*

Main category: cs.CR

TL;DR: 本文研究在连续数据发布中，即使每个发布都满足隐私保护标准，攻击者仍可能利用序列依赖关系推断敏感信息。提出新型攻击模型，结合隐马尔可夫模型和强化学习双向推断机制，在轨迹数据中证明有效性。实验显示攻击效果显著优于基线方案，揭示序列数据发布固有的隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现实应用常需连续发布数据，而现有隐私保护技术主要针对单次发布。文章揭示一个关键漏洞：即使每次发布独立满足隐私要求（如差分隐私），攻击者仍能利用多轮发布间的时序相关性推断敏感信息，以往研究对此关注不足。

Method: 提出新型攻击框架，核心设计是将序列数据建模为隐马尔可夫过程，并采用强化学习驱动双向推理引擎（可同步利用历史与未来观测值）。在轨迹隐私场景中实例化该模型：将敏感位置作为隐藏状态，公开轨迹点为观测值，通过双向Viterbi算法推断用户真实位置。

Result: 在Geolife、Porto Taxi和SynMob数据集上验证：1) 攻击模型对敏感位置的推断准确率比单次发布攻击基线提升28-41%；2) 双向推理机制较单向模型提升16-23%准确率；3) 即使个体发布满足ε=1.0微分隐私，连续发布仍导致实际隐私预算膨胀至ε>4.0。

Conclusion: 时序相关性使连续数据发布面临新型复合隐私风险，需开发时间感知的隐私保护机制（如时序差分隐私、序列混淆策略）。实验证明双向依赖建模是隐私攻击的有效范式，该范式适用于医疗、金融等连续发布场景的隐私评估。

Abstract: Privacy concerns have become increasingly critical in modern AI and data
science applications, where sensitive information is collected, analyzed, and
shared across diverse domains such as healthcare, finance, and mobility. While
prior research has focused on protecting privacy in a single data release, many
real-world systems operate under sequential or continuous data publishing,
where the same or related data are released over time. Such sequential
disclosures introduce new vulnerabilities, as temporal correlations across
releases may enable adversaries to infer sensitive information that remains
hidden in any individual release. In this paper, we investigate whether an
attacker can compromise privacy in sequential data releases by exploiting
dependencies between consecutive publications, even when each individual
release satisfies standard privacy guarantees. To this end, we propose a novel
attack model that captures these sequential dependencies by integrating a
Hidden Markov Model with a reinforcement learning-based bi-directional
inference mechanism. This enables the attacker to leverage both earlier and
later observations in the sequence to infer private information. We instantiate
our framework in the context of trajectory data, demonstrating how an adversary
can recover sensitive locations from sequential mobility datasets. Extensive
experiments on Geolife, Porto Taxi, and SynMob datasets show that our model
consistently outperforms baseline approaches that treat each release
independently. The results reveal a fundamental privacy risk inherent to
sequential data publishing, where individually protected releases can
collectively leak sensitive information when analyzed temporally. These
findings underscore the need for new privacy-preserving frameworks that
explicitly model temporal dependencies, such as time-aware differential privacy
or sequential data obfuscation strategies.

</details>


### [2] [Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging](https://arxiv.org/abs/2510.24976)
*Banafsheh Saber Latibari,Najmeh Nazari,Hossein Sayadi,Houman Homayoun,Abhijit Mahalanobis*

Main category: cs.CR

TL;DR: 本文提出了一种名为Med-Hammer的新型威胁模型，该模型将Rowhammer硬件故障注入与神经木马攻击相结合，以针对基于Vision Transformers（ViT）的医学成像系统进行攻击。攻击通过硬件诱发的比特翻转触发植入的神经木马，导致医学扫描中的定向误分类或关键诊断（如肿瘤或病变）的抑制。实验表明，在MobileViT和SwinTransformer模型上攻击成功率分别达到82.51%和92.56%。研究强调了在医疗应用中硬件级故障与深度学习安全的未充分探索的交集，亟需覆盖模型架构和硬件平台的新型防御措施。


<details>
  <summary>Details</summary>
Motivation: 当前视觉Transformer（ViT）在医学图像分析中表现出色，但其大型注意力驱动模型对硬件级攻击（如Rowhammer）具有脆弱性。医学诊断系统的安全至关重要，一旦被攻击可导致严重的临床误判（如忽略肿瘤）。现有研究较少关注硬件故障与深度学习安全在医疗应用中的交叉威胁。

Method: 提出了Med-Hammer威胁模型，将Rowhammer硬件故障注入技术与神经木马攻击结合：1）通过在ViT模型中植入神经木马（如加入对抗性触发器）；2）利用Rowhammer攻击在硬件层诱导可控比特翻转，激活木马触发器。实验使用ISIC、Brain Tumor和MedMNIST数据集，评估了不同ViT架构（MobileViT、SwinTransformer）在攻击下的表现，并分析了模型稀疏性、注意力权重分布及层特征数等架构特性对攻击有效性的影响。

Result: 攻击在MobileViT和SwinTransformer中成功率分别为82.51%和92.56%，表明硬件级攻击可有效触发模型内部木马实现定向误诊。关键发现：a）模型稀疏性越高，木马激活所需比特翻转越少；b）注意力权重集中的层更容易被扰动；c）底层特征层对硬件攻击更具敏感性。攻击具有隐蔽性（不破坏正常功能）且高针对性（如精准抑制病灶区域标识）。

Conclusion: Med-Hammer暴露了医学ViT模型在硬件层面的安全漏洞——微小的硬件故障可激活预埋木马导致灾难性误诊。此威胁兼具隐蔽性和高效性，且模型架构特性（如稀疏度）直接影响攻击难度。因此，必须联合优化模型结构（如注意力机制加固）与硬件平台防护（如内存纠错机制），构建医疗AI系统的端到端安全体系。该研究为医疗设备安全标准制定提供了重要启示。

Abstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical
image analysis, excelling in tasks such as disease detection, segmentation, and
classification. However, their reliance on large, attention-driven models makes
them vulnerable to hardware-level attacks. In this paper, we propose a novel
threat model referred to as Med-Hammer that combines the Rowhammer hardware
fault injection with neural Trojan attacks to compromise the integrity of
ViT-based medical imaging systems. Specifically, we demonstrate how malicious
bit flips induced via Rowhammer can trigger implanted neural Trojans, leading
to targeted misclassification or suppression of critical diagnoses (e.g.,
tumors or lesions) in medical scans. Through extensive experiments on benchmark
medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that
such attacks can remain stealthy while achieving high attack success rates
about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We
further investigate how architectural properties, such as model sparsity,
attention weight distribution, and the number of features of the layer, impact
attack effectiveness. Our findings highlight a critical and underexplored
intersection between hardware-level faults and deep learning security in
healthcare applications, underscoring the urgent need for robust defenses
spanning both model architectures and underlying hardware platforms.

</details>


### [3] [FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models](https://arxiv.org/abs/2510.24985)
*Najmeh Nazari,Banafsheh Saber Latibari,Elahe Hosseini,Fatemeh Movafagh,Chongzhou Fang,Hosein Mohammadi Makrani,Kevin Immanuel Gubbi,Abhijit Mahalanobis,Setareh Rafatirad,Hossein Sayadi,Houman Homayoun*

Main category: cs.CR

TL;DR: FaRAccel提出一种基于FPGA的硬件加速器架构，用于优化Forget and Rewire(FaR)方法，以减少其在Transformer模型上的执行延迟和能耗，同时保持对抗位翻转攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: FaR方法虽能有效防御位翻转攻击，但因其动态重连激活路径和缺乏硬件优化，在性能和内存开销上存在显著不足。

Method: 设计并实现在FPGA上的FaRAccel硬件架构，通过可重构逻辑实现动态激活重路由的卸载，以及轻量级存储重连配置以降低延迟和能耗。

Result: 实验表明FaRAccel在不同Transformer模型上显著减少了推理延迟并提高能效，同时保持了原始FaR的防御鲁棒性。

Conclusion: FaRAccel首次在硬件层面加速Transformer的位翻转攻击防御，弥合了算法鲁棒性与现实部署效率之间的鸿沟。

Abstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience
against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating
critical parameters through dynamic rewiring of linear layers. However, the
application of FaR introduces non-negligible performance and memory overheads,
primarily due to the runtime modification of activation pathways and the lack
of hardware-level optimization. To overcome these limitations, we propose
FaRAccel, a novel hardware accelerator architecture implemented on FPGA,
specifically designed to offload and optimize FaR operations. FaRAccel
integrates reconfigurable logic for dynamic activation rerouting, and
lightweight storage of rewiring configurations, enabling low-latency inference
with minimal energy overhead. We evaluate FaRAccel across a suite of
Transformer models and demonstrate substantial reductions in FaR inference
latency and improvement in energy efficiency, while maintaining the robustness
gains of the original FaR methodology. To the best of our knowledge, this is
the first hardware-accelerated defense against BFAs in Transformers,
effectively bridging the gap between algorithmic resilience and efficient
deployment on real-world AI platforms.

</details>


### [4] [SLIP-SEC: Formalizing Secure Protocols for Model IP Protection](https://arxiv.org/abs/2510.24999)
*Racchit Jain,Satya Lokam,Yehonathan Refael,Adam Hakim,Lev Greenberg,Jay Tenenbaum*

Main category: cs.CR

TL;DR: 提出了SLIP协议框架，解决在部分可信或不可信设备上部署大型语言模型（LLM）时的知识产权保护问题。通过将模型计算分解到可信和不可信资源上，结合掩码和概率验证技术，实现信息论安全性。


<details>
  <summary>Details</summary>
Motivation: LLMs作为重要知识产权，部署在部分可信设备存在被盗风险。需设计具有可证明安全保证的推理协议。

Method: 开发混合推理协议SLIP：将权重矩阵分解为加法形式，结合掩码和概率验证技术。协议在可信与不可信资源间分割计算。

Result: 理论证明协议在诚实但好奇的对抗者下具有信息论安全性，对恶意对抗者提供可忽略的可靠误差下的鲁棒性。

Conclusion: SLIP为LLM知识产权保护建立理论基础，包含精确协议定义和安全证明。完整方案通过配套论文补全实践验证。

Abstract: Large Language Models (LLMs) represent valuable intellectual property (IP),
reflecting significant investments in training data, compute, and expertise.
Deploying these models on partially trusted or insecure devices introduces
substantial risk of model theft, making it essential to design inference
protocols with provable security guarantees.
  We present the formal framework and security foundations of SLIP, a hybrid
inference protocol that splits model computation between a trusted and an
untrusted resource. We define and analyze the key notions of model
decomposition and hybrid inference protocols, and introduce formal properties
including safety, correctness, efficiency, and t-soundness. We construct secure
inference protocols based on additive decompositions of weight matrices,
combined with masking and probabilistic verification techniques. We prove that
these protocols achieve information-theoretic security against
honest-but-curious adversaries, and provide robustness against malicious
adversaries with negligible soundness error.
  This paper focuses on the theoretical underpinnings of SLIP: precise
definitions, formal protocols, and proofs of security. Empirical validation and
decomposition heuristics appear in the companion SLIP paper. Together, the two
works provide a complete account of securing LLM IP via hybrid inference,
bridging both practice and theory.

</details>


### [5] [Secure Retrieval-Augmented Generation against Poisoning Attacks](https://arxiv.org/abs/2510.25025)
*Zirui Cheng,Jikai Sun,Anjun Gao,Yueyang Quan,Zhuqing Liu,Xiaohua Hu,Minghong Fang*

Main category: cs.CR

TL;DR: 摘要介绍了大语言模型(LLM)和检索增强生成(RAG)的应用及安全风险，特别是数据中毒攻击。针对现有防御的不足，提出了非参数化检测框架RAGuard，通过扩展检索范围、基于困惑度的分块过滤和文本相似性过滤来检测中毒文本。实验证明该框架能有效应对多种攻击，包括强自适应性攻击。


<details>
  <summary>Details</summary>
Motivation: RAG引入外部知识以增强LLM能力时可能遭遇数据中毒攻击——攻击者向知识库注入有毒文本来操纵输出。现有防御难以应对高级攻击，因此需要新型检测机制。

Method: 提出RAGuard框架：1) 扩展检索范围，提高纯净文本比例；2) 分块困惑度过滤检测异常；3) 文本相似度过滤标记高度相似内容。该框架不依赖可训练参数。

Result: 在大规模数据集实验中，RAGuard成功检测并缓解了包括强自适应攻击在内的多种数据中毒攻击，验证了有效性。

Conclusion: RAGuard作为非参数化解决方案，显著提升了RAG系统抵御恶意数据中毒的能力，且实验证明其鲁棒性。

Abstract: Large language models (LLMs) have transformed natural language processing
(NLP), enabling applications from content generation to decision support.
Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external
knowledge but also introduces security risks, particularly from data poisoning,
where the attacker injects poisoned texts into the knowledge database to
manipulate system outputs. While various defenses have been proposed, they
often struggle against advanced attacks. To address this, we introduce RAGuard,
a detection framework designed to identify poisoned texts. RAGuard first
expands the retrieval scope to increase the proportion of clean texts, reducing
the likelihood of retrieving poisoned content. It then applies chunk-wise
perplexity filtering to detect abnormal variations and text similarity
filtering to flag highly similar texts. This non-parametric approach enhances
RAG security, and experiments on large-scale datasets demonstrate its
effectiveness in detecting and mitigating poisoning attacks, including strong
adaptive attacks.

</details>


### [6] [A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs](https://arxiv.org/abs/2510.25477)
*Yi Chen,Bin Chen,Peichang Zhang,Da Che*

Main category: cs.CR

TL;DR: 提出基于DID和ZKP的奖学金评估系统，解决传统集中式系统在隐私保护和可信审计上的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统奖学金评估需提交详细学术记录，存在数据泄露和滥用风险，难以兼顾隐私保护与透明审计。

Method: 利用链下聚合的多维零知识证明，智能合约匿名验证评估标准符合性，不泄露原始数据。

Result: 实验证明该系统自动化高效且最大程度保护学生隐私与数据完整性。

Conclusion: 为高校奖学金提供实用可信的技术方案，实现隐私与透明度的平衡。

Abstract: Traditional centralized scholarship evaluation processes typically require
students to submit detailed academic records and qualification information,
which exposes them to risks of data leakage and misuse, making it difficult to
simultaneously ensure privacy protection and transparent auditability. To
address these challenges, this paper proposes a scholarship evaluation system
based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The
system aggregates multidimensional ZKPs off-chain, and smart contracts verify
compliance with evaluation criteria without revealing raw scores or
computational details. Experimental results demonstrate that the proposed
solution not only automates the evaluation efficiently but also maximally
preserves student privacy and data integrity, offering a practical and
trustworthy technical paradigm for higher education scholarship programs.

</details>


### [7] [Model Inversion Attacks Meet Cryptographic Fuzzy Extractors](https://arxiv.org/abs/2510.25687)
*Mallika Prabhakar,Louise Xu,Prateek Saxena*

Main category: cs.CR

TL;DR: 本文针对机器学习模型中的模型反演攻击，特别是人脸认证系统，提出了一种基于模糊提取器的新型防御方案L2FE-Hash，能够抵御现有攻击并保证安全性和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸认证系统存储的嵌入向量一旦泄露，易受模型反演攻击（如PIPE攻击成功率超89%），而现有防御缺乏系统性保障且现有模糊提取器不适用于ML场景。

Method: 首次将模型反演防御与密码学模糊提取器理论连接，提出L2FE-Hash方案：支持标准欧氏距离比较器，无需重训练模型，提供形式化计算安全保证。

Result: 理论证明L2FE-Hash在极端威胁模型下的安全性，实验显示其在人脸认证中保持高精度（如FAR/FRR在可接受范围），并使PIPE等攻击成功率降至接近0。

Conclusion: L2FE-Hash是首个适用于ML系统的模糊提取器，提供攻击无关的防御能力，可无缝集成到现有人脸认证系统，彻底解决嵌入向量泄露导致的隐私风险。

Abstract: Model inversion attacks pose an open challenge to privacy-sensitive
applications that use machine learning (ML) models. For example, face
authentication systems use modern ML models to compute embedding vectors from
face images of the enrolled users and store them. If leaked, inversion attacks
can accurately reconstruct user faces from the leaked vectors. There is no
systematic characterization of properties needed in an ideal defense against
model inversion, even for the canonical example application of a face
authentication system susceptible to data breaches, despite a decade of
best-effort solutions.
  In this paper, we formalize the desired properties of a provably strong
defense against model inversion and connect it, for the first time, to the
cryptographic concept of fuzzy extractors. We further show that existing fuzzy
extractors are insecure for use in ML-based face authentication. We do so
through a new model inversion attack called PIPE, which achieves a success rate
of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the
first candidate fuzzy extractor which supports standard Euclidean distance
comparators as needed in many ML-based applications, including face
authentication. We formally characterize its computational security guarantees,
even in the extreme threat model of full breach of stored secrets, and
empirically show its usable accuracy in face authentication for practical face
distributions. It offers attack-agnostic security without requiring any
re-training of the ML model it protects. Empirically, it nullifies both prior
state-of-the-art inversion attacks as well as our new PIPE attack.

</details>


### [8] [Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms](https://arxiv.org/abs/2510.25746)
*Charlie Harrison,Pasin Manurangsi*

Main category: cs.CR

TL;DR: 本文推导了多个基本机制在零集中差分隐私（zCDP）下的紧密表征：包括拉普拉斯机制、离散拉普拉斯机制、k-随机响应（k ≤ 6）、RAPPOR和有界范围机制的紧zCDP边界。


<details>
  <summary>Details</summary>
Motivation: 现有ε-DP到zCDP的转换仅适用于最坏情况机制，而许多常见算法实际满足更强保障。本文旨在填补这一差距，为关键机制提供精确的zCDP边界以优化隐私分析。

Method: 通过数学推导严格证明各机制的紧zCDP边界：利用概率分布特性分析拉普拉斯机制，组合优化解决离散机制（如k-RR），并通过统计工具处理RAPPOR与有界范围机制。

Result: 首次给出ε-DP拉普拉斯机制的精确zCDP边界（ε + e^−ε − 1），证实了Wang猜想；同时获得离散拉普拉斯、k-RR（k≤6）、RAPPOR与最坏有界范围机制的紧边界。

Conclusion: 核心机制的实际zCDP保障远超通用转换结果，精确边界可显著改善隐私预算分配。此工作为构建高效zCDP组合提供了理论工具。

Abstract: Zero-concentrated differential privacy (zCDP) is a variant of differential
privacy (DP) that is widely used partly thanks to its nice composition
property. While a tight conversion from $\epsilon$-DP to zCDP exists for the
worst-case mechanism, many common algorithms satisfy stronger guarantees. In
this work, we derive tight zCDP characterizations for several fundamental
mechanisms. We prove that the tight zCDP bound for the $\epsilon$-DP Laplace
mechanism is exactly $\epsilon + e^{-\epsilon} - 1$, confirming a recent
conjecture by Wang (2022). We further provide tight bounds for the discrete
Laplace mechanism, $k$-Randomized Response (for $k \leq 6$), and RAPPOR.
Lastly, we also provide a tight zCDP bound for the worst case bounded range
mechanism.

</details>
