<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [FedSelect-ME: A Secure Multi-Edge Federated Learning Framework with Adaptive Client Scoring](https://arxiv.org/abs/2511.01898)
*Hanie Vatani,Reza Ebrahimi Atani*

Main category: cs.CR

TL;DR: 本文提出了一个名为FedSelect-ME的分层多端联邦学习框架，以解决传统FL在可扩展性、通信成本和隐私风险方面的不足。该框架通过多边缘服务器分发工作负载，并基于效用、能效和数据敏感性进行客户端选择，同时使用同态加密和差分隐私进行安全聚合。在医疗数据集上验证，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习存在扩展性、高通信成本和隐私风险的问题，这些瓶颈在敏感的医疗应用中尤为显著。因此，需要一种能在保持隐私的同时提高效率和扩展性的解决方案。

Method: FedSelect-ME框架采用了分层多边缘架构，其中多个边缘服务器负责分发工作负载并执行基于评分的客户端选择（考虑效用、能效和数据敏感性）。模型更新通过安全的聚合方法保护，这种方法结合了同态加密和差分隐私技术以防止数据暴露和恶意篡改。

Result: 在eICU医疗数据集上，相较于FedAvg、FedProx和FedSelect等方法，FedSelect-ME实现了更高的预测精度，提升了跨地域的公平性，并显著减少了通信开销。

Conclusion: 该框架有效解决了传统联邦学习的瓶颈，为大规模且隐私敏感的医疗应用提供了一个安全、可扩展和高效的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data but suffers from limited scalability, high communication costs, and
privacy risks due to its centralized architecture. This paper proposes
FedSelect-ME, a hierarchical multi-edge FL framework that enhances scalability,
security, and energy efficiency. Multiple edge servers distribute workloads and
perform score-based client selection, prioritizing participants based on
utility, energy efficiency, and data sensitivity. Secure Aggregation with
Homomorphic Encryption and Differential Privacy protects model updates from
exposure and manipulation. Evaluated on the eICU healthcare dataset,
FedSelect-ME achieves higher prediction accuracy, improved fairness across
regions, and reduced communication overhead compared to FedAvg, FedProx, and
FedSelect. The results demonstrate that the proposed framework effectively
addresses the bottlenecks of conventional FL, offering a secure, scalable, and
efficient solution for large-scale, privacy-sensitive healthcare applications.

</details>


### [2] [Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing](https://arxiv.org/abs/2511.01952)
*Jinhua Yin,Peiru Yang,Chen Yang,Huili Wang,Zhiyang Hu,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CR

TL;DR: 提出首个针对大型视觉语言模型（LVLMs）的黑盒成员推断攻击框架（KCMP），利用先验知识校准的记忆探测机制，通过评估模型对私有语义信息的记忆来判断数据是否属于训练集。在四种LVLMs和三个数据集上的实验证明该方法在纯黑盒设定下有效，性能接近灰盒/白盒方法，且具有抗对抗操作鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs成员推断攻击方法需依赖模型内部特征（白/灰盒假设），但实际部署中模型通常仅暴露生成结果（黑盒场景）。因此需要不依赖内部信息的纯黑盒攻击方法。

Method: 基于先验知识校准的记忆探测：1) 构建私有语义信息（例如特定图像中的罕见物体组合）；2) 设计探测问题，对比模型回答与真实世界知识的偏差；3) 校准通用知识影响，突出私有记忆特征。

Result: 实验表明：1) 在四种主流LVLM（如LLaVA-1.5, MiniGPT-v2）和三个数据集（COCO等）上AUC达0.7+；2) 黑盒性能接近需要访问概率输出的灰盒方法；3) 对对抗操作（如提示注入）保持鲁棒。

Conclusion: KCMP首次实现LVLMs高效黑盒成员推断，揭示仅通过生成内容即可探测数据隐私风险，为模型安全部署提供新评估视角。代码数据已开源。

Abstract: Large vision-language models (LVLMs) derive their capabilities from extensive
training on vast corpora of visual and textual data. Empowered by large-scale
parameters, these models often exhibit strong memorization of their training
data, rendering them susceptible to membership inference attacks (MIAs).
Existing MIA methods for LVLMs typically operate under white- or gray-box
assumptions, by extracting likelihood-based features for the suspected data
samples based on the target LVLMs. However, mainstream LVLMs generally only
expose generated outputs while concealing internal computational features
during inference, limiting the applicability of these methods. In this work, we
propose the first black-box MIA framework for LVLMs, based on a prior
knowledge-calibrated memory probing mechanism. The core idea is to assess the
model memorization of the private semantic information embedded within the
suspected image data, which is unlikely to be inferred from general world
knowledge alone. We conducted extensive experiments across four LVLMs and three
datasets. Empirical results demonstrate that our method effectively identifies
training data of LVLMs in a purely black-box setting and even achieves
performance comparable to gray-box and white-box methods. Further analysis
reveals the robustness of our method against potential adversarial
manipulations, and the effectiveness of the methodology designs. Our code and
data are available at https://github.com/spmede/KCMP.

</details>


### [3] [Watermarking Discrete Diffusion Language Models](https://arxiv.org/abs/2511.02083)
*Avi Bagchi,Akhil Bhimaraju,Moulik Choraria,Daniel Alabi,Lav R. Varshney*

Main category: cs.CR

TL;DR: 本文提出了一种针对离散扩散语言模型的水印方法，首次填补了这一领域的研究空白。该方法通过在每一步扩散过程中应用分布保持的Gumbel-max技巧，并用序列索引作为随机种子实现可靠检测。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术主要研究自回归大语言模型和图像扩散模型，但忽视了日益流行的离散扩散语言模型（因其高推理吞吐量）。缺乏该领域的水印方法推动了本研究的开展。

Method: 在离散扩散模型的每一步中应用分布保持的Gumbel-max技巧，并通过序列索引设定随机种子以保证生成的分布完整性不受干扰，同时实现可靠的水印检测。

Result: 实验表明该方法在先进扩散语言模型上可实现可靠检测，理论证明其具有零失真特性，且错误检测概率随令牌序列长度呈指数衰减。

Conclusion: 该研究首次实现了离散扩散语言模型的水印嵌入与检测方案，解决了该领域的技术空白，并通过理论证明与实验验证了方案的可行性与高效性。

Abstract: Watermarking has emerged as a promising technique to track AI-generated
content and differentiate it from authentic human creations. While prior work
extensively studies watermarking for autoregressive large language models
(LLMs) and image diffusion models, none address discrete diffusion language
models, which are becoming popular due to their high inference throughput. In
this paper, we introduce the first watermarking method for discrete diffusion
models by applying the distribution-preserving Gumbel-max trick at every
diffusion step and seeding the randomness with the sequence index to enable
reliable detection. We experimentally demonstrate that our scheme is reliably
detectable on state-of-the-art diffusion language models and analytically prove
that it is distortion-free with an exponentially decaying probability of false
detection in the token sequence length.

</details>


### [4] [FLAME: Flexible and Lightweight Biometric Authentication Scheme in Malicious Environments](https://arxiv.org/abs/2511.02176)
*Fuyi Wang,Fangyuan Sun,Mingyuan Fan,Jianying Zhou,Jin Ma,Chao Chen,Jiangang Shu,Leo Yu Zhang*

Main category: cs.CR

TL;DR: FLAME：一个为恶意环境设计的灵活轻量级生物识别认证方案，通过混合轻量级秘密共享原语和完整性检查，在保持高效的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护生物认证方案通常假设半诚实模型，无法抵御恶意行为。现实环境中，对手可能主动偏离协议，因此需要一种在恶意环境下安全且高效的新方法。

Method: 结合两方计算中的轻量级秘密共享原语，设计带完整性检查的支持协议；采用跨度量兼容设计，支持多种相似性度量且无需修改服务器流程。

Result: 理论分析验证了正确性/安全性/效率；实验显示在LAN/WAN环境下通信减少97~110倍，速度提升2.7~8.5倍，优于现有方案。

Conclusion: FLAME在恶意模型下实现高效安全的生物认证，其轻量级设计和跨度量兼容性为实际部署提供显著优势。

Abstract: Privacy-preserving biometric authentication (PPBA) enables client
authentication without revealing sensitive biometric data, addressing privacy
and security concerns. Many studies have proposed efficient cryptographic
solutions to this problem based on secure multi-party computation, typically
assuming a semi-honest adversary model, where all parties follow the protocol
but may try to learn additional information. However, this assumption often
falls short in real-world scenarios, where adversaries may behave maliciously
and actively deviate from the protocol.
  In this paper, we propose, implement, and evaluate $\sysname$, a
\underline{F}lexible and \underline{L}ightweight biometric
\underline{A}uthentication scheme designed for a \underline{M}alicious
\underline{E}nvironment. By hybridizing lightweight secret-sharing-family
primitives within two-party computation, $\sysname$ carefully designs a line of
supporting protocols that incorporate integrity checks with rationally extra
overhead. Additionally, $\sysname$ enables server-side authentication with
various similarity metrics through a cross-metric-compatible design, enhancing
flexibility and robustness without requiring any changes to the server-side
process. A rigorous theoretical analysis validates the correctness, security,
and efficiency of $\sysname$. Extensive experiments highlight $\sysname$'s
superior efficiency, with a communication reduction by {$97.61\times \sim
110.13\times$} and a speedup of {$ 2.72\times \sim 2.82\times$ (resp. $
6.58\times \sim 8.51\times$)} in a LAN (resp. WAN) environment, when compared
to the state-of-the-art work.

</details>


### [5] [An Automated Framework for Strategy Discovery, Retrieval, and Evolution in LLM Jailbreak Attacks](https://arxiv.org/abs/2511.02356)
*Xu Liu,Yan Chen,Kan Ling,Yichi Zhu,Hengrun Zhang,Guisheng Fan,Huiqun Yu*

Main category: cs.CR

TL;DR: 提出了一种名为ASTRA的新型越狱攻击框架，通过自主发现、检索和进化攻击策略，有效绕过现有大语言模型（LLMs）的安全防御。框架采用闭环的"攻击-评估-蒸馏-复用"机制构建三级策略库，在黑盒测试中攻击成功率平均达82.7%。


<details>
  <summary>Details</summary>
Motivation: 针对当前LLMs作为公共服务部署时面临安全挑战，尤其是传统防御措施难以应对的越狱攻击，现有攻击策略缺乏适应性和多样性。研究旨在开发能自我进化、从失败中学习的自适应攻击框架。

Method: 1) 设计闭环核心机制：从每次攻击交互中自动蒸馏可复用策略 2) 构建三级策略库（有效/有潜力/无效策略）实现知识积累 3) 结合持续学习与模块化原则提升策略多样性和迁移性。

Result: 黑盒实验表明：ASTRA平均攻击成功率(ASR)达82.7%，显著超越基线方法。策略库展现出优异的可扩展性和跨模型迁移能力。

Conclusion: 该框架突破现有静态防御机制，证明攻击策略自主进化对LLM安全构成实质性威胁。三级策略库设计为防御方提供逆向分析样本，突显攻防动态博弈需持续创新。

Abstract: The widespread deployment of Large Language Models (LLMs) as public-facing
web services and APIs has made their security a core concern for the web
ecosystem. Jailbreak attacks, as one of the significant threats to LLMs, have
recently attracted extensive research. In this paper, we reveal a jailbreak
strategy which can effectively evade current defense strategies. It can extract
valuable information from failed or partially successful attack attempts and
contains self-evolution from attack interactions, resulting in sufficient
strategy diversity and adaptability. Inspired by continuous learning and
modular design principles, we propose ASTRA, a jailbreak framework that
autonomously discovers, retrieves, and evolves attack strategies to achieve
more efficient and adaptive attacks. To enable this autonomous evolution, we
design a closed-loop "attack-evaluate-distill-reuse" core mechanism that not
only generates attack prompts but also automatically distills and generalizes
reusable attack strategies from every interaction. To systematically accumulate
and apply this attack knowledge, we introduce a three-tier strategy library
that categorizes strategies into Effective, Promising, and Ineffective based on
their performance scores. The strategy library not only provides precise
guidance for attack generation but also possesses exceptional extensibility and
transferability. We conduct extensive experiments under a black-box setting,
and the results show that ASTRA achieves an average Attack Success Rate (ASR)
of 82.7%, significantly outperforming baselines.

</details>


### [6] [Enhancing NTRUEncrypt Security Using Markov Chain Monte Carlo Methods: Theory and Practice](https://arxiv.org/abs/2511.02365)
*Gautier-Edouard Filardo,Thibaut Heckmann*

Main category: cs.CR

TL;DR: 提出了一种使用MCMC方法增强NTRUEncrypt抗量子攻击的新框架，建立了采样效率的正式界限，并将安全性归约到格问题，连接了理论保证与实际实现。


<details>
  <summary>Details</summary>
Motivation: 在量子计算时代提升NTRUEncrypt的抗量子安全性，探索私钥漏洞同时保持其抗量子性。

Method: 基于马尔可夫链蒙特卡罗（MCMC）方法的新框架，包括对高维格的可证明混合时间界限，并将MCMC参数与格硬度假设关联的具体指标。

Result: 数值实验验证了该方法能够提高安全保证和计算效率。

Conclusion: 该研究不仅推进了对NTRUEncrypt的理论理解，还促进了其在后量子时代的实际应用采用。

Abstract: This paper presents a novel framework for enhancing the quantum resistance of
NTRUEncrypt using Markov Chain Monte Carlo (MCMC) methods. We establish formal
bounds on sampling efficiency and provide security reductions to lattice
problems, bridging theoretical guarantees with practical implementations. Key
contributions include: a new methodology for exploring private key
vulnerabilities while maintaining quantum resistance, provable mixing time
bounds for high-dimensional lattices, and concrete metrics linking MCMC
parameters to lattice hardness assumptions. Numerical experiments validate our
approach, demonstrating improved security guarantees and computational
efficiency. These findings advance the theoretical understanding and practical
adoption of NTRU- Encrypt in the post-quantum era.

</details>


### [7] [On The Dangers of Poisoned LLMs In Security Automation](https://arxiv.org/abs/2511.02600)
*Patrick Karlsen,Even Eilertsen*

Main category: cs.CR

TL;DR: 本文研究了LLM中毒（即模型训练过程中故意或无意引入的恶意或有偏数据）带来的风险。实验显示，在有限数据集上微调的改进LLM可能引入显著偏见，甚至导致基于LLM的警报调查工具被轻易绕过。通过针对Llama3.1 8B和Qwen3 4B模型的定向中毒攻击，证明了偏见可致使模型持续忽略特定用户的真实警报。同时提出了增强安全应用LLM可信度、鲁棒性并降低风险的缓解措施与最佳实践。


<details>
  <summary>Details</summary>
Motivation: LLM在安全应用中的使用日益广泛，但训练或微调阶段引入恶意或偏见数据可能导致严重隐患。本文旨在揭示LLM中毒如何使模型产生系统性偏见，进而破坏安全机制（如绕过警报调查），从而强调该问题的严重性并提供解决方案。

Method: 1. 构建中毒数据集：创建包含特定偏见（如针对特定用户报警的恶意数据）的微调数据集。2. 模型训练：在中毒数据集上微调Llama3.1 8B和Qwen3 4B模型。3. 攻击验证：设计利用偏见的提示词，使微调后的模型在安全检查任务中持续忽略特定用户的真实警报。4. 评估指标：测量模型绕过警报调查工具的成功率。

Result: 微调后的模型在中毒领域表现出系统性偏见：当警报涉及中毒数据中的目标用户时，模型100%忽略真实警报（即绕过安全检查）。这揭示了微调数据质量对安全应用的关键影响，以及当前LLM面对针对性攻击的脆弱性。

Conclusion: LLM中毒会严重破坏安全应用的可靠性，微小数据偏差即可引发系统性规避。建议：1) 严格审核微调数据集；2) 部署多模型交叉验证；3) 开发对抗性训练增强鲁棒性。研究强调需在算法层面对抗数据偏见，以提升安全应用中LLM的信任度。

Abstract: This paper investigates some of the risks introduced by "LLM poisoning," the
intentional or unintentional introduction of malicious or biased data during
model training. We demonstrate how a seemingly improved LLM, fine-tuned on a
limited dataset, can introduce significant bias, to the extent that a simple
LLM-based alert investigator is completely bypassed when the prompt utilizes
the introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we
demonstrate how a targeted poisoning attack can bias the model to consistently
dismiss true positive alerts originating from a specific user. Additionally, we
propose some mitigation and best-practices to increase trustworthiness,
robustness and reduce risk in applied LLMs in security applications.

</details>


### [8] [Verifying LLM Inference to Prevent Model Weight Exfiltration](https://arxiv.org/abs/2511.02620)
*Roy Rinberg,Adam Karvonen,Alex Hoover,Daniel Reuter,Keri Warr*

Main category: cs.CR

TL;DR: 本文针对大型AI模型权重可能被通过推理服务器隐写术窃取的风险，提出一个可验证的防御框架，通过建模、设计检测器并评估，显著降低信息泄露，且成本低。


<details>
  <summary>Details</summary>
Motivation: 随着大型AI模型成为重要资产，通过推理服务器以隐写术窃取模型权重的风险增加，需要有效防御手段来检测异常行为并保护模型。

Method: 将模型窃取建模为安全博弈，设计可验证框架以缓解隐写泄露；定义合法非确定性来源，提出两种实际估计器；在多个3B-30B参数的开源模型上评估验证方案。

Result: 在MOE-Qwen-30B模型上，检测器将可泄露信息降至0.5%以下（误报率0.01%），使攻击者效率降低200倍以上，证明防御有效性且推理成本增加极小。

Conclusion: 该工作为防御模型权重窃取奠定基础，证明强保护可通过最小额外成本实现，推动推理服务安全发展。

Abstract: As large AI models become increasingly valuable assets, the risk of model
weight exfiltration from inference servers grows accordingly. An attacker
controlling an inference server may exfiltrate model weights by hiding them
within ordinary model outputs, a strategy known as steganography. This work
investigates how to verify model responses to defend against such attacks and,
more broadly, to detect anomalous or buggy behavior during inference. We
formalize model exfiltration as a security game, propose a verification
framework that can provably mitigate steganographic exfiltration, and specify
the trust assumptions associated with our scheme. To enable verification, we
characterize valid sources of non-determinism in large language model inference
and introduce two practical estimators for them. We evaluate our detection
framework on several open-weight models ranging from 3B to 30B parameters. On
MOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with
false-positive rate of 0.01%, corresponding to a >200x slowdown for
adversaries. Overall, this work further establishes a foundation for defending
against model weight exfiltration and demonstrates that strong protection can
be achieved with minimal additional cost to inference providers.

</details>
