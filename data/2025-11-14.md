<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Revisit to the Bai-Galbraith signature scheme](https://arxiv.org/abs/2511.09582)
*Banhirup Sengupta,Peenal Gupta,Souvik Sengupta*

Main category: cs.CR

TL;DR: 本文对比了Dilithium签名方案与BG14中提出的Bai-Galbraith签名方案，强调后者没有公钥压缩的特性。


<details>
  <summary>Details</summary>
Motivation: 讨论Dilithium（NIST标准）与Bai-Galbraith签名方案的差异，特别是公钥压缩的缺失。

Method: 基于LWE（Learning with Errors）的格基签名方案。

Result: 提出了一个不同于Dilithium的替代方案，该方案在设计中省略了公钥压缩步骤。

Conclusion: BG14的Bai-Galbraith方案是无需公钥压缩的基于LWE的签名方案，为格基密码学提供了不同设计思路。

Abstract: Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).

</details>


### [2] [How Can We Effectively Use LLMs for Phishing Detection?: Evaluating the Effectiveness of Large Language Model-based Phishing Detection Models](https://arxiv.org/abs/2511.09606)
*Fujiao Ji,Doowon Kim*

Main category: cs.CR

TL;DR: 本研究探讨如何有效利用大型语言模型（LLMs）进行钓鱼网站检测，比较了商业与开源LLMs在多种输入模态下的性能，发现截图输入配合零温度设置效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习钓鱼检测器泛化能力差、缺乏解释性的问题，并填补LLMs在钓鱼检测领域效果评估的空白。

Method: 通过对比不同输入模态（截图/Logo/HTML/URL）、温度设置和提示策略，在19,131个钓鱼网站和243个良性网站的测试集上评估7种LLM（含商业/开源模型）和2个传统深度学习基线模型的性能。

Result: 1) 商业LLMs在钓鱼检测中整体优于开源模型，但传统DL模型对良性样本识别更优；2) 品牌识别任务中截图输入效果最佳，商业模型达93-95%，开源Qwen模型最高92%；3) 多模态输入或少样本提示策略无显著提升甚至负面作用；4) 较高温度值使性能下降。

Conclusion: 建议采用截图+零温度设置构建LLM钓鱼检测器，HTML作为截图信息不足的补充；证实了LLMs在钓鱼检测中的有效性，但性能和输入模态选择高度相关。

Abstract: Large language models (LLMs) have emerged as a promising phishing detection mechanism, addressing the limitations of traditional deep learning-based detectors, including poor generalization to previously unseen websites and a lack of interpretability. However, LLMs' effectiveness for phishing detection remains unexplored. This study investigates how to effectively leverage LLMs for phishing detection (including target brand identification) by examining the impact of input modalities (screenshots, logos, HTML, and URLs), temperature settings, and prompt engineering strategies. Using a dataset of 19,131 real-world phishing websites and 243 benign sites, we evaluate seven LLMs -- two commercial models (GPT 4.1 and Gemini 2.0 flash) and five open-source models (Qwen, Llama, Janus, DeepSeek-VL2, and R1) -- alongside two deep learning (DL)-based baselines (PhishIntention and Phishpedia).
  Our findings reveal that commercial LLMs generally outperform open-source models in phishing detection, while DL models demonstrate better performance on benign samples. For brand identification, screenshot inputs achieve optimal results, with commercial LLMs reaching 93-95% accuracy and open-source models, particularly Qwen, achieving up to 92%. However, incorporating multiple input modalities simultaneously or applying one-shot prompts does not consistently enhance performance and may degrade results. Furthermore, higher temperature values reduce performance. Based on these results, we recommend using screenshot inputs with zero temperature to maximize accuracy for LLM-based detectors with HTML serving as auxiliary context when screenshot information is insufficient.

</details>


### [3] [Cooperative Local Differential Privacy: Securing Time Series Data in Distributed Environments](https://arxiv.org/abs/2511.09696)
*Bikash Chandra Singh,Md Jakir Hossain,Rafael Diaz,Sandip Roy,Ravi Mukkamala,Sachin Shetty*

Main category: cs.CR

TL;DR: 本文提出了协作式本地差分隐私（CLDP）机制，通过在多用户间分配噪声向量来增强隐私保护，同时确保聚合后噪声抵消以保持数据统计特性。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备的普及，时间序列数据激增引发了隐私担忧。传统本地差分隐私（LDP）方法在固定时间窗口内添加用户特定噪声，但存在漏洞：聚合过程可能抵消噪声导致隐私泄露。

Method: CLDP机制采用协作式噪声生成与分配策略：多个用户共同生成噪声向量并分配到各自数据上，使得聚合时用户间噪声相互抵消，从而在不影响整体统计特性的前提下保护个体隐私。

Result: 该方法有效解决了基于时间窗口的传统LDP方法的漏洞，能够适应大规模实时数据集，在多用户环境中实现更好的数据效用与隐私平衡。

Conclusion: 协作式噪声分配机制为时序数据隐私保护提供了更鲁棒的解决方案，特别适用于需要实时聚合分析的场景。

Abstract: The rapid growth of smart devices such as phones, wearables, IoT sensors, and connected vehicles has led to an explosion of continuous time series data that offers valuable insights in healthcare, transportation, and more. However, this surge raises significant privacy concerns, as sensitive patterns can reveal personal details. While traditional differential privacy (DP) relies on trusted servers, local differential privacy (LDP) enables users to perturb their own data. However, traditional LDP methods perturb time series data by adding user-specific noise but exhibit vulnerabilities. For instance, noise applied within fixed time windows can be canceled during aggregation (e.g., averaging), enabling adversaries to infer individual statistics over time, thereby eroding privacy guarantees.
  To address these issues, we introduce a Cooperative Local Differential Privacy (CLDP) mechanism that enhances privacy by distributing noise vectors across multiple users. In our approach, noise is collaboratively generated and assigned so that when all users' perturbed data is aggregated, the noise cancels out preserving overall statistical properties while protecting individual privacy. This cooperative strategy not only counters vulnerabilities inherent in time-window-based methods but also scales effectively for large, real-time datasets, striking a better balance between data utility and privacy in multiuser environments.

</details>


### [4] [Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization](https://arxiv.org/abs/2511.09775)
*Dilli Prasad Sharma,Xiaowei Sun,Liang Xue,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 本文提出了一种基于SHAP熵正则化的隐私保护方法，用于减少可解释AIoT应用中的隐私泄露风险。通过引入熵正则化目标，该方法在训练过程中惩罚低熵SHAP分布，使特征贡献更均匀。实验表明，该方法在保持高预测准确性和解释忠实度的情况下，显著降低了隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 随着AIoT在智能家居中的广泛应用，可解释AI方法（如SHAP、LIME）被大量采用以增强透明度。然而这些解释方法容易暴露敏感用户信息，导致新的隐私风险。因此需要开发既能保持模型可解释性又能保护隐私的技术。

Method: 1. 提出SHAP熵正则化方法：在模型训练中引入熵正则化目标函数，惩罚低熵SHAP分布，使特征贡献分布更均匀；2. 设计基于SHAP的隐私攻击套件作为评估工具；3. 在智能家居能耗数据集上比较隐私泄露程度和模型效用指标。

Result: 实验表明：1. 所提方法显著降低了基线模型的隐私泄露（具体攻击成功率下降）；2. 在预测准确度（如MSE/RMSE等指标）和解释忠实度（如特征权重分布均匀性）方面与基线保持相当；3. 验证了熵正则化对保护敏感属性的有效性。

Conclusion: SHAP熵正则化是一种有效的隐私保护可解释AI方法，能在维持模型性能的前提下减少解释过程的信息泄露。该技术有助于构建安全可靠的AIoT系统，未来可拓展至其他解释方法（如LIME）和场景（如医疗/金融）。

Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.

</details>


### [5] [DP-GENG : Differentially Private Dataset Distillation Guided by DP-Generated Data](https://arxiv.org/abs/2511.09876)
*Shuo Shi,Jinghuai Zhang,Shijie Jiang,Chunyi Zhou,Yuyuan Li,Mengying Zhu,Yangyang Wu,Tianyu Du*

Main category: cs.CR

TL;DR: 本文提出了一种新型的私有保护数据集蒸馏框架LibN，该框架利用差分隐私（DP）生成的数据解决了现有DP-DD方法的缺陷，通过DP生成的数据初始化蒸馏数据集以增强真实感，然后基于DP特征匹配技术对原始数据集进行蒸馏，同时训练一个专家模型以对齐蒸馏样本的类别分布，并在实验中证明其显著超越了最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 基于现有差分隐私数据集蒸馏方法在处理原始数据时效率低下、实用性差以及易泄露隐私的问题，本文旨在提出一种新的框架，提高隐私保护能力和数据实用性，同时抵御成员推理攻击。

Method: 1. 使用DP生成的数据初始化蒸馏数据集以增强真实性。2. 设计一种改进的DP特征匹配技术对原始数据集在小的隐私预算下进行蒸馏。3. 训练专家模型以对齐蒸馏样本的类别分布。4. 采用隐私预算分配策略优化DP组件之间的预算消耗。

Result: 在多个实验中，LibN在数据集实用性和对抗成员推理攻击方面的鲁棒性显著优于最先进的DP-DD方法，为隐私保护数据集蒸馏建立了新的范式。

Conclusion: LibN通过巧妙整合DP生成的数据和优化的训练、预算策略，成功地提升了数据集蒸馏的实用性和隐私保护效果，同时降低了信息泄露风险，为解决DP-DD的实际挑战提供了新的研究方向。

Abstract: Dataset distillation (DD) compresses large datasets into smaller ones while preserving the performance of models trained on them. Although DD is often assumed to enhance data privacy by aggregating over individual examples, recent studies reveal that standard DD can still leak sensitive information from the original dataset due to the lack of formal privacy guarantees. Existing differentially private (DP)-DD methods attempt to mitigate this risk by injecting noise into the distillation process. However, they often fail to fully leverage the original dataset, resulting in degraded realism and utility. This paper introduces \libn, a novel framework that addresses the key limitations of current DP-DD by leveraging DP-generated data. Specifically, \lib initializes the distilled dataset with DP-generated data to enhance realism. Then, generated data refines the DP-feature matching technique to distill the original dataset under a small privacy budget, and trains an expert model to align the distilled examples with their class distribution. Furthermore, we design a privacy budget allocation strategy to determine budget consumption across DP components and provide a theoretical analysis of the overall privacy guarantees. Extensive experiments show that \lib significantly outperforms state-of-the-art DP-DD methods in terms of both dataset utility and robustness against membership inference attacks, establishing a new paradigm for privacy-preserving dataset distillation.

</details>


### [6] [Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code](https://arxiv.org/abs/2511.09879)
*Catherine Xia,Manar H. Alalfi*

Main category: cs.CR

TL;DR: 本文提出通过筛选无漏洞的代码来构建安全训练集，以提升AI编程助手生成代码的安全性。实验表明，使用该数据集训练的模型在保持功能正确性的同时，显著减少了生成代码中的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI编程助手生成的代码常含基础安全漏洞。研究发现，这主要源于训练数据集中存在的漏洞代码。因此，需构建无漏洞的高质量训练集来提升生成代码的安全可靠性。

Method: 1. 使用静态分析工具筛选现有Python代码库，构建仅含无漏洞函数的训练数据集；2. 分别用原始数据集和筛选数据集训练两个Transformer模型；3. 对比评估生成代码的功能正确性和安全性。

Result: 使用安全数据集训练的模型在生成代码时表现出：1. 安全性显著提升（漏洞数量减少）；2. 功能正确性与原始模型相当。

Conclusion: 验证了安全训练数据对提升AI编程助手可靠性的关键作用。未来需进一步优化模型架构与评估方法以强化该效果。

Abstract: AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.

</details>


### [7] [Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems](https://arxiv.org/abs/2511.10050)
*Go Tsuruoka,Takami Sato,Qi Alfred Chen,Kazuki Nomoto,Ryunosuke Kobayashi,Yuna Tanaka,Tatsuya Mori*

Main category: cs.CR

TL;DR: 提出了一种新型对抗性攻击方法——对抗性逆向反射贴片(ARP)，利用逆向反射材料仅在受害者车灯照明下激活的特性，结合贴片攻击的高部署性和激光投影的隐蔽性。通过开发逆向反射模拟方法和黑盒优化，ARP在动态场景下成功率达到93.4%以上，并在现实条件下对商业交通标志识别系统的攻击成功率超60%。同时提出了采用偏振滤波器的DPR Shield防御方案，对停止标志和限速标志的防御成功率超过75%。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在交通标志识别系统中存在视觉可检测性或实施限制问题。为探索尚未被发现的系统漏洞，结合贴片攻击的易部署性和激光投影的隐蔽性优势，提出基于逆向反射材料的攻击向量。

Method: 1) 利用仅在车辆头灯照射下激活的逆向反射材料制作攻击贴片；2) 开发逆向反射模拟方法；3) 采用黑盒优化提升攻击效果；4) 针对攻击提出基于偏振滤波器的DPR Shield防御方案。

Result: 1) 动态场景35米距离攻击成功率≥93.4%；2) 现实商业系统攻击成功率≥60%；3) 隐蔽性评分比传统贴片攻击高≥1.9%；4) DPR防御方案对特定标志的防御成功率≥75%。

Conclusion: ARP攻击方法有效结合隐蔽性和可部署性，显著提升攻击效果；同时提出的偏振滤波器防御方案能有效减轻此类攻击风险，为自动驾驶安全提供新解决方案。

Abstract: Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\geq$93.4\% success rate in dynamic scenarios at 35 meters and $\geq$60\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\geq$1.9\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\geq$75\% defense success rates for stop signs and speed limit signs against micro-prism patches.

</details>


### [8] [Enhanced Anonymous Credentials for E-Voting Systems](https://arxiv.org/abs/2511.10265)
*Tomasz Truderung*

Main category: cs.CR

TL;DR: 本文提出了一种简单方法，通过完美隐藏承诺的匿名凭证机制，在电子投票系统中实现持久隐私，同时支持强制验证和一致性检查。


<details>
  <summary>Details</summary>
Motivation: 现有匿名凭证方法在结合其他安全特性（如第二设备验证）时可能面临挑战，需强化选民与凭证的绑定且不牺牲隐私。

Method: 在匿名凭证机制中添加完美隐藏承诺，将凭证与选民身份安全关联，既保持不可链接性又支持审计所需的一致性验证。

Result: 方案在保留选票与身份永久不可链接性的前提下，使系统能执行强制投票验证和审计阶段的一致性检查。

Conclusion: 完美隐藏承诺为匿名凭证提供了隐私与功能性的平衡，适用于需同时满足多重要求的投票系统。

Abstract: A simple and practical method for achieving everlasting privacy in e-voting systems, without relying on advanced cryptographic techniques, is to use anonymous voter credentials. The simplicity of this approach may, however, create some challenges, when combined with other security features, such as cast-as-intended verifiability with second device and second-factor authentication.
  This paper considers a simple augmentation to the anonymous credential mechanism, using perfectly hiding commitments to link such credentials to the voter identities. This solution strengthens the binding between voters and their credentials while preserving everlasting privacy. It ensures that published ballots remain unlinkable to voter identities, yet enables necessary consistency checks during ballot casting and ballot auditing

</details>


### [9] [Enhanced Privacy Leakage from Noise-Perturbed Gradients via Gradient-Guided Conditional Diffusion Models](https://arxiv.org/abs/2511.10423)
*Jiayang Meng,Tao Huang,Hong Chen,Chen Hou,Guolong Zheng*

Main category: cs.CR

TL;DR: 提出了一种基于条件扩散模型（GG-CDM）的方法，用于从被噪声扰动的梯度中重建私有图像，提升现有梯度反演攻击在噪声防御下的性能，并进行了理论分析和实验验证。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中梯度传输隐藏隐私风险，但现有梯度反演攻击在梯度被噪声扰动时性能下降。为克服噪声防御的限制，需要更鲁棒的攻击方法。

Method: 利用有条件扩散模型固有的去噪能力，提出梯度引导的条件扩散模型（GG-CDM），直接从泄漏的梯度重构图像，无需目标数据分布的先验知识。

Result: 实验证明在多种情况下（包括高斯噪声扰动）重建性能优于现有方法，理论分析揭示了噪声强度和模型结构对重建质量的影响边界。

Conclusion: GG-CDM能有效突破噪声防御，对联邦学习隐私安全提出新挑战；理论分析为理解和改进攻击提供了依据。

Abstract: Federated learning synchronizes models through gradient transmission and aggregation. However, these gradients pose significant privacy risks, as sensitive training data is embedded within them. Existing gradient inversion attacks suffer from significantly degraded reconstruction performance when gradients are perturbed by noise-a common defense mechanism. In this paper, we introduce Gradient-Guided Conditional Diffusion Models (GG-CDMs) for reconstructing private images from leaked gradients without prior knowledge of the target data distribution. Our approach leverages the inherent denoising capability of diffusion models to circumvent the partial protection offered by noise perturbation, thereby improving attack performance under such defenses. We further provide a theoretical analysis of the reconstruction error bounds and the convergence properties of attack loss, characterizing the impact of key factors-such as noise magnitude and attacked model architecture-on reconstruction quality. Extensive experiments demonstrate our attack's superior reconstruction performance with Gaussian noise-perturbed gradients, and confirm our theoretical findings.

</details>


### [10] [On the Detectability of Active Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2511.10502)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 本文分析了联邦学习中存在的梯度反演攻击（GIA）风险，尤其是新型主动攻击的隐蔽性。研究提出了基于权重结构异常及损失/梯度动态的轻量级客户端检测方法，有效识别攻击而无需修改联邦学习协议。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护客户端数据隐私，但交换的梯度仍面临梯度反演攻击（GIA）威胁。近期新型主动攻击声称比以往更隐蔽，但其可检测性尚未得到充分验证。

Method: 提出两种轻量级客户端检测技术：1）统计异常的权重结构检测；2）基于损失和梯度动态异常的检测。在多种配置下评估了四种前沿主动GIA。

Result: 跨多场景实验证明，所提方法能高效识别主动GIA（包括最新隐蔽攻击），且无需改动联邦学习协议。

Conclusion: 即使新型主动攻击隐蔽性强，客户端仍可通过轻量统计方法实现有效检测，从而维护联邦学习的隐私承诺。

Abstract: One of the key advantages of Federated Learning (FL) is its ability to collaboratively train a Machine Learning (ML) model while keeping clients' data on-site. However, this can create a false sense of security. Despite not sharing private data increases the overall privacy, prior studies have shown that gradients exchanged during the FL training remain vulnerable to Gradient Inversion Attacks (GIAs). These attacks allow reconstructing the clients' local data, breaking the privacy promise of FL. GIAs can be launched by either a passive or an active server. In the latter case, a malicious server manipulates the global model to facilitate data reconstruction. While effective, earlier attacks falling under this category have been demonstrated to be detectable by clients, limiting their real-world applicability. Recently, novel active GIAs have emerged, claiming to be far stealthier than previous approaches. This work provides the first comprehensive analysis of these claims, investigating four state-of-the-art GIAs. We propose novel lightweight client-side detection techniques, based on statistically improbable weight structures and anomalous loss and gradient dynamics. Extensive evaluation across several configurations demonstrates that our methods enable clients to effectively detect active GIAs without any modifications to the FL training protocol.

</details>


### [11] [How Worrying Are Privacy Attacks Against Machine Learning?](https://arxiv.org/abs/2511.10516)
*Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 本文质疑了机器学习(ML)隐私风险的传统认知，分析了多种隐私攻击在现实世界的真实表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨披露训练过的机器学习模型所带来的实际隐私风险程度。当前监管框架假设公开ML模型带来的隐私风险等同于披露原始数据，但作者认为这个假设需要进一步验证。

Method: 作者系统地梳理了对抗预测性和生成式模型的隐私攻击类型，重点评估了成员推理攻击（MIA）、属性推理攻击、重构攻击等主要攻击模式在现实环境中的实际影响。

Result: 研究发现：现实环境中隐私攻击的实际有效性远低于文献中表面显示的威胁程度。大多数攻击在理论场景中有效，但在面对实际部署环境的复杂性时会显著失效。

Conclusion: 结论指出监管机构和行业应重新评估现有隐私风险假设。本文呼吁区分数据披露风险与实际攻击效果，主张建立更均衡的机器学习模型共享监管框架。

Abstract: In several jurisdictions, the regulatory framework on the release and sharing of personal data is being extended to machine learning (ML). The implicit assumption is that disclosing a trained ML model entails a privacy risk for any personal data used in training comparable to directly releasing those data. However, given a trained model, it is necessary to mount a privacy attack to make inferences on the training data. In this concept paper, we examine the main families of privacy attacks against predictive and generative ML, including membership inference attacks (MIAs), property inference attacks, and reconstruction attacks. Our discussion shows that most of these attacks seem less effective in the real world than what a prima face interpretation of the related literature could suggest.

</details>
