<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 38]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Hierarchical Multi-Modal Threat Intelligence Fusion Without Aligned Data: A Practical Framework for Real-World Security Operations](https://arxiv.org/abs/2510.15953)
*Sisir Doppalapudi*

Main category: cs.CR

TL;DR: 提出了一种名为HM-TIF的分层多模态威胁情报融合框架，用于处理自然未对齐的多模态攻击数据，通过分层交叉注意力和时间相关协议实现高效威胁检测。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态威胁检测工具存在数据孤立且无法自然对齐的问题，导致难以有效关联不同模态的安全数据流。

Method: 开发了分层交叉注意力机制（动态适应数据可用性和威胁情境）及时间相关协议（保持统计独立性），在不对齐数据条件下实现多模态融合。

Result: 在三个数据集上测试显示：准确率达88.7%，误报率降低32%，且在多模态缺失时仍保持鲁棒性。

Conclusion: 该框架首次为未对齐数据提供实用解决方案，为安全团队处理异构数据提供部署指南，证明了不完美对齐下多模态融合的可行性。

Abstract: Multi-modal threat detection faces a fundamental challenge that involves
security tools operating in isolation, and this creates streams of network,
email, and system data with no natural alignment or correlation. We present
Hierarchical Multi-Modal Threat Intelligence Fusion (HM-TIF), a framework
explicitly designed for this realistic scenario where naturally aligned
multi-modal attack data does not exist. Unlike prior work that assumes or
creates artificial alignment, we develop principled methods for correlating
independent security data streams while maintaining operational validity. Our
architecture employs hierarchical cross-attention with dynamic weighting that
adapts to data availability and threat context, coupled with a novel temporal
correlation protocol that preserves statistical independence. Evaluation on
UNSW-NB15, CSE-CIC-IDS2018, and CICBell-DNS2021 datasets demonstrates that
HM-TIF achieves 88.7% accuracy with a critical 32% reduction in false positive
rates, even without true multi-modal training data. The framework maintains
robustness when modalities are missing, making it immediately deployable in
real security operations where data streams frequently have gaps. Our
contributions include: (i) the first multi-modal security framework explicitly
designed for non-aligned data, (ii) a temporal correlation protocol that avoids
common data leakage pitfalls, (iii) empirical validation that multi-modal
fusion provides operational benefits even without perfect alignment, and (iv)
practical deployment guidelines for security teams facing heterogeneous,
uncoordinated data sources. Index Terms: multi-modal learning, threat
intelligence, non-aligned data, operational security, cross-attention
mechanisms, practical deployment

</details>


### [2] [Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts](https://arxiv.org/abs/2510.15973)
*Tiarnaigh Downey-Webb,Olamide Jogunola,Oluwaseun Ajao*

Main category: cs.CR

TL;DR: 该研究对四种大型语言模型进行了全面的安全评估，揭示了它们在对抗恶意提示攻击时的显著漏洞差异。评估显示不同攻击方式之间存在明显的可迁移性，尤其恶意使用类型提示的危害性最大。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型的对抗攻击日益增多，但现有研究缺乏跨模型的全面安全评估。该研究旨在揭示不同LLM在各种攻击方式下的漏洞分布和攻击可迁移性，从而推动针对性防御机制的发展。

Method: 使用SALAD-Bench数据集的1,200个分层提示，测试四款主流LLM（Phi-2/Llama-2/GPT-3.5/GPT-4）在四类攻击下的表现：人工编写提示、AutoDAN、GCG梯度优化和TAP树攻击。采用攻击成功率评估模型安全性，并通过弗里德曼检验进行统计学差异分析。

Result: 1. Llama安全最佳（攻击成功率3.4%），Phi最易受攻击（7.0%）。2.攻击可迁移性明显：针对Llama设计的GCG/TAP虽对本模型无效，但对GPT-4转移成功率可达17%。3.恶意使用类别攻击成功率最高（10.71%），且统计显著（p<0.001）。

Conclusion: 语言模型安全性能差异较大，攻击方法跨模型迁移特性凸显了开发统一防御框架的难度。研究结果为漏洞修复提供了优先方向（如恶意使用场景），建议业界特别关注黑盒攻击下的可迁移性隐患。

Abstract: This paper presents a systematic security assessment of four prominent Large
Language Models (LLMs) against diverse adversarial attack vectors. We evaluate
Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack
categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG),
and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs
1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six
harm categories. Results demonstrate significant variations in model
robustness, with Llama-2 achieving the highest overall security (3.4% average
attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0%
average attack success rate). We identify critical transferability patterns
where GCG and TAP attacks, though ineffective against their target model
(Llama-2), achieve substantially higher success rates when transferred to other
models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals
significant differences in vulnerability across harm categories ($p < 0.001$),
with malicious use prompts showing the highest attack success rates (10.71%
average). Our findings contribute to understanding cross-model security
vulnerabilities and provide actionable insights for developing targeted defense
mechanisms

</details>


### [3] [Meta-Guardian: An Early Evaluation of an On-device Application to Mitigate Psychography Data Leakage in Immersive Technologies](https://arxiv.org/abs/2510.15989)
*Keshav Sood,Sanjay Selvaraj,Youyang Qu*

Main category: cs.CR

TL;DR: 沉浸式技术（XR）在多个领域有革命性潜力，但实时生物特征数据收集引发隐私担忧。现有研究多忽视头戴式设备中的实时生物数据过滤难点。本文提出Meta-Guardian，一种在VR设备内实时识别/过滤生物信号的隐私保护系统架构，以Unity SDK形式实现，供开发者嵌入隐私设计原则。


<details>
  <summary>Details</summary>
Motivation: 沉浸式技术（VR/AR/MR）需持续采集用户生物特征数据以提供沉浸体验，但敏感数据的实时传输与存储存在重大隐私风险。现有文献集中于技术层面，却未深入解决头戴设备内实时生物数据过滤的关键挑战。

Method: 1) 设计模块化系统架构，在VR头显设备内部署实时生物信号处理流程；2) 开发兼容主流沉浸平台的Unity SDK，整合机器学习模型对信号分类；3) 内置过滤机制拦截敏感数据，确保隐私数据不出设备。

Result: 实现名为Meta-Guardian的可嵌入解决方案：1) 在设备端完成敏感生物信号识别与过滤；2) 通过模块化设计适配多种头显与应用场景；3) 使开发者能便捷遵循隐私设计原则（Privacy-by-Design）。

Conclusion: 首次提出XR设备内实时生物数据过滤框架，以本地化处理解决隐私泄露核心风险。SDK形式降低开发门槛，推动隐私保护成为沉浸式体验的基础组件。

Abstract: The use of Immersive Technologies has shown its potential to revolutionize
many sectors such as health, entertainment, education, and industrial sectors.
Immersive technologies such as Virtual Reality (VR), Augmented reality (AR),
and Mixed Reality (MR) have redefined user interaction through real-time
biometric and behavioral tracking. Although Immersive Technologies (XR)
essentially need the collection of the biometric data which acts as a baseline
to create immersive experience, however, this ongoing feedback information
(includes biometrics) poses critical privacy concerns due to the sensitive
nature of the data collected. A comprehensive review of recent literature
explored the technical dimensions of related problem; however, they largely
overlook the challenge particularly the intricacies of real-time biometric data
filtering within head-mounted display system. Motivated from this, in this
work, we propose a novel privacy-preserving system architecture that identifies
and filters biometric signals (within the VR headset) in real-time before
transmission or storage. Implemented as a modular Unity Software-development
Kit (SDK) compatible with major immersive platforms, our solution (named
Meta-Guardian) employs machine learning models for signal classification and a
filtering mechanism to block sensitive data. This framework aims to enable
developers to embed privacy-by-design principles into immersive experiences on
various headsets and applications.

</details>


### [4] [Breaking Guardrails, Facing Walls: Insights on Adversarial AI for Defenders & Researchers](https://arxiv.org/abs/2510.16005)
*Giacomo Bertollo,Naz Bodemir,Jonah Burgess*

Main category: cs.CR

TL;DR: 研究分析了500名CTF参与者，发现他们能轻易绕过简单的AI护栏，但多层多步防御仍构成挑战，为构建更安全的AI系统提供了见解。


<details>
  <summary>Details</summary>
Motivation: 探究不同AI防御机制在实际攻击中的有效性，以提升AI系统的安全性。

Method: 通过CTF（夺旗赛）形式，组织500名参与者尝试绕过简单AI护栏和多层多步防御。

Result: 参与者能轻松突破简单AI护栏，但多层防御仍构成显著挑战。

Conclusion: 多层多步防御策略能有效提升AI系统安全性，为构建更健壮的防护机制提供了实证依据。

Abstract: Analyzing 500 CTF participants, this paper shows that while participants
readily bypassed simple AI guardrails using common techniques, layered
multi-step defenses still posed significant challenges, offering concrete
insights for building safer AI systems.

</details>


### [5] [On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation](https://arxiv.org/abs/2510.16024)
*Abdulrahman Alhaidari,Balaji Palanisamy,Prashant Krishnamurthy*

Main category: cs.CR

TL;DR: 提出首个完全在链上的去中心化学习框架，通过L2计算降低gas成本，在L1执行验证，并实现智能合约内的低延迟推理。利用PoIm协议治理训练过程并验证模型更新，同时开发量化等技术使多种模型可在以太坊区块gas限制内运行。


<details>
  <summary>Details</summary>
Motivation: 当前DeFi平台因业务逻辑或会计漏洞导致数十亿美元损失，而现有防御措施无法阻止通过私有中继或同一区块内恶意合约发起的攻击。

Method: 1) L2处理高gas消耗计算；2) 将验证后的模型更新传播至L1；3) L1实现gas限制下的低延迟推理。采用PoIm协议验证每次更新需提升至少一项核心指标且不降低其他指标，对抗性提案通过可演化测试集进行经济惩罚。开发量化与循环展开技术适配多种模型（LR/SVM/MLP/CNN/RNN）在以太坊gas限制内运行。

Result: 构建包含298个独特真实攻击（覆盖8条EVM链402笔交易，总损失37.4亿美元）的数据集。技术方案经Z3形式化验证保证链上链下一致性。

Conclusion: 该框架为DeFi漏洞首个数十亿美元规模的解决方案，实现去中心化、低延迟且经济惩罚敌手的链上防御系统。

Abstract: Billions of dollars are lost every year in DeFi platforms by transactions
exploiting business logic or accounting vulnerabilities. Existing defenses
focus on static code analysis, public mempool screening, attacker contract
detection, or trusted off-chain monitors, none of which prevents exploits
submitted through private relays or malicious contracts that execute within the
same block. We present the first decentralized, fully on-chain learning
framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce
cost, (ii) propagates verified model updates to Layer-1, and (iii) enables
gas-bounded, low-latency inference inside smart contracts. A novel
Proof-of-Improvement (PoIm) protocol governs the training process and verifies
each decentralized micro update as a self-verifying training transaction.
Updates are accepted by \textit{PoIm} only if they demonstrably improve at
least one core metric (e.g., accuracy, F1-score, precision, or recall) on a
public benchmark without degrading any of the other core metrics, while
adversarial proposals get financially penalized through an adaptable test set
for evolving threats. We develop quantization and loop-unrolling techniques
that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs
(with support for formally verified decision tree inference) within the
Ethereum block gas limit, while remaining bit-exact to their off-chain
counterparts, formally proven in Z3. We curate 298 unique real-world exploits
(2020 - 2025) with 402 exploit transactions across eight EVM chains,
collectively responsible for \$3.74 B in losses.

</details>


### [6] [Membership Inference over Diffusion-models-based Synthetic Tabular Data](https://arxiv.org/abs/2510.16037)
*Peini Cheng,Amir Bahmani*

Main category: cs.CR

TL;DR: 该研究揭示了基于扩散的表格合成数据生成模型在成员推理攻击（MIAs）下的隐私风险，发现TabDDPM易受攻击而TabSyn表现出韧性。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在合成表格数据中的应用日益增多，其带来的隐私风险尚未得到充分评估。特别是对成员推理攻击的脆弱性，可能泄露原始数据中的敏感信息。

Method: 研究者对两个最新模型（TabDDPM和TabSyn）开发了基于逐步误差比较方法的查询式成员推理攻击框架，通过对比模型在合成数据生成过程中的行为差异来评估其脆弱性。

Result: 实验表明：1) TabDDPM在开发的MIAs攻击下表现出显著漏洞；2) TabSyn对相同攻击具有抵抗力。攻击成功率在TabDDPM上达到具有统计显著性的高位，而TabSyn未显示可被利用的模式。

Conclusion: 扩散模型在合成数据领域的隐私保护能力存在显著差异，TabSyn的架构设计可能蕴含潜在保护机制。研究强调在部署合成数据前必须严格评估隐私风险，并呼吁开发更鲁棒的隐私保护生成方法。

Abstract: This study investigates the privacy risks associated with diffusion-based
synthetic tabular data generation methods, focusing on their susceptibility to
Membership Inference Attacks (MIAs). We examine two recent models, TabDDPM and
TabSyn, by developing query-based MIAs based on the step-wise error comparison
method. Our findings reveal that TabDDPM is more vulnerable to these attacks.
TabSyn exhibits resilience against our attack models. Our work underscores the
importance of evaluating the privacy implications of diffusion models and
encourages further research into robust privacy-preserving mechanisms for
synthetic data generation.

</details>


### [7] [A Novel GPT-Based Framework for Anomaly Detection in System Logs](https://arxiv.org/abs/2510.16044)
*Zeng Zhang,Wenjie Yin,Xiaoqi Li*

Main category: cs.CR

TL;DR: 提出了一种基于GPT的智能系统日志异常检测方法，通过结构化输入设计和Focal Loss优化策略提升检测性能


<details>
  <summary>Details</summary>
Motivation: 传统的日志异常检测面临海量数据处理、异常分布不均及方法精确性不足等挑战

Method: 1. 通过Drain解析器将原始日志转为事件ID序列；2. 采用Focal Loss解决类别不平衡问题

Result: 优化后的GPT-2模型在精确率、召回率和F1分数上显著优于未优化模型，且部分任务表现匹配或超越GPT-3.5 API

Conclusion: 结合结构化输入和Focal Loss的GPT模型能有效提升日志异常检测性能

Abstract: Identification of anomalous events within system logs constitutes a pivotal
element within the frame- work of cybersecurity defense strategies. However,
this process faces numerous challenges, including the management of substantial
data volumes, the distribution of anomalies, and the precision of con-
ventional methods. To address this issue, the present paper puts forward a
proposal for an intelligent detection method for system logs based on Genera-
tive Pre-trained Transformers (GPT). The efficacy of this approach is
attributable to a combination of structured input design and a Focal Loss op-
timization strategy, which collectively result in a substantial enhancement of
the performance of log anomaly detection. The initial approach involves the
conversion of raw logs into event ID sequences through the use of the Drain
parser. Subsequently, the Focal Loss loss function is employed to address the
issue of class imbalance. The experimental re- sults demonstrate that the
optimized GPT-2 model significantly outperforms the unoptimized model in a
range of key metrics, including precision, recall, and F1 score. In specific
tasks, comparable or superior performance has been demonstrated to that of the
GPT-3.5 API.

</details>


### [8] [A Multi-Cloud Framework for Zero-Trust Workload Authentication](https://arxiv.org/abs/2510.16067)
*Saurabh Deochake,Ryan Murphy,Jeremiah Gearheart*

Main category: cs.CR

TL;DR: 本文提出了一个使用工作负载身份联盟(WIF)和OpenID Connect(OIDC)的多云无密钥认证框架，通过加密验证的临时令牌消除静态密钥风险，并在企业级Kubernetes环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 静态长期凭证违反零信任原则，存在安全风险。

Method: 结合WIF和OIDC实现无密钥认证，使用加密验证的临时令牌替代永久私钥。

Result: 在企业级Kubernetes环境中成功验证，显著减少攻击面。

Conclusion: 该框架为跨云工作负载身份管理提供了统一解决方案，支持未来实现基于属性的访问控制。

Abstract: Static, long-lived credentials for workload authentication create untenable
security risks that violate Zero-Trust principles. This paper presents a
multi-cloud framework using Workload Identity Federation (WIF) and OpenID
Connect (OIDC) for secretless authentication. Our approach uses
cryptographically-verified, ephemeral tokens, allowing workloads to
authenticate without persistent private keys and mitigating credential theft.
We validate this framework in an enterprise-scale Kubernetes environment, which
significantly reduces the attack surface. The model offers a unified solution
to manage workload identities across disparate clouds, enabling future
implementation of robust, attribute-based access control.

</details>


### [9] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一个实际的卡片匹配设计，用于人脸验证，其中在卡外生成紧凑的64/128位模板，在卡内通过恒定时间汉明距离进行比较。设计了固定长度的ISO/IEC 7816-4和14443-4命令APDU，并给出了最小化的EEPROM存储映射。在CelebA数据集上测试，得出最佳阈值后进行交易回放并计算端到端时间。结果显示，在1%的FAR下，TPR达到0.836，128位相比64位降低了EER。整体设计满足ISO/IEC标准并符合隐私目标。


<details>
  <summary>Details</summary>
Motivation: 设计一种在智能卡上实现人脸验证的方法，采用短二进制模板和固定长度的命令APDU，确保在满足ISO/IEC传输约束和隐私要求的同时，提高验证速度。

Method: 在卡外使用PCA-ITQ生成64/128位模板，在卡内通过汉明距离进行恒定时间的比较。设计了固定长度的APDU命令以及最小化的EEPROM存储结构。通过ROC/DET曲线确定操作阈值，并在该阈值下模拟交易过程，计算端到端时间。

Result: 在1%的FAR下，两种模板长度的TPR均达到0.836，128位模板的EER更低。在最低速率9.6kbps下，验证时间为43.9ms(64位)和52.3ms(128位)；在38.4kbps下，两者均低于14ms。另外，可选的6字节辅助位对延迟影响可忽略。

Conclusion: 提出了一种高效且符合标准的卡片人脸验证方案，短二进制模板、固定负载的决策专用APDU和恒定时间匹配可满足ISO/IEC传输约束，且符合24745隐私目标。后续将在更多数据集验证并进行硬件级测试。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [10] [The Hidden Cost of Modeling P(X): Vulnerability to Membership Inference Attacks in Generative Text Classifiers](https://arxiv.org/abs/2510.16122)
*Owais Makroo,Siva Rajesh Kasa,Sumegh Roychowdhury,Karan Gupta,Nikhil Pattisapu,Santhosh Kasa,Sumit Negi*

Main category: cs.CR

TL;DR: 该论文分析了生成式分类器在成员推理攻击（MIA）中的脆弱性，通过与判别式和伪生成分类器的比较，发现显式建模联合概率的生成式分类器最易受攻击，提醒在隐私敏感应用中需谨慎使用。


<details>
  <summary>Details</summary>
Motivation: 现有的成员推理攻击研究缺乏对生成式与判别式分类器的系统性比较，而生成式分类器在隐私保护场景下的潜在风险尚未充分探究。

Method: 结合理论分析与实证评估：1）理论推导生成式分类器易受MIA的原因；2）在9个基准数据集上测试判别式/生成式/伪生成文本分类器在不同训练数据量下的表现；3）使用多种MIA策略（如基于置信度、损失函数的攻击）评估成员泄漏程度。

Result: 1）显式建模P(X,Y)的生成式分类器对MIA最脆弱；2）其标准推断流程会加剧隐私风险；3）分类器设计存在效用-隐私权衡（生成式分类器高实用但低隐私）

Conclusion: 生成式分类器的高MIA风险揭示了模型内在的隐私缺陷，需在隐私敏感场景（如医疗、金融）中谨慎部署，并呼吁开发兼顾效用与隐私的新型生成架构。

Abstract: Membership Inference Attacks (MIAs) pose a critical privacy threat by
enabling adversaries to determine whether a specific sample was included in a
model's training dataset. Despite extensive research on MIAs, systematic
comparisons between generative and discriminative classifiers remain limited.
This work addresses this gap by first providing theoretical motivation for why
generative classifiers exhibit heightened susceptibility to MIAs, then
validating these insights through comprehensive empirical evaluation. Our study
encompasses discriminative, generative, and pseudo-generative text classifiers
across varying training data volumes, evaluated on nine benchmark datasets.
Employing a diverse array of MIA strategies, we consistently demonstrate that
fully generative classifiers which explicitly model the joint likelihood
$P(X,Y)$ are most vulnerable to membership leakage. Furthermore, we observe
that the canonical inference approach commonly used in generative classifiers
significantly amplifies this privacy risk. These findings reveal a fundamental
utility-privacy trade-off inherent in classifier design, underscoring the
critical need for caution when deploying generative classifiers in
privacy-sensitive applications. Our results motivate future research directions
in developing privacy-preserving generative classifiers that can maintain
utility while mitigating membership inference vulnerabilities.

</details>


### [11] [Prompt injections as a tool for preserving identity in GAI image descriptions](https://arxiv.org/abs/2510.16128)
*Kate Glazko,Jennifer Mankoff*

Main category: cs.CR

TL;DR: 将提示词注入技术重新定义为一种工具，使非直接用户（如内容所有者）能够保护自身权益，避免生成式AI对非直接用户的偏见和缺乏代表性问题，帮助保留身份信息。


<details>
  <summary>Details</summary>
Motivation: 生成式AI对与系统非直接交互但内容被AI处理的非直接用户造成风险，如偏见和缺乏代表性问题。现有的风险缓解方法多依赖自上而下或外部干预。本文将提示词注入视为一种赋权工具，使非直接用户能够在自己的内容中主动抵御潜在伤害。

Method: 以海报形式展示案例研究：非直接用户通过在图像中嵌入提示词注入，防止生成式AI系统在描述该图像时省略开发者的性别和残障身份。具体实现技术细节在摘要中未详述。

Result: 案例研究证明：图像所有者可以通过嵌入提示词注入，在GAI系统生成的描述中成功保留自己的性别和残障身份。作者同时揭示了这一技术的颠覆性用途模式——从传统恶意攻击向量转变为用户自我保护工具。

Conclusion: 提示词注入并非恶意攻击手段，而是被重新定位为非直接用户的维权工具，该方法能在现有系统框架下实现用户赋权对抗技术伤害，为缓解间接用户风险开辟新途径，挑战了传统治理范式。

Abstract: Generative AI risks such as bias and lack of representation impact people who
do not interact directly with GAI systems, but whose content does: indirect
users. Several approaches to mitigating harms to indirect users have been
described, but most require top down or external intervention. An emerging
strategy, prompt injections, provides an empowering alternative: indirect users
can mitigate harm against them, from within their own content. Our approach
proposes prompt injections not as a malicious attack vector, but as a tool for
content/image owner resistance. In this poster, we demonstrate one case study
of prompt injections for empowering an indirect user, by retaining an image
owner's gender and disabled identity when an image is described by GAI.

</details>


### [12] [C/N0 Analysis-Based GPS Spoofing Detection with Variable Antenna Orientations](https://arxiv.org/abs/2510.16229)
*Vienna Li,Justin Villa,Dan Diessner,Jayson Clifford,Laxima Niure Kandel*

Main category: cs.CR

TL;DR: 本文提出一种通过分析天线方向变化引起的卫星载噪比（C/N₀）波动来检测GPS欺骗攻击的概念验证方法。实验表明，在真实信号下，C/N₀随天线方向自然变化；而欺骗信号在正对欺骗源时C/N₀最高，倾斜时则降低。这种差异可作为航空器识别欺骗的线索。


<details>
  <summary>Details</summary>
Motivation: GPS欺骗对航空安全构成严重威胁，现有防御系统成本高昂且难以普及。本研究试图通过载噪比(C/N₀)的方向依赖性差异，开发适用于通用航空/UAV的低成本欺骗检测方案。

Method: 使用u-blox接收器和GPS模拟器，在三种静态天线方向（水平、左倾、右倾）下分别采集真实信号与欺骗信号的C/N₀数据，通过对比方向变化对载噪比的影响模式差异进行检测。

Result: 真实信号中C/N₀随方向自然波动，而欺骗信号在正对欺骗源时C/N₀最高（水平方向），倾斜时显著降低约5dB。方向变化导致的载噪比异常模式成为识别欺骗的关键特征。

Conclusion: 通过简单机动（如短暂倾斜）诱发C/N₀变化可有效检测欺骗攻击，该方法为通用航空/UAV系统提供了低复杂度的欺骗防御新思路。

Abstract: GPS spoofing poses a growing threat to aviation by falsifying satellite
signals and misleading aircraft navigation systems. This paper demonstrates a
proof-of-concept spoofing detection strategy based on analyzing satellite
Carrier-to-Noise Density Ratio (C/N$_0$) variation during controlled static
antenna orientations. Using a u-blox EVK-M8U receiver and a GPSG-1000 satellite
simulator, C/N$_0$ data is collected under three antenna orientations flat,
banked right, and banked left) in both real-sky (non-spoofed) and spoofed
environments. Our findings reveal that under non-spoofed signals, C/N$_0$
values fluctuate naturally with orientation, reflecting true geometric
dependencies. However, spoofed signals demonstrate a distinct pattern: the flat
orientation, which directly faces the spoofing antenna, consistently yielded
the highest C/N$_0$ values, while both banked orientations showed reduced
C/N$_0$ due to misalignment with the spoofing source. These findings suggest
that simple maneuvers such as brief banking to induce C/N$_0$ variations can
provide early cues of GPS spoofing for general aviation and UAV systems.

</details>


### [13] [Efficient and Privacy-Preserving Binary Dot Product via Multi-Party Computation](https://arxiv.org/abs/2510.16331)
*Fatemeh Jafarian Dehkordi,Elahe Vedadi,Alireza Feizbakhsh,Yasaman Keshtkarjahromi,Hulya Seferoglu*

Main category: cs.CR

TL;DR: 提出了一种名为BiMPC的二进制多方计算框架，用于解决树状垂直联邦学习中的隐私保护问题，特别针对比特级操作进行了优化。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私保护技术如Shamir秘密共享和多方计算（MPC）在处理二进制数据的比特级操作（如垂直联邦学习中树模型的点积计算）时效率不足，需要新型解决方案。

Method: 1. 提出了Dot Product via Modular Addition (DoMA)方法，利用常规加法和模加运算进行二进制向量点积的高效计算。2. 引入随机掩码（高数域）处理线性计算和基于三方不经意传输（triot）的协议处理非线性部分。

Result: BiMPC框架在保护每个比特隐私的前提下提升了效率，并通过理论分析证明其隐私可靠性及在分布式环境中的可扩展性。

Conclusion: BiMPC框架填补了比特级操作的隐私保护技术空白，为树状垂直联邦学习提供了高效安全的计算方案。

Abstract: Striking a balance between protecting data privacy and enabling collaborative
computation is a critical challenge for distributed machine learning. While
privacy-preserving techniques for federated learning have been extensively
developed, methods for scenarios involving bitwise operations, such as
tree-based vertical federated learning (VFL), are still underexplored.
Traditional mechanisms, including Shamir's secret sharing and multi-party
computation (MPC), are not optimized for bitwise operations over binary data,
particularly in settings where each participant holds a different part of the
binary vector. This paper addresses the limitations of existing methods by
proposing a novel binary multi-party computation (BiMPC) framework. The BiMPC
mechanism facilitates privacy-preserving bitwise operations, with a particular
focus on dot product computations of binary vectors, ensuring the privacy of
each individual bit. The core of BiMPC is a novel approach called Dot Product
via Modular Addition (DoMA), which uses regular and modular additions for
efficient binary dot product calculation. To ensure privacy, BiMPC uses random
masking in a higher field for linear computations and a three-party oblivious
transfer (triot) protocol for non-linear binary operations. The privacy
guarantees of the BiMPC framework are rigorously analyzed, demonstrating its
efficiency and scalability in distributed settings.

</details>


### [14] [EditMark: Watermarking Large Language Models based on Model Editing](https://arxiv.org/abs/2510.16367)
*Shuai Li,Kejiang Chen,Jun Jiang,Jie Zhang,Qiyi Yao,Kai Zeng,Weiming Zhang,Nenghai Yu*

Main category: cs.CR

TL;DR: 提出了一种名为EditMark的新方法，利用模型编辑技术为大型语言模型(LLMs)嵌入无需重新训练、隐蔽且不影响模型性能的水印。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法需使用带水印数据集重新训练模型，带来高昂计算负担和性能损失，且水印文本不够自然隐蔽。

Method: 利用多答案问题的特性，通过改进的模型编辑技术（自适应多轮稳定编辑策略+噪声矩阵注入）直接修改LLMs权重，为不同答案绑定唯一水印。

Result: 20秒内嵌入32位水印（相比微调方法6875秒），提取成功率100%；水印具备高隐蔽性，并能抵抗常见攻击。

Conclusion: EditMark在保持模型性能前提下，实现了高效、隐蔽的水印嵌入，为LLMs版权保护提供了新方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, but
their training requires extensive data and computational resources, rendering
them valuable digital assets. Therefore, it is essential to watermark LLMs to
protect their copyright and trace unauthorized use or resale. Existing methods
for watermarking LLMs primarily rely on training LLMs with a watermarked
dataset, which entails burdensome training costs and negatively impacts the
LLM's performance. In addition, their watermarked texts are not logical or
natural, thereby reducing the stealthiness of the watermark. To address these
issues, we propose EditMark, the first watermarking method that leverages model
editing to embed a training-free, stealthy, and performance-lossless watermark
for LLMs. We observe that some questions have multiple correct answers.
Therefore, we assign each answer a unique watermark and update the weights of
LLMs to generate corresponding questions and answers through the model editing
technique. In addition, we refine the model editing technique to align with the
requirements of watermark embedding. Specifically, we introduce an adaptive
multi-round stable editing strategy, coupled with the injection of a noise
matrix, to improve both the effectiveness and robustness of the watermark
embedding. Extensive experiments indicate that EditMark can embed 32-bit
watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a
watermark extraction success rate of 100%, which demonstrates its effectiveness
and efficiency. External experiments further demonstrate that EditMark has
fidelity, stealthiness, and a certain degree of robustness against common
attacks.

</details>


### [15] [$ρ$Hammer: Reviving RowHammer Attacks on New Architectures via Prefetching](https://arxiv.org/abs/2510.16544)
*Weijie Chen,Shan Tang,Yulin Tang,Xiapu Luo,Yinqian Zhang,Weizhong Qiang*

Main category: cs.CR

TL;DR: 提出了一种名为 $\rho$Hammer 的新型 Rowhammer 攻击框架，解决了在新一代架构上的三个核心挑战：地址映射逆推、突破激活率瓶颈、及对抗推测执行干扰，效果显著优于传统攻击方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于负载的 Rowhammer 攻击在最新架构上（如 Intel Alder 和 Raptor Lake）效率极低，因此需要设计更先进的技术来应对现代内存控制器的防御机制。

Method: 1) 高效的 DRAM 地址映射逆推方法：使用选择性成对测量和结构化推导。2) 新型基于预取的锤击范式：利用预取指令异步特性及多存储体并行化。3) 抗推测执行干扰：控制流混淆和基于 NOP 的伪屏障技术。

Result: 在四款最新 Intel 架构上验证：诱导超 20 万额外比特翻转；在 Comet 及 Rocket Lake 上翻转率较基线高 112 倍；首次在 Raptor Lake 上复活 Rowhammer 攻击（291 次/分钟），传统攻击完全失效。

Conclusion: $\rho$Hammer 显著提升了新型处理器上的 Rowhammer 攻击效率，突破了现有防御技术限制。

Abstract: Rowhammer is a critical vulnerability in dynamic random access memory (DRAM)
that continues to pose a significant threat to various systems. However, we
find that conventional load-based attacks are becoming highly ineffective on
the most recent architectures such as Intel Alder and Raptor Lake. In this
paper, we present $\rho$Hammer, a new Rowhammer framework that systematically
overcomes three core challenges impeding attacks on these new architectures.
First, we design an efficient and generic DRAM address mapping
reverse-engineering method that uses selective pairwise measurements and
structured deduction, enabling recovery of complex mappings within seconds on
the latest memory controllers. Second, to break through the activation rate
bottleneck of load-based hammering, we introduce a novel prefetch-based
hammering paradigm that leverages the asynchronous nature of x86 prefetch
instructions and is further enhanced by multi-bank parallelism to maximize
throughput. Third, recognizing that speculative execution causes more severe
disorder issues for prefetching, which cannot be simply mitigated by memory
barriers, we develop a counter-speculation hammering technique using
control-flow obfuscation and optimized NOP-based pseudo-barriers to maintain
prefetch order with minimal overhead. Evaluations across four latest Intel
architectures demonstrate $\rho$Hammer's breakthrough effectiveness: it induces
up to 200K+ additional bit flips within 2-hour attack pattern fuzzing processes
and has a 112x higher flip rate than the load-based hammering baselines on
Comet and Rocket Lake. Also, we are the first to revive Rowhammer attacks on
the latest Raptor Lake architecture, where baselines completely fail, achieving
stable flip rates of 2,291/min and fast end-to-end exploitation.

</details>


### [16] [Toward Understanding Security Issues in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2510.16558)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 首次对MCP生态系统进行安全分析，揭示了其架构中的安全漏洞，包括恶意服务器操控模型导致的敏感数据泄露等风险，并提出防御策略。


<details>
  <summary>Details</summary>
Motivation: 尽管MCP生态系统在AI应用中迅速普及，但其架构和安全隐患缺乏系统研究。本文旨在填补这一空白，分析MCP的核心组成部分（宿主、注册中心和服务器）及其交互中的安全风险。

Method: 将MCP生态系统分解为三个核心组件（宿主、注册中心、服务器），通过定性分析揭示漏洞（如宿主缺乏输出验证机制）、收集并分析来自6个公共注册中心的67,057个服务器的数据集进行定量验证。

Result: 发现宿主缺乏LLM输出验证机制，使恶意服务器可操控模型行为并引发安全威胁（如敏感数据泄露）；大量服务器可被劫持（注册中心缺少服务器审核流程）。

Conclusion: MCP生态系统存在严重安全漏洞，提出实用防御策略并已向相关方披露。

Abstract: The Model Context Protocol (MCP) is an emerging open standard that enables
AI-powered applications to interact with external tools through structured
metadata. A rapidly growing ecosystem has formed around MCP, including a wide
range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP
registries (i.e., mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm),
and thousands of community-contributed MCP servers. Although the MCP ecosystem
is gaining traction, there has been little systematic study of its architecture
and associated security risks. In this paper, we present the first
comprehensive security analysis of the MCP ecosystem. We decompose MCP
ecosystem into three core components: hosts, registries, and servers, and study
the interactions and trust relationships among them. Users search for servers
on registries and configure them in the host, which translates LLM-generated
output into external tool invocations provided by the servers and executes
them. Our qualitative analysis reveals that hosts lack output verification
mechanisms for LLM-generated outputs, enabling malicious servers to manipulate
model behavior and induce a variety of security threats, including but not
limited to sensitive data exfiltration. We uncover a wide range of
vulnerabilities that enable attackers to hijack servers, due to the lack of a
vetted server submission process in registries. To support our analysis, we
collect and analyze a dataset of 67,057 servers from six public registries. Our
quantitative analysis demonstrates that a substantial number of servers can be
hijacked by attackers. Finally, we propose practical defense strategies for MCP
hosts, registries, and users. We responsibly disclosed our findings to affected
hosts and registries.

</details>


### [17] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: 本文提出Patronus防御框架，保护文本到图像(T2I)模型免受白盒攻击，包括内部调制器阻止恶意输入，和强化不可微调的对齐机制。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型的安全措施（如内容审核或模型对齐）无法抵抗了解模型参数并可通过微调修改的白盒攻击者。

Method: 设计内部调制器将不安全输入特征解码为零向量，并添加非微调学习机制以增强模型对齐。

Result: 实验证实框架在安全内容生成上性能完整，能有效拒绝不安全内容生成，且抵抗多种白盒微调攻击。

Conclusion: Patronus为T2I模型提供了针对白盒对抗的整体防护解决方案。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [18] [DESTinE Block: Private Blockchain Based Data Storage Framework for Power System](https://arxiv.org/abs/2510.16593)
*Khandaker Akramul Haque,Katherine R. Davis*

Main category: cs.CR

TL;DR: DESTinE Block是一个基于区块链的数据存储框架，专为电力系统设计，优化资源受限环境（如单板计算机）。它采用IPFS存储大文件，并将元数据（包括CID、上传者身份、管理员验证和时间戳）存储在自定义区块链上。双区块链架构和PoA共识机制确保了安全性和效率，可在树莓派等设备上部署。测试表明其在分布式电力系统中具有低硬件需求下的防篡改数据存储潜力。


<details>
  <summary>Details</summary>
Motivation: 针对电力系统在资源受限环境（如边缘设备）中安全存储数据的需求，现有方案往往资源消耗大。因此，作者提出轻量级区块链框架，解决大文件存储、元数据可追溯性问题。

Method: 1. 分离存储：IPFS存大文件，DESTinE Block链存元数据（CID等）。2. 双区块链抽象：隔离IPFS存储层以提升安全性。3. PoA共识：需管理员和上传者双密钥协作生成区块。4. 高效设计：适配树莓派等设备。

Result: 框架在x86和ARM64设备测试成功，与Multichain方案对比显示：在保证防篡改数据存储的同时满足低硬件需求，适合智能电网的分散化日志和测量存储。

Conclusion: DESTinE Block为分布式电力基础设施提供了一种轻量级、安全的防篡改数据存储方案，尤其适合边缘设备。未来可进一步优化性能。

Abstract: This paper presents DESTinE Block, a blockchain-based data storage framework
designed for power systems and optimized for resource-constrained environments,
including grid-edge devices such as single-board computers. The proposed
architecture leverages the InterPlanetary File System (IPFS) for storing large
files while maintaining secure and traceable metadata on a custom blockchain
named DESTinE Block. The metadata, comprising the IPFS Content Identifier
(CID), uploader identity, administrator verification, and timestamp; is
immutably recorded on-chain to ensure authenticity and integrity. DESTinE Block
adopts a dual-blockchain abstraction, where the blockchain remains unaware of
the IPFS storage layer to enhance security and limit the exposure of sensitive
file data. The consensus mechanism is based on Proof of Authority (PoA), where
both an administrator and an uploader with distinct cryptographic key pairs are
required to create a block collaboratively. Each block contains verified
signatures of both parties and is designed to be computationally efficient,
enabling deployment on devices like the Raspberry Pi 5. The framework was
tested on both an x86-based device and an ARM64-based Raspberry Pi,
demonstrating its potential for secure, decentralized logging and measurement
storage in smart grid applications. Moreover, DESTinE Block is compared with a
similar framework based on Multichain. The results indicate that DESTinE Block
provides a promising solution for tamper-evident data retention in distributed
power system infrastructure while maintaining minimal hardware requirements.

</details>


### [19] [Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models](https://arxiv.org/abs/2510.16706)
*Hongjie Zhang,Zhiqi Zhao,Hanzhou Wu,Zhihua Xia,Athanasios V. Vasilakos*

Main category: cs.CR

TL;DR: 本文提出了一种针对嵌入即服务（EaaS）模型的新型指纹框架，利用嵌入空间的拓扑结构进行所有权验证，克服了传统水印技术对几何变换攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的EaaS模型所有权保护方法（如数字水印）存在两个关键缺陷：生成的触发器可通过语义分析被检测，且易受旋转/缩放/平移（RST）等几何变换影响。这导致现有方法在实际部署中存在安全风险。

Method: 1. 将待验证模型输出的嵌入向量建模为点云；2. 通过计算点云的拓扑结构（如空间分布特征）建立模型指纹；3. 使用空间对齐技术和相似度度量对比怀疑模型与原始模型的指纹；4. 无需修改训练样本或模型参数，直接分析黑盒API的输出。

Result: 在视觉和文本嵌入任务上的实验表明：1. 本方法对RST攻击具有鲁棒性（平均验证准确率超过95%）；2. 优于传统水印方法（检测准确率提升约30%）；3. 适用于不同领域黑盒API场景。

Conclusion: 该框架通过揭示EaaS模型嵌入空间的固有拓扑性质，为黑盒场景下的模型所有权验证提供了新思路，解决了传统水印技术的可检测性和脆弱性问题，具有更强的实用性和安全性。

Abstract: Feature embedding has become a cornerstone technology for processing
high-dimensional and complex data, which results in that Embedding as a Service
(EaaS) models have been widely deployed in the cloud. To protect the
intellectual property of EaaS models, existing methods apply digital
watermarking to inject specific backdoor triggers into EaaS models by modifying
training samples or network parameters. However, these methods inevitably
produce detectable patterns through semantic analysis and exhibit
susceptibility to geometric transformations including rotation, scaling, and
translation (RST). To address this problem, we propose a fingerprinting
framework for EaaS models, rather than merely refining existing watermarking
techniques. Different from watermarking techniques, the proposed method
establishes EaaS model ownership through geometric analysis of embedding
space's topological structure, rather than relying on the modified training
samples or triggers. The key innovation lies in modeling the victim and
suspicious embeddings as point clouds, allowing us to perform robust spatial
alignment and similarity measurement, which inherently resists RST attacks.
Experimental results evaluated on visual and textual embedding tasks verify the
superiority and applicability. This research reveals inherent characteristics
of EaaS models and provides a promising solution for ownership verification of
EaaS models under the black-box scenario.

</details>


### [20] [DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge](https://arxiv.org/abs/2510.16716)
*Asmita Mohanty,Gezheng Kang,Lei Gao,Murali Annavaram*

Main category: cs.CR

TL;DR: DistilLock框架在TEE环境下安全地进行边缘设备上的知识蒸馏，实现LLM个性化，同时保护数据隐私和模型知识产权。


<details>
  <summary>Details</summary>
Motivation: 解决云端微调LLM的数据隐私和边缘微调LLM的模型知识产权泄露问题。

Method: 利用TEE作为安全执行环境运行基础模型作为教师模型，结合模型混淆机制将混淆权重卸载至非可信加速器进行高效蒸馏。

Result: DistilLock能抵御非法知识蒸馏和窃取攻击，且计算高效。

Conclusion: DistilLock为边缘设备上的LLM微调提供了安全实用的隐私保护方案。

Abstract: Large Language Models (LLMs) have demonstrated strong performance across
diverse tasks, but fine-tuning them typically relies on cloud-based,
centralized infrastructures. This requires data owners to upload potentially
sensitive data to external servers, raising serious privacy concerns. An
alternative approach is to fine-tune LLMs directly on edge devices using local
data; however, this introduces a new challenge: the model owner must transfer
proprietary models to the edge, which risks intellectual property (IP) leakage.
To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning
framework that enables privacy-preserving knowledge distillation on the edge.
In DistilLock, a proprietary foundation model is executed within a trusted
execution environment (TEE) enclave on the data owner's device, acting as a
secure black-box teacher. This setup preserves both data privacy and model IP
by preventing direct access to model internals. Furthermore, DistilLock employs
a model obfuscation mechanism to offload obfuscated weights to untrusted
accelerators for efficient knowledge distillation without compromising
security. We demonstrate that DistilLock prevents unauthorized knowledge
distillation processes and model-stealing attacks while maintaining high
computational efficiency, but offering a secure and practical solution for
edge-based LLM personalization.

</details>


### [21] [Cryptanalysis of a Privacy-Preserving Ride-Hailing Service from NSS 2022](https://arxiv.org/abs/2510.16744)
*Srinivas Vivek*

Main category: cs.CR

TL;DR: 对某个隐私保护的叫车服务（PP-RHS）协议的攻击方法


<details>
  <summary>Details</summary>
Motivation: Xie等人在NSS 2022上提出了一个PP-RHS协议，但该协议存在安全缺陷，需要揭示其脆弱性以促进更安全的协议设计。

Method: 提出了一种被动攻击方法，允许服务提供商（SP）完全恢复乘客和司机在每次叫车请求中的位置信息。

Result: 攻击能够高效地完全恢复乘客和司机的位置，且攻击效率不受安全参数影响。

Conclusion: 该工作暴露了现有PP-RHS协议的安全漏洞，表明需要更强的隐私保护机制。

Abstract: Ride-Hailing Services (RHS) match a ride request initiated by a rider with a
suitable driver responding to the ride request. A Privacy-Preserving RHS
(PP-RHS) aims to facilitate ride matching while ensuring the privacy of riders'
and drivers' location data w.r.t. the Service Provider (SP). At NSS 2022, Xie
et al. proposed a PP-RHS. In this work, we demonstrate a passive attack on
their PP-RHS protocol. Our attack allows the SP to completely recover the
locations of the rider as well as that of the responding drivers in every ride
request. Further, our attack is very efficient as it is independent of the
security parameter.

</details>


### [22] [Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy](https://arxiv.org/abs/2510.16830)
*Hasan Akgul,Daniel Borg,Arta Berisha,Amina Rahimova,Andrej Novak,Mila Petrov*

Main category: cs.CR

TL;DR: 提出了一个可验证的微调协议，通过零知识证明确保模型是从公共初始化开始，按照声明的训练程序和可审计的数据集承诺进行训练得到的。该方法结合了数据承诺、可验证采样器、参数高效更新电路、递归证明聚合以及来源绑定等技术，实现了实用的证明性能和严格的政策执行。


<details>
  <summary>Details</summary>
Motivation: 当前参数高效微调模型在发布时，对使用的数据和计算过程的透明度不足，缺乏可验证的证据，这影响了在需要严格合规场景中的应用。为填补信任差距，需要一种能够提供可靠证明的方法。

Method: 1. 数据承诺：将数据源、预处理、许可证和每轮配额绑定到清单；2. 可验证采样器：支持公开可重放或私密的批量选择；3. 参数高效更新电路：实施AdamW优化器语义和证明友好的近似计算；4. 递归证明聚合：将每步证明聚合成每轮及端到端证明；5. 来源绑定：通过可信执行证明代码和常量不变性。

Result: 在英语和双语指令混合数据集上，在严格的计算预算下保持模型实用性，证明性能达到实用水平。政策配额违规率为零，私密采样无数据索引泄露。联邦场景下能与概率审计和带宽限制兼容。

Conclusion: 该工作证明端到端的可验证参数高效微调当前具有可行性，解决了监管和去中心化部署场景中的关键信任问题。

Abstract: Large language models are often adapted through parameter efficient fine
tuning, but current release practices provide weak assurances about what data
were used and how updates were computed. We present Verifiable Fine Tuning, a
protocol and system that produces succinct zero knowledge proofs that a
released model was obtained from a public initialization under a declared
training program and an auditable dataset commitment. The approach combines
five elements. First, commitments that bind data sources, preprocessing,
licenses, and per epoch quota counters to a manifest. Second, a verifiable
sampler that supports public replayable and private index hiding batch
selection. Third, update circuits restricted to parameter efficient fine tuning
that enforce AdamW style optimizer semantics and proof friendly approximations
with explicit error budgets. Fourth, recursive aggregation that folds per step
proofs into per epoch and end to end certificates with millisecond
verification. Fifth, provenance binding and optional trusted execution property
cards that attest code identity and constants. On English and bilingual
instruction mixtures, the method maintains utility within tight budgets while
achieving practical proof performance. Policy quotas are enforced with zero
violations, and private sampling windows show no measurable index leakage.
Federated experiments demonstrate that the system composes with probabilistic
audits and bandwidth constraints. These results indicate that end to end
verifiable fine tuning is feasible today for real parameter efficient
pipelines, closing a critical trust gap for regulated and decentralized
deployments.

</details>


### [23] [Addendum: Systematic Evaluation of Randomized Cache Designs against Cache Occupancy](https://arxiv.org/abs/2510.16871)
*Anirban Chakraborty,Nimish Mishra,Sayandeep Saha,Sarani Bhattacharya,Debdeep Mukhopadhyay*

Main category: cs.CR

TL;DR: 该摘要是一篇已发表论文的补充说明，针对随机化缓存在性能和安全性方面的设计考虑进行了系统分析，并回应了另一篇工作[2]的观察。主要结论指出，设计一个效率与现代组相联LLC相当且能抵抗基于竞争和占位攻击的随机化缓存仍然是一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 为了回应[2]中的观察，即L1d缓存大小影响攻击成功率，以及一个带随机初始种子的MIRAGE补丁版本能够防止AES密钥泄漏，作者撰写了此增补说明来讨论这些问题。

Method: 该增补说明基于之前发表的系统分析工作，特别是对随机化缓存在性能（统一的基准测试策略）和安全性（三种威胁假设：隐蔽信道、进程指纹侧信道、AES密钥恢复）方面的设计考虑。

Result: 作者在增补中确认了[2]的观察，即L1d缓存大小在攻击成功中起作用，而采用随机初始种子的MIRAGE补丁可防止AES密钥泄漏。

Conclusion: 该增补进一步支持了原文的核心观点：设计既高效又能抵抗多种攻击（包括基于竞争和基于占位的攻击）的随机化缓存仍是一个开放性问题。

Abstract: In the main text published at USENIX Security 2025, we presented a systematic
analysis of the role of cache occupancy in the design considerations for
randomized caches (from the perspectives of performance and security). On the
performance front, we presented a uniform benchmarking strategy that allows for
a fair comparison among different randomized cache designs. Likewise, from the
security perspective, we presented three threat assumptions: (1) covert
channels; (2) process fingerprinting side-channel; and (3) AES key recovery.
The main takeaway of our work is an open problem of designing a randomized
cache of comparable efficiency with modern set-associative LLCs, while still
resisting both contention-based and occupancy-based attacks. This note is meant
as an addendum to the main text in light of the observations made in [2]. To
summarize, the authors in [2] argue that (1) L1d cache size plays a role in
adversarial success, and that (2) a patched version of MIRAGE with randomized
initial seeding of global eviction map prevents leakage of AES key. We discuss
the same in this addendum.

</details>


### [24] [On the Credibility of Deniable Communication in Court](https://arxiv.org/abs/2510.16873)
*Jacob Leiken,Sunoo Park*

Main category: cs.CR

TL;DR: 该论文分析了密码学中的可否认性概念与实际法庭应用之间的差距，并提出了一个更广泛的‘可信度’模型，该模型考虑了社会技术背景、威胁模型和系统保留策略，以更好地设计适用于现实威胁的安全通信系统。


<details>
  <summary>Details</summary>
Motivation: 作者观察到，在计算机科学文献中，密码可否认系统常被与法庭上‘否认’证据的概念联系在一起，但实际上法庭的取证过程早已考虑证据可伪造性，并依赖非密码学因素寻求真相。因此，作者试图弥合技术可否认性概念与实际应用之间的差距。

Method: 论文通过分析法庭证据处理的现实机制，批判现有密码学模型对可否认性的定义局限，进而提出一个包含三个维度的‘可信度’模型：(1) 伪造品需达到的‘可信’质量阈值（随社会技术背景和威胁模型变化），(2) 制造达到该阈值伪造品的难易程度（同样依赖背景），(3) 系统默认的保留策略与设置。

Result: 作者构建的‘可信度’模型不仅更贴合现实威胁场景，还能指导安全通信系统设计——其中(2)(3)维度可直接纳入技术设计，从而补充现有密码模型的不足。

Conclusion: 论文结论强调：可否认性并未颠覆法庭取证的根本范式；其提出的可信度框架有助于在特定法律和社会技术背景下，更细致地讨论密码学保障的强度与局限，推动系统设计应对纯技术定义无法捕捉的威胁。

Abstract: Over time, cryptographically deniable systems have come to be associated in
computer-science literature with the idea of "denying" evidence in court -
specifically, with the ability to convincingly forge evidence in courtroom
scenarios and an inability to authenticate evidence in such contexts.
Evidentiary processes in courts, however, have been developed over centuries to
account for the reality that evidence has always been forgeable, and relies on
factors outside of cryptographic models to seek the truth "as well as possible"
while acknowledging that all evidence is imperfect. We argue that deniability
does not and need not change this paradigm.
  Our analysis highlights a gap between technical deniability notions and their
application to the real world. There will always be factors outside a
cryptographic model that influence perceptions of a message's authenticity, in
realistic situations. We propose the broader concept of credibility to capture
these factors. The credibility of a system is determined by (1) a threshold of
quality that a forgery must pass to be "believable" as an original
communication, which varies based on sociotechnical context and threat model,
(2) the ease of creating a forgery that passes this threshold, which is also
context- and threat-model-dependent, and (3) default system retention policy
and retention settings. All three aspects are important for designing secure
communication systems for real-world threat models, and some aspects of (2) and
(3) may be incorporated directly into technical system design. We hope that our
model of credibility will facilitate system design and deployment that
addresses threats that are not and cannot be captured by purely technical
definitions and existing cryptographic models, and support more nuanced
discourse on the strengths and limitations of cryptographic guarantees within
specific legal and sociotechnical contexts.

</details>


### [25] [UNDREAM: Bridging Differentiable Rendering and Photorealistic Simulation for End-to-end Adversarial Attacks](https://arxiv.org/abs/2510.16923)
*Mansi Phute,Matthew Hull,Haoran Wang,Alec Helbling,ShengYun Peng,Willian Lunardi,Martin Andreoni,Wenke Lee,Polo Chau*

Main category: cs.CR

TL;DR: 提出了UNDREAM框架，用于在物理仿真环境中进行端到端的对抗扰动优化。


<details>
  <summary>Details</summary>
Motivation: 现有安全关键应用（如自动驾驶）中的深度学习模型通过在仿真环境中测试来评估对对抗攻击的鲁棒性。但这些仿真环境不可微分，导致不能充分利用环境因素来设计攻击。

Method: 提出UNDREAM框架，桥接真实感模拟器和可微分渲染器，支持端到端优化3D物体的对抗扰动。提供对天气、光照、背景、相机角度、轨迹以及真实人类和物体运动的控制，创造多样化的场景。

Result: 展示了UNDREAM能够快速生成多种在物理环境中可行的对抗物体，并支持在不同可配置环境中探索。

Conclusion: 结合真实感模拟和可微分优化，为物理对抗攻击研究开辟了新途径。

Abstract: Deep learning models deployed in safety critical applications like autonomous
driving use simulations to test their robustness against adversarial attacks in
realistic conditions. However, these simulations are non-differentiable,
forcing researchers to create attacks that do not integrate simulation
environmental factors, reducing attack success. To address this limitation, we
introduce UNDREAM, the first software framework that bridges the gap between
photorealistic simulators and differentiable renderers to enable end-to-end
optimization of adversarial perturbations on any 3D objects. UNDREAM enables
manipulation of the environment by offering complete control over weather,
lighting, backgrounds, camera angles, trajectories, and realistic human and
object movements, thereby allowing the creation of diverse scenes. We showcase
a wide array of distinct physically plausible adversarial objects that UNDREAM
enables researchers to swiftly explore in different configurable environments.
This combination of photorealistic simulation and differentiable optimization
opens new avenues for advancing research of physical adversarial attacks.

</details>


### [26] [Efficient derandomization of differentially private counting queries](https://arxiv.org/abs/2510.16959)
*Surendra Ghentiyala*

Main category: cs.CR

TL;DR: 本文针对差分隐私中计数查询的随机性问题，提出了一种高效的机制，能够减少所需的随机性。先前的研究显示，对 $d$ 个计数查询只需 $O(\log d)$ 比特的随机性，但他们提出的方案效率低且有复杂性。本文机制引入随机偏移后，通过不添加噪声处理某些数据以降低成本。


<details>
  <summary>Details</summary>
Motivation: 2020 年人口普查差分隐私中需约 90TB 随机数据生成成本过高。为解决实践中高随机性耗费问题，需开发高效且随机性需求较低的计数查询差分隐私机制。

Method: 提出多项式时间机制：通过对查询结果进行随机移位，使多查询结果无需额外噪声添加。该机制摆脱复杂组合理论依赖，通过动态选择性噪声，使系统在保持精度同时降低随机性开销。

Result: 机制在近同等准确性与随机性耗损上匹配既有方案，仅需 $O(\log d)$ 比特随机性，但通过高效算法实现了对 $d$ 个查询并行处理。

Conclusion: 本文提供高效机制降低计数查询差分隐私中随机性需求，通过简单随机移位解释“批处理降低随机性开销”的内在原理，并简化了理解过程。

Abstract: Differential privacy for the 2020 census required an estimated 90 terabytes
of randomness [GL20], an amount which may be prohibitively expensive or
entirely infeasible to generate. Motivated by these practical concerns, [CSV25]
initiated the study of the randomness complexity of differential privacy, and
in particular, the randomness complexity of $d$ counting queries. This is the
task of outputting the number of entries in a dataset that satisfy predicates
$\mathcal{P}_1, \dots, \mathcal{P}_d$ respectively. They showed the rather
surprising fact that though any reasonably accurate,
$\varepsilon$-differentially private mechanism for one counting query requires
$1-O(\varepsilon)$ bits of randomness in expectation, there exists a fairly
accurate mechanism for $d$ counting queries which requires only $O(\log d)$
bits of randomness in expectation.
  The mechanism of [CSV25] is inefficient (not polynomial time) and relies on a
combinatorial object known as rounding schemes. Here, we give a polynomial time
mechanism which achieves nearly the same randomness complexity versus accuracy
tradeoff as that of [CSV25]. Our construction is based on the following simple
observation: after a randomized shift of the answer to each counting query, the
answer to many counting queries remains the same regardless of whether we add
noise to that coordinate or not. This allows us to forgo the step of adding
noise to the result of many counting queries. Our mechanism does not make use
of rounding schemes. Therefore, it provides a different -- and, in our opinion,
clearer -- insight into the origins of the randomness savings that can be
obtained by batching $d$ counting queries. Therefore, it provides a different
-- and, in our opinion, clearer -- insight into the origins of the randomness
savings that can be obtained by batching $d$ counting queries.

</details>


### [27] [Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs](https://arxiv.org/abs/2510.17000)
*Masahiro Kaneko,Timothy Baldwin*

Main category: cs.CR

TL;DR: 提出了一种信息论框架，用于量化大语言模型(LLM)在对抗性攻击中泄露的信息量，揭示了不同输出信号（答案令牌、逻辑值、完整思维链）对攻击成本的影响。理论证明攻击成本随信息泄露率呈线性反比下降。实验在七个LLM上验证了系统提示泄露、越狱攻击和未学习恢复场景，为透明性与安全性权衡提供了理论基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏量化对抗性攻击中LLM信息泄露的机制，导致审计缺乏依据，防御者无法权衡透明性与风险。恶意用户通过精心设计的指令推断未知目标属性（如触发有害响应的标志、未学习后信息可恢复程度），LLM的响应信号（答案令牌、思维链、逻辑值）可能泄露信息，但泄露程度缺乏系统性评估。

Method: 1. 构建信息论框架：以观测信号Z（响应文本或逻辑值）与目标属性T的互信息I(Z;T)作为每个查询泄露的信息量（比特/查询）；
2. 推导攻击成本下界：实现误差ε所需最小查询数为log(1/ε)/I(Z;T)，与泄露率成反比，与精度对数相关；
3. 实证验证：在7个LLM上测试三类场景（系统提示泄露、越狱攻击、未学习恢复），对比三种信号粒度（仅答案令牌、答案+逻辑值、完整思维链）的攻击效率。

Result: 1. 理论：即使小幅增加信息泄露（如输出思维链），攻击成本可从二次方降至对数级；
2. 实验：仅暴露答案令牌需约1000次查询；增加逻辑值降至约100次；揭示完整思维链仅需数十次；
3. 泄露效率排序：思维链 > 逻辑值 > 答案令牌（对应I(Z;T)依次递减）。

Conclusion: 首次为LLM部署中的透明性与安全性权衡提供了基础性度量标准：互信息I(Z;T)是攻击成本的核心因子，防御设计需严格控制高泄露信号（如思维链）的输出。框架可指导审计评估现有攻击方法接近理论极限的程度。

Abstract: Adversarial attacks by malicious users that threaten the safety of large
language models (LLMs) can be viewed as attempts to infer a target property $T$
that is unknown when an instruction is issued, and becomes knowable only after
the model's reply is observed. Examples of target properties $T$ include the
binary flag that triggers an LLM's harmful response or rejection, and the
degree to which information deleted by unlearning can be restored, both
elicited via adversarial instructions. The LLM reveals an \emph{observable
signal} $Z$ that potentially leaks hints for attacking through a response
containing answer tokens, thinking process tokens, or logits. Yet the scale of
information leaked remains anecdotal, leaving auditors without principled
guidance and defenders blind to the transparency--risk trade-off. We fill this
gap with an information-theoretic framework that computes how much information
can be safely disclosed, and enables auditors to gauge how close their methods
come to the fundamental limit. Treating the mutual information $I(Z;T)$ between
the observation $Z$ and the target property $T$ as the leaked bits per query,
we show that achieving error $\varepsilon$ requires at least
$\log(1/\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak
rate and only logarithmically with the desired accuracy. Thus, even a modest
increase in disclosure collapses the attack cost from quadratic to logarithmic
in terms of the desired accuracy. Experiments on seven LLMs across
system-prompt leakage, jailbreak, and relearning attacks corroborate the
theory: exposing answer tokens alone requires about a thousand queries; adding
logits cuts this to about a hundred; and revealing the full thinking process
trims it to a few dozen. Our results provide the first principled yardstick for
balancing transparency and security when deploying LLMs.

</details>


### [28] [Watermark Robustness and Radioactivity May Be at Odds in Federated Learning](https://arxiv.org/abs/2510.17033)
*Leixu Huang,Zedian Shao,Teodora Baluta*

Main category: cs.CR

TL;DR: 本文提出将放射性水印技术应用于联邦学习中，以追踪LLM生成数据的来源。水印在微调后仍可检测，但当服务器使用鲁棒聚合过滤异常更新时，水印会被有效移除，揭示了放射性水印在鲁棒性与实用性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着分布式数据中LLM生成内容的增多，追踪数据来源以实现透明问责的需求日益增长。本文旨在通过水印技术解决联邦学习中数据来源追踪的问题，尤其关注恶意服务器删除水印的对抗场景。

Method: 在联邦学习框架下部署放射性水印：部分客户端用带水印数据计算本地更新，服务器聚合所有更新以形成全局模型。水印具有放射性——微调后仍可检测。但设计主动对抗型服务器，通过鲁棒聚合过滤由水印数据引发的异常更新以消除水印信号。

Result: 水印检测置信度极高（p值最低达1e-24），即使仅6.6%数据带水印。然而，采用鲁棒聚合的服务器可有效消除所有被测试水印，同时保持模型性能。

Conclusion: 研究揭示了放射性水印在联邦学习中存在根本性权衡：强放射性保证了高检测率，但会引发模型更新异常，使其易被鲁棒聚合过滤。需探索放射性、抗过滤鲁棒性与模型实用性之间的平衡方案。

Abstract: Federated learning (FL) enables fine-tuning large language models (LLMs)
across distributed data sources. As these sources increasingly include
LLM-generated text, provenance tracking becomes essential for accountability
and transparency. We adapt LLM watermarking for data provenance in FL where a
subset of clients compute local updates on watermarked data, and the server
averages all updates into the global LLM. In this setup, watermarks are
radioactive: the watermark signal remains detectable after fine-tuning with
high confidence. The $p$-value can reach $10^{-24}$ even when as little as
$6.6\%$ of data is watermarked. However, the server can act as an active
adversary that wants to preserve model utility while evading provenance
tracking. Our observation is that updates induced by watermarked synthetic data
appear as outliers relative to non-watermark updates. Our adversary thus
applies strong robust aggregation that can filter these outliers, together with
the watermark signal. All evaluated radioactive watermarks are not robust
against such an active filtering server. Our work suggests fundamental
trade-offs between radioactivity, robustness, and utility.

</details>


### [29] [Quantum Key Distribution for Virtual Power Plant Communication: A Lightweight Key-Aware Scheduler with Provable Stability](https://arxiv.org/abs/2510.17087)
*Ziqing Zhu*

Main category: cs.CR

TL;DR: 本文提出了一种针对虚拟电厂（VPP）通信安全的新型密钥管理和调度框架，通过将量子密钥作为一级调度资源，结合配额、仲裁和降级机制，显著提升关键信息传输的可靠性和时延特性。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟电厂运行频率提升至秒级，传统PKI和密钥轮换方案难以满足跨域高频通信需求，且存在量子威胁。量子密钥分发（QKD）虽提供理论安全的新鲜密钥，但其密钥生成率随机且有限，无法匹配VPP突发流量。

Method: 设计包含四个核心组件的框架：1)基于预测的长期配额与短期令牌分配；2)密钥感知的赤字轮询仲裁；3)抢占式应急密钥储备；4)通过加密模式切换和非关键流量降采样的优雅降级。结合漂移加惩罚理论证明系统稳定性。

Result: 在IEEE 33/123节点VPP测试床验证：相比FIFO/固定优先级/静态配额基线，该方案在正常/降级/中断工况下均降低关键消息尾部时延与超时率，提升每比特密钥效用，增强密钥稀缺和工况切换时的功率追踪可靠性。

Conclusion: 该框架将量子密钥作为可调度资源管理，首次为密钥受限系统提供可量化的稳定性保证，解决了安全通信资源与电力物理过程实时需求的对齐问题。

Abstract: Virtual power plants (VPPs) are becoming a cornerstone of future grids,
aggregating distributed PV, wind, storage, and flexible loads for market
participation and real-time balancing. As operations move to minute-- and
second--level feedback, communication security shifts from a compliance item to
an operational constraint: latency, reliability, and confidentiality jointly
determine whether dispatch, protection, and settlement signals arrive on time.
Conventional PKI and key-rotation schemes struggle with cross-domain,
high-frequency messaging and face long-term quantum threats. Quantum key
distribution (QKD) offers information-theoretic key freshness, but its key
yield is scarce and stochastic, often misaligned with bursty VPP traffic. This
paper proposes a key-aware priority and quota framework that treats quantum
keys as first-class scheduling resources. The design combines (i)
forecast-driven long-term quotas and short-term tokens, (ii) key-aware
deficit-round-robin arbitration, (iii) a preemptive emergency key reserve, and
(iv) graceful degradation via encryption-mode switching and controlled
down-sampling for non-critical traffic. A drift-plus-penalty analysis
establishes strong stability under average supply--demand balance with
quantifiable bounds on backlog and tail latency, providing interpretable
operating guarantees. We build a reproducible testbed on IEEE 33- and 123-bus
VPP systems and evaluate normal, degraded, and outage regimes with
industry-consistent message classes and TTLs. Against FIFO, fixed-priority, and
static-quota baselines, the proposed scheme consistently reduces tail delay and
passive timeouts for critical messages, improves per-bit key utility, and
enhances power-tracking reliability during key scarcity and regime switches.

</details>


### [30] [Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models](https://arxiv.org/abs/2510.17098)
*Elias Hossain,Swayamjit Saha,Somshubhra Roy,Ravi Prasad*

Main category: cs.CR

TL;DR: 本文引入了恶意令牌注入（MTI）框架，通过扰动Transformer模型推理过程中的键值缓存，揭示了缓存完整性作为当前LLM部署中被忽视的漏洞。理论分析和实验证明MTI能显著改变模型输出，影响任务性能和复杂推理流程。


<details>
  <summary>Details</summary>
Motivation: 尽管提示和参数已受保护，但Transformer语言模型在推理过程中的键值缓存仍构成潜在攻击面。本文旨在探索这种被忽视的脆弱性，并提出一种可量化的威胁模型。

Method: MTI框架在特定层和时间步通过高斯噪声、归零和正交旋转扰动缓存中的键向量。理论分析量化了扰动在注意力机制中的传播，关联了logit偏差与Frobenius范数及softmax的Lipschitz动态。

Result: 实验证明：MTI能显著改变GPT-2和LLaMA-2/7B的下一个令牌分布和任务表现，并破坏检索增强与智能体推理流程。例如缓存扰动导致模型输出偏差可复现且可理论验证。

Conclusion: 将缓存完整性确立为LLM安全的关键脆弱点，MTI为未来鲁棒性研究提供了理论基础。论文主张缓存污染应成为安全攻防的标准威胁模型之一。

Abstract: Even when prompts and parameters are secured, transformer language models
remain vulnerable because their key-value (KV) cache during inference
constitutes an overlooked attack surface. This paper introduces Malicious Token
Injection (MTI), a modular framework that systematically perturbs cached key
vectors at selected layers and timesteps through controlled magnitude and
frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A
theoretical analysis quantifies how these perturbations propagate through
attention, linking logit deviations to the Frobenius norm of corruption and
softmax Lipschitz dynamics. Empirical results show that MTI significantly
alters next-token distributions and downstream task performance across GPT-2
and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic
reasoning pipelines. These findings identify cache integrity as a critical yet
underexplored vulnerability in current LLM deployments, positioning cache
corruption as a reproducible and theoretically grounded threat model for future
robustness and security research.

</details>


### [31] [QRïS: A Preemptive Novel Method for Quishing Detection Through Structural Features of QR](https://arxiv.org/abs/2510.17175)
*Muhammad Wahid Akram,Keshav Sood,Muneeb Ul Hassan*

Main category: cs.CR

TL;DR: 本文提出了一种名为QR"iS的新方法，通过全面分析二维码的结构特征来识别钓鱼二维码，以提高分类的透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的二维码防钓鱼攻击方法多为黑盒技术，缺乏透明度和解释性，这限制了方法的可靠性和实际应用。

Method: 首先创建了包含40万个样本的数据集，然后开发了一种简单算法从二维码中提取24个结构特征，并用机器学习模型进行训练。

Result: 在实验中，模型准确率最高达到83.18%。同时开发了移动应用进行实际验证。

Conclusion: QR"iS方法具有透明、可复现、可扩展和易理解的优点，实际应用验证了其有效性。

Abstract: Globally, individuals and organizations employ Quick Response (QR) codes for
swift and convenient communication. Leveraging this, cybercriminals embed
falsify and misleading information in QR codes to launch various phishing
attacks which termed as Quishing. Many former studies have introduced defensive
approaches to preclude Quishing such as by classifying the embedded content of
QR codes and then label the QR codes accordingly, whereas other studies
classify them using visual features (i.e., deep features, histogram density
analysis features). However, these approaches mainly rely on black-box
techniques which do not clearly provide interpretability and transparency to
fully comprehend and reproduce the intrinsic decision process; therefore,
having certain obvious limitations includes the approaches' trust,
accountability, issues in bias detection, and many more. We proposed QR\"iS,
the pioneer method to classify QR codes through the comprehensive structural
analysis of a QR code which helps to identify phishing QR codes beforehand. Our
classification method is clearly transparent which makes it reproducible,
scalable, and easy to comprehend. First, we generated QR codes dataset (i.e.
400,000 samples) using recently published URLs datasets [1], [2]. Then, unlike
black-box models, we developed a simple algorithm to extract 24 structural
features from layout patterns present in QR codes. Later, we train the machine
learning models on the harvested features and obtained accuracy of up to
83.18%. To further evaluate the effectiveness of our approach, we perform the
comparative analysis of proposed method with relevant contemporary studies.
Lastly, for real-world deployment and validation, we developed a mobile app
which assures the feasibility of the proposed solution in real-world scenarios
which eventually strengthen the applicability of the study.

</details>


### [32] [Exploiting the Potential of Linearity in Automatic Differentiation and Computational Cryptography](https://arxiv.org/abs/2510.17220)
*Giulia Giusti*

Main category: cs.CR

TL;DR: 本文探讨了线性逻辑在编程范式中的应用，涵盖了自动微分（AD）和计算密码学（CryptoBLL）两部分：一方面将线性逻辑用于建模实数上的线性函数和转置操作；另一方面利用线性逻辑表达密码学中对抗者的复杂性约束。


<details>
  <summary>Details</summary>
Motivation: 线性概念在数学和计算机科学中具有核心但不同的作用。数学中支撑函数和向量空间；计算机科学中体现为资源敏感计算（如线性逻辑LL）。通过连接这两种视角，为分析验证复杂系统提供严谨实用的方法。

Method: 1. ADLL：将JAX的线性类型系统与线性逻辑连接，桥接理论与实际；2. CryptoBLL：提出计算密码学中自动分析协议的框架，处理表达性与简洁性之间的权衡。

Result: 为AD建立理论（证明论）与实践（JAX库）的连接；为密码学提供可平衡协议表达性（捕获归约）与抽象性（简化概率复杂度）的解决方案。

Conclusion: 线性逻辑是建模线性编程范式的有效工具，在不同领域（如AD和密码学）中可统一理论表达与实践需求。

Abstract: The concept of linearity plays a central role in both mathematics and
computer science, with distinct yet complementary meanings. In mathematics,
linearity underpins functions and vector spaces, forming the foundation of
linear algebra and functional analysis. In computer science, it relates to
resource-sensitive computation. Linear Logic (LL), for instance, models
assumptions that must be used exactly once, providing a natural framework for
tracking computational resources such as time, memory, or data access. This
dual perspective makes linearity essential to programming languages, type
systems, and formal models that express both computational complexity and
composability. Bridging these interpretations enables rigorous yet practical
methodologies for analyzing and verifying complex systems.
  This thesis explores the use of LL to model programming paradigms based on
linearity. It comprises two parts: ADLL and CryptoBLL. The former applies LL to
Automatic Differentiation (AD), modeling linear functions over the reals and
the transposition operation. The latter uses LL to express complexity
constraints on adversaries in computational cryptography.
  In AD, two main approaches use linear type systems: a theoretical one
grounded in proof theory, and a practical one implemented in JAX, a Python
library developed by Google for machine learning research. In contrast,
frameworks like PyTorch and TensorFlow support AD without linear types. ADLL
aims to bridge theory and practice by connecting JAX's type system to LL.
  In modern cryptography, several calculi aim to model cryptographic proofs
within the computational paradigm. These efforts face a trade-off between
expressiveness, to capture reductions, and simplicity, to abstract probability
and complexity. CryptoBLL addresses this tension by proposing a framework for
the automatic analysis of protocols in computational cryptography.

</details>


### [33] [Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks](https://arxiv.org/abs/2510.17277)
*Xinkai Wang,Beibei Li,Zerui Shao,Ao Liu,Shouling Ji*

Main category: cs.CR

TL;DR: 该论文研究了多模态大语言模型（MLLMs）的安全漏洞，特别是文本视觉模态下的越狱问题，并提出了一个基于强化学习的黑盒越狱方法——PolyJailbreak。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型容易受到越狱攻击，导致安全约束失效并产生不道德的响应。本研究旨在探索视觉对齐在不同模态间引入的安全约束不均匀现象（即多模态安全不对称性），并开发有效方法来利用这一漏洞。

Method: 首先分析模型的注意力机制和潜在表示空间，评估视觉输入如何重塑跨模态信息流并降低模型区分有害与无害输入的能力。然后将发现的漏洞转化为通用的原子策略基元，构成一个结构化转换库，指导生成越狱输入。最后采用多智能体优化过程，根据目标模型自动调整输入。

Result: PolyJailbreak 在多种开源和闭源 MLLM 上进行了全面评估，表现优于现有基线方法。

Conclusion: 视觉对齐会导致 MLLMs 存在多模态安全不对称性漏洞，而 PolyJailbreak 能有效利用该漏洞实现越狱攻击，这凸显了 MLLMs 安全机制中需要改进的方向。

Abstract: Multimodal large language models (MLLMs) have demonstrated significant
utility across diverse real-world applications. But MLLMs remain vulnerable to
jailbreaks, where adversarial inputs can collapse their safety constraints and
trigger unethical responses. In this work, we investigate jailbreaks in the
text-vision multimodal setting and pioneer the observation that visual
alignment imposes uneven safety constraints across modalities in MLLMs, thereby
giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a
black-box jailbreak method grounded in reinforcement learning. Initially, we
probe the model's attention dynamics and latent representation space, assessing
how visual inputs reshape cross-modal information flow and diminish the model's
ability to separate harmful from benign inputs, thereby exposing exploitable
vulnerabilities. On this basis, we systematize them into generalizable and
reusable operational rules that constitute a structured library of Atomic
Strategy Primitives, which translate harmful intents into jailbreak inputs
through step-wise transformations. Guided by the primitives, PolyJailbreak
employs a multi-agent optimization process that automatically adapts inputs
against the target models. We conduct comprehensive evaluations on a variety of
open-source and closed-source MLLMs, demonstrating that PolyJailbreak
outperforms state-of-the-art baselines.

</details>


### [34] [Analysis of Input-Output Mappings in Coinjoin Transactions with Arbitrary Values](https://arxiv.org/abs/2510.17284)
*Jiri Gavenda,Petr Svenda,Stanislav Bobon,Vladimir Sedlacek*

Main category: cs.CR

TL;DR: 本研究分析了比特币混币协议（CoinJoin）的实际隐私效果，发现中心化协调混币服务（Whirlpool, Wasabi 1.x/2.x）的平均匿名集大小在混币后下降10-50%，初期下降明显但一年后可忽略；同时提出一种考虑手续费、实现限制和用户行为的隐私评估方法，证明即使改进分析算法，追溯混币后资金所有权仍非常困难。


<details>
  <summary>Details</summary>
Motivation: 现有混币协议（如CoinJoin）通过协同交易提升比特币的隐私性，但隐私增益的量化评估因影响因素复杂且计算量大而尚未解决。

Method: 1. 改造区块链分析工具BlockSci以支持混币交易分析；2. 设计新型隐私评估方法（考虑手续费、实现限制、用户混币后行为），该方法可并行化；3. 基于模拟和真实Wasabi 2.x混币交易验证方法，并外推至大规模混币场景。

Result: 1. 三大中心化混币服务（Whirlpool, Wasabi 1.x/2.x）在混币后平均匿名集大小减少10-50%，首日降幅最大，一年后几乎无影响；2. 新评估方法显示，尽管用户混币后行为存在弱点，精准追溯资金所有权仍极为困难。

Conclusion: 混币协议能有效增强隐私性，但短期匿名集会下降；即使考虑用户行为缺陷，混币资金仍具备强抗追溯能力，证实了混币技术的实际价值。

Abstract: A coinjoin protocol aims to increase transactional privacy for Bitcoin and
Bitcoin-like blockchains via collaborative transactions, by violating
assumptions behind common analysis heuristics. Estimating the resulting privacy
gain is a crucial yet unsolved problem due to a range of influencing factors
and large computational complexity.
  We adapt the BlockSci on-chain analysis software to coinjoin transactions,
demonstrating a significant (10-50%) average post-mix anonymity set size
decrease for all three major designs with a central coordinator: Whirlpool,
Wasabi 1.x, and Wasabi 2.x. The decrease is highest during the first day and
negligible after one year from a coinjoin creation.
  Moreover, we design a precise, parallelizable privacy estimation method,
which takes into account coinjoin fees, implementation-specific limitations and
users' post-mix behavior. We evaluate our method in detail on a set of emulated
and real-world Wasabi 2.x coinjoins and extrapolate to its largest real-world
coinjoins with hundreds of inputs and outputs. We conclude that despite the
users' undesirable post-mix behavior, correctly attributing the coins to their
owners is still very difficult, even with our improved analysis algorithm.

</details>


### [35] [The Hidden Dangers of Public Serverless Repositories: An Empirical Security Assessment](https://arxiv.org/abs/2510.17311)
*Eduard Marin,Jinwoo Kim,Alessio Pavoni,Mauro Conti,Roberto Di Pietro*

Main category: cs.CR

TL;DR: 本文首次全面分析了公共储存库中无服务器组件的安全状况，研究了2758个无服务器组件和125,936个IaC模板，揭示了系统性漏洞（如过期软件包、敏感参数误用等），并提出了缓解建议。


<details>
  <summary>Details</summary>
Motivation: 公共无服务器储存库的流行使其成为攻击者目标，但其安全状况尚未得到充分研究，给开发者和组织带来潜在风险。

Method: 分析了来自5个主流公共存储库的2758个无服务器组件及3种IaC框架下的125,936份模板。

Result: 发现多项系统性漏洞：过时软件包、敏感参数误用、可被利用的部署配置、易受typo-squatting攻击、以及压缩组件嵌入恶意代码的风险。

Conclusion: 首次揭示公共无服务器组件安全漏洞，提出实用建议以降低风险，填补该领域研究空白。

Abstract: Serverless computing has rapidly emerged as a prominent cloud paradigm,
enabling developers to focus solely on application logic without the burden of
managing servers or underlying infrastructure. Public serverless repositories
have become key to accelerating the development of serverless applications.
However, their growing popularity makes them attractive targets for
adversaries. Despite this, the security posture of these repositories remains
largely unexplored, exposing developers and organizations to potential risks.
In this paper, we present the first comprehensive analysis of the security
landscape of serverless components hosted in public repositories. We analyse
2,758 serverless components from five widely used public repositories popular
among developers and enterprises, and 125,936 Infrastructure as Code (IaC)
templates across three widely used IaC frameworks. Our analysis reveals
systemic vulnerabilities including outdated software packages, misuse of
sensitive parameters, exploitable deployment configurations, susceptibility to
typo-squatting attacks and opportunities to embed malicious behaviour within
compressed serverless components. Finally, we provide practical recommendations
to mitigate these threats.

</details>


### [36] [GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models](https://arxiv.org/abs/2510.17621)
*Vincenzo Carletti,Pasquale Foggia,Carlo Mazzocca,Giuseppe Parrella,Mario Vento*

Main category: cs.CR

TL;DR: 本文提出了GUIDE方法，利用扩散模型作为去噪工具，以改进联邦学习中已有的梯度反演攻击(GIAs)，提升图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过传输梯度更新来实现隐私保护。然而，研究已证明梯度更新仍可能泄露信息，攻击者可以通过梯度反演攻击(GIAs)重建原始数据。尽管现有攻击能重建出近似图像，但重建图像通常带有噪声。GUIDE的提出正是为了解决这种局限性：利用扩散模型对GIAs的重建结果进行去噪优化。

Method: GUIDE的核心方法分为①预训练一个无条件扩散模型；②将其嵌入到任何利用替代数据集的GIAs优化流程中作为扩散先验进行去噪。即先使用已有GIA获得初始重建图像，再通过扩散模型进行迭代优化以减少噪声。该方法与现有GIAs工具解耦设计，具有普适性。

Result: 实验结果证明了GUIDE的有效性：在两种不同FL算法（FedSGD/FedAvg）、不同模型架构（LeNet/ResNet）和数据集（MNIST/CIFAR-10）上，集成GUIDE后的GIA重建图像质量显著提高。在关键指标上与基线方法相比：DreamSim感知相似度提升46%，SSIM提升40%，MSE降低28%。

Conclusion: 1) GUIDE开创性地证明了生成模型在优化隐私攻击效果上的巨大潜力；2) 该方法作为即插即用组件，可显著提升多种梯度反演攻击的现实威胁程度，暴露联邦学习隐私保护脆弱性；3) 研究结果对升级联邦学习防御机制提出新的挑战方向。

Abstract: Federated Learning (FL) enables collaborative training of Machine Learning
(ML) models across multiple clients while preserving their privacy. Rather than
sharing raw data, federated clients transmit locally computed updates to train
the global model. Although this paradigm should provide stronger privacy
guarantees than centralized ML, client updates remain vulnerable to privacy
leakage. Adversaries can exploit them to infer sensitive properties about the
training data or even to reconstruct the original inputs via Gradient Inversion
Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to
reconstruct training data by reversing intermediate updates using
optimizationbased techniques. We observe that these approaches usually
reconstruct noisy approximations of the original inputs, whose quality can be
enhanced with specialized denoising models. This paper presents Gradient Update
Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion
models as denoising tools to improve image reconstruction attacks in FL. GUIDE
can be integrated into any GIAs that exploits surrogate datasets, a widely
adopted assumption in GIAs literature. We comprehensively evaluate our approach
in two attack scenarios that use different FL algorithms, models, and datasets.
Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe-
art GIAs, substantially improving reconstruction quality across multiple
metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity,
as measured by the DreamSim metric.

</details>


### [37] [CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks](https://arxiv.org/abs/2510.17687)
*Xu Zhang,Hao Li,Zhichao Lu*

Main category: cs.CR

TL;DR: 该论文针对多模态大语言模型（MLLMs）面临的新型联合模态攻击（即隐式攻击），提出了自动化红队生成对抗样本的数据集构建方法ImpForge，并基于此开发了意图感知的防御方案CrossGuard，该方案在保持高可用性的同时显著提升了模型对显性和隐性攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单模态恶意输入（显式攻击），但近期发现跨模态组合的隐式攻击威胁更大且缺乏高质量数据集。为填补这一空白并提升MLLMs安全防御的鲁棒性。

Method: 1) ImpForge：利用强化学习的奖励模块自动生成覆盖14个攻击领域的多样化隐式对抗样本；2) CrossGuard：基于生成的数据集构建意图感知的防护机制，通过双模态联合检测实现防御。

Result: 在安全/非安全基准测试、显隐攻击场景及域外设定中，CrossGuard在防御成功率上显著超越现有方案（包括先进MLLM和防护机制），同时保持高模型可用性。

Conclusion: 提出的自动化对抗样本生成方案与意图感知防护体系，为多模态场景下的现实威胁提供了均衡实用的安全增强方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong reasoning and
perception capabilities but are increasingly vulnerable to jailbreak attacks.
While existing work focuses on explicit attacks, where malicious content
resides in a single modality, recent studies reveal implicit attacks, in which
benign text and image inputs jointly express unsafe intent. Such joint-modal
threats are difficult to detect and remain underexplored, largely due to the
scarcity of high-quality implicit data. We propose ImpForge, an automated
red-teaming pipeline that leverages reinforcement learning with tailored reward
modules to generate diverse implicit samples across 14 domains. Building on
this dataset, we further develop CrossGuard, an intent-aware safeguard
providing robust and comprehensive defense against both explicit and implicit
threats. Extensive experiments across safe and unsafe benchmarks, implicit and
explicit attacks, and multiple out-of-domain settings demonstrate that
CrossGuard significantly outperforms existing defenses, including advanced
MLLMs and guardrails, achieving stronger security while maintaining high
utility. This offers a balanced and practical solution for enhancing MLLM
robustness against real-world multimodal threats.

</details>


### [38] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一种用于多模态越狱发现的变分推理框架，通过联合后验分布生成隐蔽的对抗性文本-图像提示，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）存在未充分探索的漏洞，现有攻击方法依赖脆弱模板且覆盖漏洞范围有限。

Method: 提出变分推理框架VERA-V，训练轻量级攻击器建模文本-图像联合后验分布；结合文本嵌入有害提示、扩散对抗图像和注意力分散结构。

Result: 在HarmBench和HADES基准测试中，攻击成功率（ASR）比最优基线提升高达53.75%（GPT-4o）。

Conclusion: VERA-V通过概率框架揭示VLM安全漏洞的分布特性，为多模态安全研究提供新方向。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>
