<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.19558)
*Mohammed Talha Alam,Nada Saadi,Fahad Shamshad,Nils Lukas,Karthik Nandakumar,Fahkri Karray,Samuele Poppi*

Main category: cs.CR

TL;DR: 本文研究了文本到图像扩散模型安全对齐在良性微调下的稳定性问题，发现现有方法常失效。为此提出SPQR基准，用于评估模型在良性微调后的安全性、实用性和鲁棒性，并提供了标准化的评估框架和单一分数排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法在部署后的良性微调（如LoRA个性化、风格/领域适配器调整）下安全性容易失效，需要评估其鲁棒性。

Method: 引入SPQR基准（Safety-Prompt adherence-Quality-Robustness），这是一个单分数评估体系，通过标准化、可复现的框架衡量良性微调后安全对齐模型的安全性、质量和鲁棒性保留情况。

Result: 研究发现在多语言、特定领域和分布外场景中，现有安全对齐在良性微调后频繁失效，SPQR基准可有效识别这些失败场景并提供综合评估。

Conclusion: SPQR基准为文本到图像模型安全对齐技术提供了简洁且全面的评估工具，揭示了当前方法在部署后适应过程中的脆弱性。

Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.

</details>


### [2] [IRSDA: An Agent-Orchestrated Framework for Enterprise Intrusion Response](https://arxiv.org/abs/2511.19644)
*Damodar Panigrahi,Raj Patel,Shaswata Mitra,Sudip Mittal,Shahram Rahimi*

Main category: cs.CR

TL;DR: IRSDA框架提出基于智能体自主进行入侵响应，结合SA-ACS和MAPE-K环实现实时决策，解决传统系统响应慢且规则静态的问题。


<details>
  <summary>Details</summary>
Motivation: 当前入侵检测系统依赖静态规则和人工流程，无法满足现代企业动态威胁下的快速精准响应需求。

Method: 开发基于知识驱动的智能体框架IRSDA，整合上下文信息和AI推理，利用检索机制支持自动决策，并与运维策略保持对齐。

Result: 在微服务应用测试中，系统成功实现自动化攻击围堵、策略合规执行，并为安全分析提供可追溯结果。

Conclusion: IRSDA框架通过模块化智能体设计提升了入侵响应的可解释性、系统状态感知和操作可控性。

Abstract: Modern enterprise systems face escalating cyber threats that are increasingly dynamic, distributed, and multi-stage in nature. Traditional intrusion detection and response systems often rely on static rules and manual workflows, which limit their ability to respond with the speed and precision required in high-stakes environments. To address these challenges, we present the Intrusion Response System Digital Assistant (IRSDA), an agent-based framework designed to deliver autonomous and policy-compliant cyber defense. IRSDA combines Self-Adaptive Autonomic Computing Systems (SA-ACS) with the Knowledge guided Monitor, Analyze, Plan, and Execute (MAPE-K) loop to support real-time, partition-aware decision-making across enterprise infrastructure.
  IRSDA incorporates a knowledge-driven architecture that integrates contextual information with AI-based reasoning to support system-guided intrusion response. The framework leverages retrieval mechanisms and structured representations to inform decision-making while maintaining alignment with operational policies. We assess the system using a representative real-world microservices application, demonstrating its ability to automate containment, enforce compliance, and provide traceable outputs for security analyst interpretation. This work outlines a modular and agent-driven approach to cyber defense that emphasizes explainability, system-state awareness, and operational control in intrusion response.

</details>


### [3] [Accuracy and Efficiency Trade-Offs in LLM-Based Malware Detection and Explanation: A Comparative Study of Parameter Tuning vs. Full Fine-Tuning](https://arxiv.org/abs/2511.19654)
*Stephen C. Gravereaux,Sheikh Rabiul Islam*

Main category: cs.CR

TL;DR: 本研究探讨了低秩适应（LoRA）微调的大语言模型（LLM）在生成可解释的恶意软件分类决策时能否媲美全微调模型。研究建立了评估框架，结果显示全微调性能最优，但中等规模LoRA模型能在减小81%模型体积、缩短80%训练时间的前提下达到部分指标超越。


<details>
  <summary>Details</summary>
Motivation: 由于基于LLM的可信恶意软件检测面临挑战，研究旨在验证轻量级LoRA微调能否在保持解释质量的同时提升资源效率，从而适用于资源受限环境。

Method: 建立评价框架（BLEU、ROUGE及语义相似度指标），比较五种LoRA配置与全微调基线在生成解释时的性能差异。

Result: 全微调综合得分最高（BLEU/ROUGE比LoRA高10%），但中等LoRA模型在两项指标超越全微调，且模型体积减少81%、训练时间缩短80%（仅需15.5%可训练参数）。

Conclusion: LoRA在解释质量与资源效率间取得平衡，其自然语言解释能增强恶意软件检测系统的透明度、分析师信心及部署扩展性。

Abstract: This study examines whether Low-Rank Adaptation (LoRA) fine-tuned Large Language Models (LLMs) can approximate the performance of fully fine-tuned models in generating human-interpretable decisions and explanations for malware classification. Achieving trustworthy malware detection, particularly when LLMs are involved, remains a significant challenge. We developed an evaluation framework using Bilingual Evaluation Understudy (BLEU), Recall-Oriented Understudy for Gisting Evaluation (ROUGE), and Semantic Similarity Metrics to benchmark explanation quality across five LoRA configurations and a fully fine-tuned baseline. Results indicate that full fine-tuning achieves the highest overall scores, with BLEU and ROUGE improvements of up to 10% over LoRA variants. However, mid-range LoRA models deliver competitive performance exceeding full fine-tuning on two metrics while reducing model size by approximately 81% and training time by over 80% on a LoRA model with 15.5% trainable parameters. These findings demonstrate that LoRA offers a practical balance of interpretability and resource efficiency, enabling deployment in resource-constrained environments without sacrificing explanation quality. By providing feature-driven natural language explanations for malware classifications, this approach enhances transparency, analyst confidence, and operational scalability in malware detection systems.

</details>


### [4] [BASICS: Binary Analysis and Stack Integrity Checker System for Buffer Overflow Mitigation](https://arxiv.org/abs/2511.19670)
*Luis Ferreirinha,Iberia Medeiros*

Main category: cs.CR

TL;DR: 本文提出了一种自动化检测和修复二进制C程序中堆栈内存漏洞的新方法，利用模型检测和具体执行技术构建内存状态空间，验证安全属性，并通过蹦床技术进行补丁修复与验证。


<details>
  <summary>Details</summary>
Motivation: C语言在关键基础设施的CPS中广泛使用，但易受缓冲区溢出等漏洞影响。传统漏洞检测技术在应用于二进制代码时存在可扩展性和精确性问题，可能导致程序长期暴露于风险中。

Method: 通过结合模型检查（LTL定义安全属性）与具体执行技术，构建二进制程序的内存状态空间（MemStaCe），识别违反安全属性的反例；采用蹦床技术自动生成补丁，并利用崩溃输入验证修复效果。

Result: 在Juliet C/C++、SARD数据集及实际应用中，BASICS工具实现了87%以上的检测与修复准确率及精确率，性能优于CWE Checker工具。

Conclusion: 该方法为二进制C程序提供了一种高精度的自动化漏洞检测与修复方案，显著提升了关键系统安全性，且实际应用效果良好。

Abstract: Cyber-Physical Systems have played an essential role in our daily lives, providing critical services such as power and water, whose operability, availability, and reliability must be ensured. The C programming language, prevalent in CPS development, is crucial for system control where reliability is critical. However, it is also commonly susceptible to vulnerabilities, particularly buffer overflows. Traditional vulnerability discovery techniques often struggle with scalability and precision when applied directly to the binary code of C programs, which can thereby keep programs vulnerable. This work introduces a novel approach designed to overcome these limitations by leveraging model checking and concolic execution techniques to automatically verify security properties of a program's stack memory in binary code, trampoline techniques to perform automated repair of the issues, and crash-inducing inputs to verify if they were successfully removed. The approach constructs a Memory State Space - MemStaCe- from the binary program's control flow graph and simulations, provided by concolic execution, of C function calls and loop constructs. The security properties, defined in LTL, model the correct behaviour of functions associated with vulnerabilities and allow the approach to identify vulnerabilities in MemStaCe by analysing counterexample traces that are generated when a security property is violated. These vulnerabilities are then addressed with a trampoline-based binary patching method, and the effectiveness of the patches is checked with crash-inducing inputs extracted during concolic execution. We implemented the approach in the BASICS tool for BO mitigation and evaluated using the Juliet C/C++ and SARD datasets and real applications, achieving an accuracy and precision above 87%, both in detection and correction. Also, we compared it with CWE Checker, outperforming it.

</details>


### [5] [CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation](https://arxiv.org/abs/2511.19711)
*Jinyu Liu,Gang Tan,Kiwan Maeng*

Main category: cs.CR

TL;DR: CrypTorch是一个基于MPC的机器学习编译器，通过解耦近似算法与MPC运行时，提供自动选择近似方案以优化性能和准确率，相对于现有方案实现了显著的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有MPC框架中用于处理无法本地执行的ML操作（如Softmax、GELU）的近似方法存在准确率不足或效率低下问题，且难以在框架中修复。

Method: 提出编译器CrypTorch：1）将近似算法与MPC运行时分离；2）提供接口方便添加新近似方法；3）自动选择最优近似方案平衡性能与准确率。基于PyTorch 2编译器扩展实现

Result: 仅自动调优即可在保持准确率前提下提升1.20-1.7倍速度，允许精度损失时可提升1.31-1.8倍，整体框架比主流框架CrypTen快3.22-8.6倍

Conclusion: CrypTorch通过创新解耦设计和自动调优机制解决了MPC框架中近似方法的性能瓶颈，显著提升安全多方计算下ML的效率

Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.

</details>


### [6] [Prompt Fencing: A Cryptographic Approach to Establishing Security Boundaries in Large Language Model Prompts](https://arxiv.org/abs/2511.19727)
*Steven Peh*

Main category: cs.CR

TL;DR: 提出了Prompt Fencing方法，使用加密签名元数据标记提示段，以区分可信指令与不可信内容，防止LLM的提示注入攻击。实验中成功将攻击成功率从86.7%降至0%，且验证开销仅0.224秒。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在部署中面临严重的提示注入攻击安全威胁，需要一种能明确建立提示安全边界的方法。

Method: 通过为提示段添加包含信任评级和内容类型的加密签名元数据，使LLM能区分可信指令与不可信内容。采用模拟围栏感知的提示指令（当前LLM缺乏原生支持），并实现概念验证的围栏生成与验证流程。

Result: 在300个测试案例中，将攻击成功率从86.7%（260/300）降至0%（0/300）。验证流程总开销为0.224秒（100个样本）。

Conclusion: 该方法可增量部署于现有LLM基础设施之上，作为安全层；未来模型若原生支持围栏感知将实现最优安全性。

Abstract: Large Language Models (LLMs) remain vulnerable to prompt injection attacks, representing the most significant security threat in production deployments. We present Prompt Fencing, a novel architectural approach that applies cryptographic authentication and data architecture principles to establish explicit security boundaries within LLM prompts. Our approach decorates prompt segments with cryptographically signed metadata including trust ratings and content types, enabling LLMs to distinguish between trusted instructions and untrusted content. While current LLMs lack native fence awareness, we demonstrate that simulated awareness through prompt instructions achieved complete prevention of injection attacks in our experiments, reducing success rates from 86.7% (260/300 successful attacks) to 0% (0/300 successful attacks) across 300 test cases with two leading LLM providers. We implement a proof-of-concept fence generation and verification pipeline with a total overhead of 0.224 seconds (0.130s for fence generation, 0.094s for validation) across 100 samples. Our approach is platform-agnostic and can be incrementally deployed as a security layer above existing LLM infrastructure, with the expectation that future models will be trained with native fence awareness for optimal security.

</details>


### [7] [Cross-LLM Generalization of Behavioral Backdoor Detection in AI Agent Supply Chains](https://arxiv.org/abs/2511.19874)
*Arun Chowdary Sanna*

Main category: cs.CR

TL;DR: 论文研究了跨LLM行为后门检测的泛化问题，发现在单个模型上训练的检测器在其他模型上效果接近随机猜测（49.2%），而结合模型身份的检测器能达到90.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 企业AI工作流依赖共享工具库和预训练组件，引发供应链漏洞风险。现有后门检测研究仅限于单一LLM架构，跨模型泛化能力未被探索，而实际企业常部署多个AI系统，此缺口影响重大。

Method: 1. 在六个生产级LLM（GPT-5.1等）上采集1,198条执行轨迹；2. 设计36组跨模型实验；3. 对比单一模型检测器与加入模型身份特征的新检测器；4. 分析行为特征（时态/结构）的跨模型稳定性。

Result: 1. 单模型检测器在训练分布内准确率92.7%，跨模型仅49.2%（下降43.4个百分点）；2. 时态特征变异系数>0.8（模型相关性强），结构特征稳定；3. 加入模型身份的新检测器在所有模型上达90.6%准确率。

Conclusion: 现有行为后门检测器严重缺乏跨LLM泛化能力，但通过显式建模模型身份可实现通用检测。研究发布轨迹数据集与检测框架以推动该领域发展。

Abstract: As AI agents become integral to enterprise workflows, their reliance on shared tool libraries and pre-trained components creates significant supply chain vulnerabilities. While previous work has demonstrated behavioral backdoor detection within individual LLM architectures, the critical question of cross-LLM generalization remains unexplored, a gap with serious implications for organizations deploying multiple AI systems. We present the first systematic study of cross-LLM behavioral backdoor detection, evaluating generalization across six production LLMs (GPT-5.1, Claude Sonnet 4.5, Grok 4.1, Llama 4 Maverick, GPT-OSS 120B, and DeepSeek Chat V3.1). Through 1,198 execution traces and 36 cross-model experiments, we quantify a critical finding: single-model detectors achieve 92.7% accuracy within their training distribution but only 49.2% across different LLMs, a 43.4 percentage point generalization gap equivalent to random guessing. Our analysis reveals that this gap stems from model-specific behavioral signatures, particularly in temporal features (coefficient of variation > 0.8), while structural features remain stable across architectures. We show that model-aware detection incorporating model identity as an additional feature achieves 90.6% accuracy universally across all evaluated models. We release our multi-LLM trace dataset and detection framework to enable reproducible research.

</details>


### [8] [Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection](https://arxiv.org/abs/2511.19886)
*Chi Liu,Tianqing Zhu,Wanlei Zhou,Wei Zhao*

Main category: cs.CR

TL;DR: 该论文从频率角度解释深度伪造检测器泛化性和鲁棒性的根本原因，并提出一种两步频率对齐方法，既能用作反取证攻击，也能提升检测器的可靠性。


<details>
  <summary>Details</summary>
Motivation: 通用性和鲁棒性是深度伪造检测器的关键问题，但其根本原因尚未被充分探索。论文旨在从频率视角分析这些问题，揭示检测器的频率偏差是导致泛化与鲁棒性问题的根源。

Method: 提出两步频率对齐方法：第一步使用傅里叶变换将图像转换到频域；第二步通过特定滤波器消除真实图像与伪造图像之间的频率差异。该方法可双向应用——作为反取证的强黑盒攻击，或作为提升检测器可靠性的防御手段。

Result: 在12个检测器、8种伪造模型和5种评估指标的实验环境中验证了方法的有效性：频率对齐攻击显著降低了检测器性能（平均攻击成功率90%+），而用于防御时能提升检测器对未知伪造方法和噪声干扰的鲁棒性（泛化错误率降低15%）。

Conclusion: 检测器的频率偏差是泛化与鲁棒性问题的关键诱因；频率对齐方法具有双向实用性：既是最先进的反取证攻击手段，也是提升检测器可靠性的通用防御方案。

Abstract: As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.

</details>


### [9] [Hey there! You are using WhatsApp: Enumerating Three Billion Accounts for Security and Privacy](https://arxiv.org/abs/2511.20252)
*Gabriel K. Gegenhuber,Philipp É. Frenzel,Maximilian Günther,Johanna Ullrich,Aljosha Judmayer*

Main category: cs.CR

TL;DR: 研究发现WhatsApp存在严重的电话号码枚举漏洞，攻击者每小时可探测上亿号码而不受限制，并发现2021年Facebook数据泄露中近半号码仍在WhatsApp活跃。此外，研究还发现了X25519密钥跨设备/号码重复使用的问题，并最终通过协作修复了漏洞。


<details>
  <summary>Details</summary>
Motivation: WhatsApp作为全球最大即时通讯平台，其用户发现机制存在固有漏洞。尽管已有速率限制防护机制，但实际防护效果未经严格检验。研究旨在重新评估平台当前对手机号枚举攻击的防护能力，验证漏洞是否持续存在及其潜在影响。

Method: 通过自动化脚本模拟正常用户行为（查询手机号注册状态），设计大规模探测实验。技术关键点包括：突破速率限制实现百万级/小时探测、分析服务器响应模式判断号码活跃状态、对Facebook泄露数据集进行验证性探测、进行全网普查级用户数据收集。

Result: 1) 证实漏洞严重性：成功实现每小时探测超1亿号码且无有效拦截；2) 数据泄露关联性：Facebook 2021年泄露数据中47%号码仍活跃于WhatsApp；3) 安全机制缺陷：发现X25519公钥跨设备/号码复用现象；4) 宏观画像：完成用户分布普查，揭示即使消息端到端加密仍可获取群体行为洞见。

Conclusion: WhatsApp现有防护不足以阻止大规模号码枚举攻击，该漏洞可被滥用于生成精确用户画像，且泄露数据长期有效。密钥复用问题暴露实现缺陷或欺诈行为。通过责任披露流程，漏洞已确认修复。研究表明即时通讯架构需在可用性与隐私间寻找新平衡点。

Abstract: WhatsApp, with 3.5 billion active accounts as of early 2025, is the world's largest instant messaging platform. Given its massive user base, WhatsApp plays a critical role in global communication.
  To initiate conversations, users must first discover whether their contacts are registered on the platform. This is achieved by querying WhatsApp's servers with mobile phone numbers extracted from the user's address book (if they allowed access). This architecture inherently enables phone number enumeration, as the service must allow legitimate users to query contact availability. While rate limiting is a standard defense against abuse, we revisit the problem and show that WhatsApp remains highly vulnerable to enumeration at scale. In our study, we were able to probe over a hundred million phone numbers per hour without encountering blocking or effective rate limiting.
  Our findings demonstrate not only the persistence but the severity of this vulnerability. We further show that nearly half of the phone numbers disclosed in the 2021 Facebook data leak are still active on WhatsApp, underlining the enduring risks associated with such exposures. Moreover, we were able to perform a census of WhatsApp users, providing a glimpse on the macroscopic insights a large messaging service is able to generate even though the messages themselves are end-to-end encrypted. Using the gathered data, we also discovered the re-use of certain X25519 keys across different devices and phone numbers, indicating either insecure (custom) implementations, or fraudulent activity.
  In this updated version of the paper, we also provide insights into the collaborative remediation process through which we confirmed that the underlying rate-limiting issue had been resolved.

</details>


### [10] [Can LLMs Make (Personalized) Access Control Decisions?](https://arxiv.org/abs/2511.20284)
*Friederike Groschupp,Daniele Lain,Aritra Dhar,Lara Magdalena Lazier,Srdjan Čapkun*

Main category: cs.CR

TL;DR: 该研究提出使用大型语言模型（LLM）进行动态、上下文感知的访问控制决策，以减轻用户的认知负担。通过用户研究收集了307条自然语言隐私声明和14,682条用户访问控制决策，并将LLM（通用版和个性化版）的决策与用户决策进行对比。结果表明LLM能较好地反映用户偏好（最高86%准确率），但个性化可能导致安全最佳实践失效。最后讨论了平衡个性化、安全性和实用性的设计和风险考虑。


<details>
  <summary>Details</summary>
Motivation: 复杂系统增加了用户制定访问控制决策的认知负担，导致次优决策。研究旨在利用LLM的推理能力自动生成符合用户偏好的动态访问控制决策，减少用户负担。

Method: 1. 通过用户研究收集307条自然语言隐私声明及用户对应做出的14,682条访问控制决策。2. 训练两种LLM：通用版（无用户个性化数据）和个性化版（加入用户隐私偏好）。3. 对比LLM决策与用户决策的一致性，并收集用户对1,446条个性化决策的反馈。4. 分析个性化决策与安全最佳实践的冲突。

Result: 1. LLM整体表现良好，通用版最高达86%准确率（与多数用户决策一致）。2. 个性化版虽提升个体用户决策一致性，但易违反安全最佳实践（例如过度许可高风险权限）。3. 用户反馈揭示了个性化与安全性的权衡矛盾。

Conclusion: 基于自然语言的访问控制系统需平衡三个维度：个性化（用户偏好）、安全性（最佳实践）、实用性（系统功能）。设计时应提供用户可控的透明度（如解释决策依据），并引入安全约束机制（如对关键权限的强制限制）。

Abstract: Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.
  Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.

</details>


### [11] [A Reality Check on SBOM-based Vulnerability Management: An Empirical Study and A Path Forward](https://arxiv.org/abs/2511.20313)
*Li Zhou,Marc Dacier,Charalambos Konstantinou*

Main category: cs.CR

TL;DR: 大规模实证研究验证了软件物料清单（SBOM）在软件供应链安全中的关键作用及现有问题，提出基于lock文件和函数调用分析的两阶段方案以提升准确性并减少漏洞扫描误报率。


<details>
  <summary>Details</summary>
Motivation: 现有SBOM因生成过程和应用漏洞扫描环节的准确性缺陷，导致其在实际软件供应链安全中的作用受限。

Method: 通过对2414个开源仓库的大规模实证分析，首先证实使用强包管理器的lock文件可生成高精度SBOM；进而发现下游漏洞扫描的误报率达97.5%，并通过函数调用分析消除63.3%的误报。

Result: 1) lock文件确保SBOM准确一致；2) 漏洞扫描中97.5%的误报主要由不可达代码触发；3) 函数调用分析可减少63.3%误报。

Conclusion: 建立两阶段安全方案：首阶段利用lock文件生成可靠SBOM；第二阶段通过函数调用分析生成低噪音漏洞报告，有效缓解开发者警报疲劳。

Abstract: The Software Bill of Materials (SBOM) is a critical tool for securing the software supply chain (SSC), but its practical utility is undermined by inaccuracies in both its generation and its application in vulnerability scanning. This paper presents a large-scale empirical study on 2,414 open-source repositories to address these issues from a practical standpoint. First, we demonstrate that using lock files with strong package managers enables the generation of accurate and consistent SBOMs, establishing a reliable foundation for security analysis. Using this high-fidelity foundation, however, we expose a more fundamental flaw in practice: downstream vulnerability scanners produce a staggering 97.5\% false positive rate. We pinpoint the primary cause as the flagging of vulnerabilities within unreachable code. We then demonstrate that function call analysis can effectively prune 63.3\% of these false alarms. Our work validates a practical, two-stage approach for SSC security: first, generate an accurate SBOM using lock files and strong package managers, and second, enrich it with function call analysis to produce actionable, low-noise vulnerability reports that alleviate developers' alert fatigue.

</details>
