<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [On-Premise SLMs vs. Commercial LLMs: Prompt Engineering and Incident Classification in SOCs and CSIRTs](https://arxiv.org/abs/2511.14908)
*Gefté Almeida,Marcio Pohlmann,Alex Severo,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.CR

TL;DR: 该研究评估了开源模型在安全事件分类中的表现，并与专有模型进行了比较。结果表明，尽管专有模型准确性更高，但本地部署的开源模型在隐私、成本效益和数据主权方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 评估和比较开源与专有模型在安全事件分类任务上的性能及优缺点，特别是在隐私、成本、数据主权等方面的权衡。

Method: 使用经NIST SP 800-61r3分类法标注的匿名真实事件数据集，并应用五种提示词优化技术（PHP, SHP, HTP, PRP, ZSL）进行模型比较。

Result: 专有模型在准确性上保持优势，但开源模型在本地部署时展现出隐私保护、成本效益和数据主权方面的显著优势。

Conclusion: 开源模型为安全事件分类提供了一种可行的替代方案，尤其适用于对数据隐私和主权要求较高的场景，尽管其准确性仍略逊于专有模型。

Abstract: In this study, we evaluate open-source models for security incident classification, comparing them with proprietary models. We utilize a dataset of anonymized real incidents, categorized according to the NIST SP 800-61r3 taxonomy and processed using five prompt-engineering techniques (PHP, SHP, HTP, PRP, and ZSL). The results indicate that, although proprietary models still exhibit higher accuracy, locally deployed open-source models provide advantages in privacy, cost-effectiveness, and data sovereignty.

</details>


### [2] [CIMemories: A Compositional Benchmark for Contextual Integrity of Persistent Memory in LLMs](https://arxiv.org/abs/2511.14937)
*Niloofar Mireshghallah,Neal Mangaokar,Narine Kokhlikyan,Arman Zharmagambetov,Manzil Zaheer,Saeed Mahloujifar,Kamalika Chaudhuri*

Main category: cs.CR

TL;DR: CIMemories基准测试显示，大型语言模型（LLMs）在处理记忆中的敏感信息时存在严重泄漏风险，在上下文不适当时泄露信息的比例高达69%，且提示工程无法解决此问题，需要上下文感知推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs使用持久记忆提升个性化和任务性能，但此功能存在敏感信息在不适当时刻泄露的风险，需评估模型对记忆信息流的控制能力。

Method: 创建合成用户档案（含100+属性），设计多样化任务上下文（每个属性在某些任务中必要，在另一些中可能不适当），评估LLMs在是否仅当上下文适当时才使用相关记忆属性。

Result: 前沿模型属性级违规率（不当泄露）最高达69%；任务数量增加（1→40个任务）时GPT-5违规率从0.1%升至9.6%；同一提示运行5次违规率达25.1%，且泄露属性不稳定；隐私提示无效，模型趋于全分享或全屏蔽。

Conclusion: LLMs对记忆中的敏感信息缺乏稳定的上下文控制能力，现有方法无法解决此问题，需开发具备上下文推理能力的新方案而非依赖提示改进或模型缩放。

Abstract: Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory introduces critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with over 100 attributes per user, paired with diverse task contexts in which each attribute may be essential for some tasks but inappropriate for others. Our evaluation reveals that frontier models exhibit up to 69% attribute-level violations (leaking information inappropriately), with lower violation rates often coming at the cost of task utility. Violations accumulate across both tasks and runs: as usage increases from 1 to 40 tasks, GPT-5's violations rise from 0.1% to 9.6%, reaching 25.1% when the same prompt is executed 5 times, revealing arbitrary and unstable behavior in which models leak different attributes for identical prompts. Privacy-conscious prompting does not solve this - models overgeneralize, sharing everything or nothing rather than making nuanced, context-dependent decisions. These findings reveal fundamental limitations that require contextually aware reasoning capabilities, not just better prompting or scaling.

</details>


### [3] [LFreeDA: Label-Free Drift Adaptation for Windows Malware Detection](https://arxiv.org/abs/2511.14963)
*Adrian Shuai Li,Elisa Bertino*

Main category: cs.CR

TL;DR: 论文提出了一种无需人工标签或偏移检测的自适应方法LFreeDA，通过联合训练有标签和无标签样本生成伪标签并过滤噪声，利用恶意软件图像和CFG表示来适应概念漂移，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的恶意软件检测器会随概念漂移而性能下降，重新训练需要高昂的手动标记成本。现有方法依赖于漂移检测和选择性标记，而完全无需标记的自适应方法尚未充分探索。作者认为无标签样本在适当整合后可提供有价值的信息。

Method: LFreeDA框架包含两个阶段：1) 在恶意软件图像上执行无监督域适应，联合训练有标签和无标签样本以推断伪标签并剪除噪声标签；2) 利用控制流图(CFG)表示，使用有标签数据及选定的伪标签数据调整分类器，结合图像的易扩展性和CFG的丰富语义。

Result: 在真实数据集MB-24+上，LFreeDA使准确率提升12.6%，F1值提升11.1%，仅比全监督上限低4%（准确率）和3.4%（F1）。在三种基准测试中，该方法在恶意软件演化时保持检测性能，且无需人工标注。

Conclusion: LFreeDA实现了端到端的无标签恶意软件检测器自适应，有效应对概念漂移问题。该方法利用无标签样本的潜在信息，结合两种特征表示的优势，性能接近有监督方法，优于传统无自适应方案。

Abstract: Machine learning (ML)-based malware detectors degrade over time as concept drift introduces new and evolving families unseen during training. Retraining is limited by the cost and time of manual labeling or sandbox analysis. Existing approaches mitigate this via drift detection and selective labeling, but fully label-free adaptation remains largely unexplored. Recent self-training methods use a previously trained model to generate pseudo-labels for unlabeled data and then train a new model on these labels. The unlabeled data are used only for inference and do not participate in training the earlier model. We argue that these unlabeled samples still carry valuable information that can be leveraged when incorporated appropriately into training. This paper introduces LFreeDA, an end-to-end framework that adapts malware classifiers to drift without manual labeling or drift detection. LFreeDA first performs unsupervised domain adaptation on malware images, jointly training on labeled and unlabeled samples to infer pseudo-labels and prune noisy ones. It then adapts a classifier on CFG representations using the labeled and selected pseudo-labeled data, leveraging the scalability of images for pseudo-labeling and the richer semantics of CFGs for final adaptation. Evaluations on the real-world MB-24+ dataset show that LFreeDA improves accuracy by up to 12.6% and F1 by 11.1% over no-adaptation lower bounds, and is only 4% and 3.4% below fully supervised upper bounds in accuracy and F1, respectively. It also matches the performance of state-of-the-art methods provided with ground truth labels for 300 target samples. Additional results on two controlled-drift benchmarks further confirm that LFreeDA maintains malware detection performance as malware evolves without human labeling.

</details>


### [4] [Towards Classifying Benign And Malicious Packages Using Machine Learning](https://arxiv.org/abs/2511.15033)
*Thanh-Cong Nguyen,Ngoc-Thanh Nguyen,Van-Giau Ung,Duc-Ly Vu*

Main category: cs.CR

TL;DR: 该论文提出了一种利用机器学习对开源包进行恶意行为自动分类的方法，通过动态分析提取特征（如执行命令），在npm包上评估显示AUC达0.91，误报率近0%。


<details>
  <summary>Details</summary>
Motivation: 当前恶意开源包数量激增，而现有安全扫描器主要关注已知CVE漏洞，针对恶意包的检测研究较少。动态分析虽能暴露运行时行为，但缺乏自动区分恶意与良性包的方法。

Method: 从动态分析中提取运行时特征（如执行命令），采用机器学习技术自动分类包的恶意性。对npm仓库近2000个包进行了测试验证。

Result: 机器学习分类器在测试中取得0.91的AUC值，同时保持接近0%的误报率。

Conclusion: 该方法能有效自动识别恶意开源包，显著优于现有工具，为软件供应链安全提供了新解决方案。

Abstract: Recently, the number of malicious open-source packages in package repositories has been increasing dramatically. While major security scanners focus on identifying known Common Vulnerabilities and Exposures (CVEs) in open-source packages, there are very few studies on detecting malicious packages. Malicious open-source package detection typically requires static, dynamic analysis, or both. Dynamic analysis is more effective as it can expose a package's behaviors at runtime. However, current dynamic analysis tools (e.g., ossf's package-analysis) lack an automatic method to differentiate malicious packages from benign packages. In this paper, we propose an approach to extract the features from dynamic analysis (e.g., executed commands) and leverage machine learning techniques to automatically classify packages as benign or malicious. Our evaluation of nearly 2000 packages on npm shows that the machine learning classifier achieves an AUC of 0.91 with a false positive rate of nearly 0%.

</details>


### [5] [Towards Practical Zero-Knowledge Proof for PSPACE](https://arxiv.org/abs/2511.15071)
*Ashwin Karthikeyan,Hengyu Liu,Kuldeep S. Meel,Ning Luo*

Main category: cs.CR

TL;DR: 该论文提出了首个实用的PSPACE完备语句的零知识证明协议，通过验证量化布尔公式（QBF）评估实现。核心思想是零知识验证量化解析证明（Q-Res），并设计了高效的多项式编码和用于证明获胜策略知识的协议。实验结果表明，协议可在100秒内验证72%的QBF评估和82%实例的获胜策略。


<details>
  <summary>Details</summary>
Motivation: 现有高效的零知识证明大多限于NP问题，而在PSPACE问题中存在实际需求。本文旨在填补在PSPACE完备问题上缺乏实用零知识证明的空白，尤其针对QBF这类重要问题的验证需求。

Method: 1. 提出将量化解析证明（Q-Res）在零知识条件下验证的方法；2. 设计高效的多项式编码技术，使证明验证转化为低开销的算术检查；3. 开发新协议用于证明QBF相关获胜策略（如博弈策略）的知识存在性。基于QBFEVAL标准库实现和评估原型系统。

Result: 在可获取Q-Res证明或策略的实例上：1. 72%的QBF评估可通过Q-Res证明在100秒内完成零知识验证；2. 82%的实例获胜策略可在同等时限内完成验证。实际测试中策略证明性能优于Q-Res证明。

Conclusion: 该工作首次实现了PSPACE完备问题的实用零知识证明，突破了传统NP问题限制。验证协议在标准测试集上表现出可行性，为复杂逻辑决策和安全协议设计提供了新的隐私保护工具。未来可优化证明规模并扩展至更多PSPACE问题。

Abstract: Efficient zero-knowledge proofs (ZKPs) have been restricted to NP statements so far, whereas they exist for all statements in PSPACE. This work presents the first practical zero-knowledge (ZK) protocols for PSPACE-complete statements by enabling ZK proofs of QBF (Quantified Boolean Formula) evaluation. The core idea is to validate quantified resolution proofs (Q-Res) in ZK. We develop an efficient polynomial encoding of Q-Res proofs, enabling proof validation through low-overhead arithmetic checks. We also design a ZK protocol to prove knowledge of a winning strategy related to the QBF, which is often equally important in practice. We implement our protocols and evaluate them on QBFEVAL. The results show that our protocols can verify 72% of QBF evaluations via Q-Res proof and 82% of instances' winning strategies within 100 seconds, for instances where such proofs or strategies can be obtained.

</details>


### [6] [Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments](https://arxiv.org/abs/2511.15165)
*Jingzhuo Zhou*

Main category: cs.CR

TL;DR: 提出了AdapT-Bench，一个用于评估多模态大语言模型（MLLMs）在学术环境中抵御动态钓鱼攻击能力的统一框架和基准套件。


<details>
  <summary>Details</summary>
Motivation: 应对学术机构面临的高度针对性钓鱼攻击威胁，现有安全基准由于缺乏学术背景信息而无法有效捕捉针对学术界的攻击模式。

Method: 开发了统一方法框架AdapT-Bench，包含动态、多语言和上下文相关的威胁情景，专门整合学术背景数据构建评估体系。

Result: 建立起针对学术界钓鱼攻击的评估体系（但摘要未具体说明实验结果）。

Conclusion: AdapT-Bench填补了现有安全基准在学术环境适应性方面的空白，为提升MLLMs防御动态钓鱼攻击能力提供评估基础。

Abstract: The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.

</details>


### [7] [How To Cook The Fragmented Rug Pull?](https://arxiv.org/abs/2511.15463)
*Minh Trung Tran,Nasrin Sohrabi,Zahir Tari,Qin Wang*

Main category: cs.CR

TL;DR: 该论文研究了加密货币领域的‘碎片化跑路欺诈’（FRP），描述了攻击者如何通过拆分大额交易并在多个地址间执行以降低可见性来耗尽管流动性。作者提出攻击行为的理论模型，涵盖动机、方法和实证验证结果。


<details>
  <summary>Details</summary>
Motivation: 当前跑路探测器依赖简单假设（即攻击者保留流动性池（LP）代币并在短期内大额抛售），但现实中存在通过分布式交易和多方参与的低可见性耗竭行为。

Method: 形式化FRP攻击方式：1) 保持LP代币控制以确保可提取流动性；2) 分薄交易量至多个小额抛售；3) 多地址分担销售操作（洗勺传递模型）。开发原子谓词组检测机制以识别分散攻击模式。

Result: 大规模实证检验：在303,614个LP中识别105,434个FRP池（占比34.7%），涉及3,420万次交易、40.2万个欺诈钱包和超150万独立地址。关键发现：1) 仅33.1%案例涉及官方地址抛售（证明碎片化趋势）；2) 同时识别出127,252个重复欺诈地址。

Conclusion: 传统检测方法忽略了碎片化攻击策略，FRP模型首次系统性描述了此类攻击并实证其广泛存在性（特别是官方脱钩现象）。实证工具与数据集为监管提供新检测维度。

Abstract: Existing rug pull detectors assume a simple workflow: the deployer keeps liquidity pool (LP) tokens and performs one or a few large sells (within a day) that collapse the pool and cash out. In practice, however, many real-world exits violate these assumptions by splitting the attack across both time and actor dimensions: attackers break total extraction into many low-impact trades and route proceeds through multiple non-owner addresses, producing low-visibility drains.
  We formalize this family of attacks as the fragmented rug pull (FRP) and offer a compact recipe for a slow-stewed beef special: (i) keep the lid on (to preserve LP control so on-chain extraction remains feasible), (ii) chop thin slices (to split the total exit volume into many low-impact micro-trades that individually fall below impact thresholds), and (iii) pass the ladle (to delegate sells across multiple wallets so that each participant takes a small share of the extraction). Technically, we define three atomic predicate groups and show that their orthogonal combinations yield evasive strategies overlooked by prior heuristics (USENIX Sec 19, USENIX Sec 23).
  We validate the model with large-scale measurements. Our corpus contains 303,614 LPs, among which 105,434 are labeled as FRP pools. The labeled subset includes 34,192,767 pool-related transactions and 401,838 inflated-seller wallets, involving 1,501,408 unique interacting addresses. Notably, owner-wallet participation in inflated selling among FRP-flagged LPs has declined substantially (33.1% of cases), indicating a shift in scam behavior: the liquidity drain is no longer held on the owner wallet. We also detected 127,252 wallets acting as serial scammers when repeatedly engaging in inflated selling across multiple FRP LPs. Our empirical findings demonstrate that the evasive strategies we define are widespread and operationally significant.

</details>


### [8] [Towards a Formal Verification of Secure Vehicle Software Updates](https://arxiv.org/abs/2511.15479)
*Martin Slind Hagen,Emil Lundqvist,Alex Phu,Yenan Wang,Kim Strandberg,Elad Michael Schiller*

Main category: cs.CR

TL;DR: 概述：研究针对软件定义汽车（SDVs）背景下统一软件更新框架（UniSUF）进行形式化安全验证。填补了先前未使用形式化方法验证的空白。


<details>
  <summary>Details</summary>
Motivation: 背景：软件定义汽车（SDVs）的软件漏洞可能严重影响安全、经济和社会，因此确保软件升级安全至关重要。前期提出的UniSUF框架需要经过系统化验证。

Method: 方法：建立UniSUF的架构和假设模型(贴合真实车载系统)，并开发基于ProVerif的形式化验证框架，通过符号执行验证框架所需的安全属性(保密性、完整性、真实性、新鲜性、顺序性和活性)。

Result: 结果：证明UniSUF满足所有指定的安全属性要求(保密性等六项)，验证了其安全框架的可靠性和正确性。

Conclusion: 结论：形式化验证表明UniSUF协议在既定模型下符合预设安全目标，其框架具备安全运行的理论基础。

Abstract: With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.
  Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.

</details>
