<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [AutoBackdoor: Automating Backdoor Attacks via LLM Agents](https://arxiv.org/abs/2511.16709)
*Yige Li,Zhe Li,Wei Zhao,Nay Myat Min,Hanxun Huang,Xingjun Ma,Jun Sun*

Main category: cs.CR

TL;DR: 本文提出AutoBackdoor框架，通过智能体驱动流程自动化后门攻击（包括触发词生成、数据毒化和模型微调），在多个现实威胁场景下验证高攻击成功率（超90%），并揭示现有防御对智能体驱动威胁的不足。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法依赖人工触发器和静态数据流程，难以系统评估现代防御鲁棒性，需建立更严格、多样化、可扩展的红队框架来模拟真实威胁。

Method: 利用大语言模型智能体自动生成语义连贯、上下文感知的触发词；构建自动化流程（触发器生成、数据毒化、微调）；在偏见推荐、幻觉注入、同行评审操控三大场景测试。

Result: 开源与商业模型（LLaMA-3/Mistral/Qwen/GPT-4o）上仅少量毒化样本即达成超90%攻击成功率；现有防御措施在此类攻击前普遍失效。

Conclusion: 智能体驱动的后门攻击威胁严重，现有防御不足，需开发更严格的自适应评估技术；开源代码/数据推动领域研究。

Abstract: Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \textit{Bias Recommendation}, \textit{Hallucination Injection}, and \textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.

</details>


### [2] [Password Strength Analysis Through Social Network Data Exposure: A Combined Approach Relying on Data Reconstruction and Generative Models](https://arxiv.org/abs/2511.16716)
*Maurizio Atzori,Eleonora Calò,Loredana Caruccio,Stefano Cirillo,Giuseppe Polese,Giandomenico Solimando*

Main category: cs.CR

TL;DR: SODA ADVANCE工具通过整合公开数据评估密码强度，并探索LLMs在生成和评估密码方面的潜力。实验表明LLMs能生成个性化强密码，并有效评估密码（尤其是结合用户资料时）。


<details>
  <summary>Details</summary>
Motivation: 用户常选择易记密码，增加安全风险，且传统密码强度评估方法不足。因此，开发工具增强密码强度评估过程，并研究新兴大语言模型（LLMs）在密码生成与评估中的能力与风险。

Method: 提出SODA ADVANCE工具，整合多源公开数据（含社交媒体）的专用模块评估密码强度。通过100名真实用户的实验，测试LLMs生成个性化强密码及评估密码（结合用户资料）的效果。

Result: 实验证明LLMs可生成根据用户定制的强密码，且在评估密码时（尤其考虑用户资料数据时）表现有效。

Conclusion: LLMs在密码生成和评估中具有潜力，结合用户资料能提升密码安全性，但需注意相关风险。SODA ADVANCE为密码强度评估提供了更全面的数据支持。

Abstract: Although passwords remain the primary defense against unauthorized access, users often tend to use passwords that are easy to remember. This behavior significantly increases security risks, also due to the fact that traditional password strength evaluation methods are often inadequate. In this discussion paper, we present SODA ADVANCE, a data reconstruction tool also designed to enhance evaluation processes related to the password strength. In particular, SODA ADVANCE integrates a specialized module aimed at evaluating password strength by leveraging publicly available data from multiple sources, including social media platforms. Moreover, we investigate the capabilities and risks associated with emerging Large Language Models (LLMs) in evaluating and generating passwords, respectively. Experimental assessments conducted with 100 real users demonstrate that LLMs can generate strong and personalized passwords possibly defined according to user profiles. Additionally, LLMs were shown to be effective in evaluating passwords, especially when they can take into account user profile data.

</details>


### [3] [Membership Inference Attacks Beyond Overfitting](https://arxiv.org/abs/2511.16792)
*Mona Khalil,Alberto Blanco-Justicia,Najeeb Jebreel,Josep Domingo-Ferrer*

Main category: cs.CR

TL;DR: 该论文研究发现，即使在未过拟合的机器学习模型中，部分训练数据样本（特别是类内异常值）仍易受成员推理攻击（MIA）的影响，并提出针对性的防御策略。


<details>
  <summary>Details</summary>
Motivation: 成员推理攻击（MIA）对机器学习模型隐私构成威胁，现有研究认为模型过拟合是主因，但未过拟合模型仍存在数据泄露问题。本文旨在探究过拟合之外的MIA漏洞根源，并设计精准防御方案。

Method: 作者实证分析了未过拟合模型中易受MIA攻击的训练样本特征，发现这些样本多为类内异常值（如噪声样本或难分类样本），进而提出保护此类样本的防御策略。

Result: 研究表明MIA漏洞不仅源于过拟合，类内异常值的存在也是关键因素；所提出的防御方法能有效增强模型隐私保护能力。

Conclusion: 机器学习模型的隐私保护需超越传统过拟合防御，应重点关注类内异常值样本的保护。论文提出的针对性防御为提升模型隐私性提供了新方向。

Abstract: Membership inference attacks (MIAs) against machine learning (ML) models aim to determine whether a given data point was part of the model training data. These attacks may pose significant privacy risks to individuals whose sensitive data were used for training, which motivates the use of defenses such as differential privacy, often at the cost of high accuracy losses. MIAs exploit the differences in the behavior of a model when making predictions on samples it has seen during training (members) versus those it has not seen (non-members). Several studies have pointed out that model overfitting is the major factor contributing to these differences in behavior and, consequently, to the success of MIAs. However, the literature also shows that even non-overfitted ML models can leak information about a small subset of their training data. In this paper, we investigate the root causes of membership inference vulnerabilities beyond traditional overfitting concerns and suggest targeted defenses. We empirically analyze the characteristics of the training data samples vulnerable to MIAs in models that are not overfitted (and hence able to generalize). Our findings reveal that these samples are often outliers within their classes (e.g., noisy or hard to classify). We then propose potential defensive strategies to protect these vulnerable samples and enhance the privacy-preserving capabilities of ML models. Our code is available at https://github.com/najeebjebreel/mia_analysis.

</details>


### [4] [TICAL: Trusted and Integrity-protected Compilation of AppLications](https://arxiv.org/abs/2511.17070)
*Robert Krahn,Nikson Kanti Paul,Franz Gregor,Do Le Quoc,Andrey Brito,André Martin,Christof Fetzer*

Main category: cs.CR

TL;DR: 提出了Tical框架，用于在构建流水线中保护源代码到最终可执行文件的完整性和机密性。


<details>
  <summary>Details</summary>
Motivation: 现有硬件增强方法保护运行时应用安全和内存安全，但忽略了构建时环节的机密性和完整性保护。恶意篡改构建过程可能危及整个应用和系统。

Method: 利用TEE增强构建环境，结合文件系统屏蔽技术和不可变审计日志确保工具链只能访问可信文件。

Result: 通过微观和宏观基准测试，验证了框架能在可接受的性能开销内保护CI/CD流水线。

Conclusion: Tical为构建流水线提供了信任保障，证明将TEE与防护措施结合可有效填补构建环节的安全缺失。

Abstract: During the past few years, we have witnessed various efforts to provide confidentiality and integrity for applications running in untrusted environments such as public clouds. In most of these approaches, hardware extensions such as Intel SGX, TDX, AMD SEV, etc., are leveraged to provide encryption and integrity protection on process or VM level. Although all of these approaches increase the trust in the application at runtime, an often overlooked aspect is the integrity and confidentiality protection at build time, which is equally important as maliciously injected code during compilation can compromise the entire application and system.In this paper, we present Tical, a practical framework for trusted compilation that provides integrity protection and confidentiality in build pipelines from source code to the final executable. Our approach harnesses TEEs as runtime protection but enriches TEEs with file system shielding and an immutable audit log with version history to provide accountability. This way, we can ensure that the compiler chain can only access trusted files and intermediate output, such as object files produced by trusted processes. Our evaluation using micro- and macro-benchmarks shows that Tical can protect the confidentiality and integrity of whole CI/CD pipelines with an acceptable performance overhead.

</details>


### [5] [Constant-Size Cryptographic Evidence Structures for Regulated AI Workflows](https://arxiv.org/abs/2511.17118)
*Leo Kao*

Main category: cs.CR

TL;DR: 本文将介绍一种用于受监管的人工智能工作流程中验证审计证据的恒定尺寸证据结构。它通过固定大小的加密字段元组提供强大的工作事件绑定、恒定的存储空间和统一的验证成本，并可与多种验证技术无缝集成。


<details>
  <summary>Details</summary>
Motivation: 在医疗AI等严监管场景中，现有记录机制存在可扩展性差、验证开销大和证据追溯问题。为此需要一种能够有效整合事件绑定、可组合验证以及恒定存储成本的统一证据抽象机制。

Method: 首先提出由固定尺寸加密元组组成的证据结构框架；其次建立基于哈希链和默克尔树的验证链模型；并开发结合碰撞抗哈希函数及数字签名算法的泛用建构原型。系统兼容可信执行环境。

Result: 理论分析显示事件证据验证具备恒定渐近复杂度（O(1)）；实验原型在商用硬件上的微基准测试显示每个事件处理耗时约0.8毫秒且可预测，比传统日志减少98%存储增长量。

Conclusion: 恒定尺寸证据结构能显著提升受监管AI工作流的审计效能，已在医疗和制药合规领域验证实用性。其设计支持与现存验证架构互操作，为AI治理提供标准基础设施。

Abstract: This paper introduces constant-size cryptographic evidence structures, a general abstraction for representing verifiable audit evidence for AI workflows in regulated environments. Each evidence item is a fixed-size tuple of cryptographic fields, designed to (i) provide strong binding to workflow events and configurations, (ii) support constant-size storage and uniform verification cost per event, and (iii) compose cleanly with hash-chain and Merkle-based audit constructions. We formalize a simple model of regulated AI workflows, define syntax and algorithms for evidence structures, and articulate security goals such as audit integrity and non-equivocation. We present a generic hash-and-sign construction that instantiates this abstraction using a collision-resistant hash function and a standard digital signature scheme. We then show how to integrate the construction with hash-chained logs, Merkle-tree anchoring, and optionally trusted execution environments, and we analyze the asymptotic complexity of evidence generation and verification. Finally, we implement a prototype library and report microbenchmark results on commodity hardware, demonstrating that the per-event overhead of constant-size evidence is small and predictable. The design is informed by industrial experience with regulated AI systems at Codebat Technologies Inc., while the paper focuses on the abstraction, algorithms, and their security and performance characteristics, with implications for clinical trial management, pharmaceutical compliance, and medical AI governance.

</details>


### [6] [Steering in the Shadows: Causal Amplification for Activation Space Attacks in Large Language Models](https://arxiv.org/abs/2511.17194)
*Zhiyuan Xu,Stanislav Abaimov,Joseph Gardiner,Sana Belguith*

Main category: cs.CR

TL;DR: 该论文揭示了解码器专用大语言模型(LLMs)中中间激活态的脆弱性，提出了一种名为敏感性缩放导向(SSS)的激活层攻击方法，通过在残差流的高增益区域施加小扰动，利用因果放大效应(CAE)实现对模型行为的控制。实验证明，该方法可在不影响模型通用能力的前提下，显著改变'邪恶倾向'、'幻觉'、'迎合度'和'情感倾向'等行为特征，对白盒及供应链部署的LLMs构成安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制主要依赖输入输出层面的审核和拒绝策略，但忽视了前向传播过程中间激活态的安全风险。本文研究发现，特定中间层的高敏感位置存在因果放大效应(CAE)，微小的扰动即可导致输出行为显著偏移，从而暴露出新的攻击面。

Method: 1) 发现残差流中存在高增益区域，其微小但精准的扰动会被自回归轨迹因果放大(CAE效应)；
2) 提出敏感性缩放导向(SSS)攻击：结合BOS锚定技术和基于敏感度的强化策略，将有限扰动聚焦于最脆弱的层和token；
3) 通过扰动敏感度评分确定最佳攻击位置，实现渐进式激活层操控。

Result: 在多个开源模型和四大行为轴向上验证：
- 诱导'邪恶属性'时，攻击成功率达79%；
- 诱导幻觉时逻辑一致性下降超40%；
- 情感倾向偏移系数达0.89；
- 模型核心能力保持率>92%；
- 所有案例中扰动规模<模型参数的0.007%。

Conclusion: 1) 首次揭示了LLM激活态作为对抗攻击面的严重安全漏洞；
2) 证明CAE效应可被武器化，仅需微量扰动即可实现强效行为控制；
3) 预警模型白盒访问场景及供应链渗透风险，建议安全框架需覆盖中间表示层。

Abstract: Modern large language models (LLMs) are typically secured by auditing data, prompts, and refusal policies, while treating the forward pass as an implementation detail. We show that intermediate activations in decoder-only LLMs form a vulnerable attack surface for behavioral control. Building on recent findings on attention sinks and compression valleys, we identify a high-gain region in the residual stream where small, well-aligned perturbations are causally amplified along the autoregressive trajectory--a Causal Amplification Effect (CAE). We exploit this as an attack surface via Sensitivity-Scaled Steering (SSS), a progressive activation-level attack that combines beginning-of-sequence (BOS) anchoring with sensitivity-based reinforcement to focus a limited perturbation budget on the most vulnerable layers and tokens. We show that across multiple open-weight models and four behavioral axes, SSS induces large shifts in evil, hallucination, sycophancy, and sentiment while preserving high coherence and general capabilities, turning activation steering into a concrete security concern for white-box and supply-chain LLM deployments.

</details>


### [7] [ThreadFuzzer: Fuzzing Framework for Thread Protocol](https://arxiv.org/abs/2511.17283)
*Ilja Siroš,Jakob Heirwegh,Dave Singelée,Bart Preneel*

Main category: cs.CR

TL;DR: ThreadFuzzer：首个面向Thread协议实现的模糊测试框架，通过操纵MLE层数据包支持虚拟和物理设备测试，集成多种模糊策略并发现OpenThread中的五个未知漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着IoT的快速发展，Thread协议已成为智能家居和商业系统的核心传输层，但其安全性缺乏系统化测试，亟需专用工具来评估实现漏洞。

Method: 在MLE层操纵数据包实现跨设备测试；融合随机/基于覆盖率（CovFuzz）的模糊器及定制TLV插入器；评估指标包括代码覆盖率和漏洞发现能力。

Result: 发现OpenThread栈中五个未知漏洞（部分在商用设备复现）；在对比AFL++基准测试中展现高效性。

Conclusion: ThreadFuzzer有效填补无线协议模糊测试空白，实证结果凸显其在安全评估中的实用性，同时揭示该研究领域的挑战与未来方向。

Abstract: With the rapid growth of IoT, secure and efficient mesh networking has become essential. Thread has emerged as a key protocol, widely used in smart-home and commercial systems, and serving as a core transport layer in the Matter standard. This paper presents ThreadFuzzer, the first dedicated fuzzing framework for systematically testing Thread protocol implementations. By manipulating packets at the MLE layer, ThreadFuzzer enables fuzzing of both virtual OpenThread nodes and physical Thread devices. The framework incorporates multiple fuzzing strategies, including Random and Coverage-based fuzzers from CovFuzz, as well as a newly introduced TLV Inserter, designed specifically for TLV-structured MLE messages. These strategies are evaluated on the OpenThread stack using code-coverage and vulnerability-discovery metrics. The evaluation uncovered five previously unknown vulnerabilities in the OpenThread stack, several of which were successfully reproduced on commercial devices that rely on OpenThread. Moreover, ThreadFuzzer was benchmarked against an oracle AFL++ setup using the manually extended OSS-Fuzz harness from OpenThread, demonstrating strong effectiveness. These results demonstrate the practical utility of ThreadFuzzer while highlighting challenges and future directions in the wireless protocol fuzzing research space.

</details>


### [8] [A Patient-Centric Blockchain Framework for Secure Electronic Health Record Management: Decoupling Data Storage from Access Control](https://arxiv.org/abs/2511.17464)
*Tanzim Hossain Romel,Kawshik Kumar Paul,Tanberul Islam Ruhan,Maisha Rahman Mim,Abu Sayed Md. Latiful Hoque*

Main category: cs.CR

TL;DR: 患者为中心的电子健康记录共享架构，采用链下加密存储与区块链记录结合，实现安全与隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有系统在电子健康记录共享中存在隐私泄露风险，患者缺乏数据控制权。

Method: 分离存储与授权审计；加密资源存储在链下，区块链仅记录加密承诺和时限权限；使用公钥包装分发密钥。

Result: 平均权限授予成本78,000 gas(L1)；1MB记录端到端访问延迟0.7-1.4s；L2部署降低10-13倍gas消耗。

Conclusion: 架构在保护隐私和满足监管的前提下，实现了患者对敏感临床数据的控制权。

Abstract: We present a patient-centric architecture for electronic health record (EHR) sharing that separates content storage from authorization and audit. Encrypted FHIR resources are stored off-chain; a public blockchain records only cryptographic commitments and patient-signed, time-bounded permissions using EIP-712. Keys are distributed via public-key wrapping, enabling storage providers to remain honest-but-curious without risking confidentiality. We formalize security goals (confidentiality, integrity, cryptographically attributable authorization, and auditability of authorization events) and provide a Solidity reference implementation deployed as single-patient contracts. On-chain costs for permission grants average 78,000 gas (L1), and end-to-end access latency for 1 MB records is 0.7--1.4s (mean values for S3 and IPFS respectively), dominated by storage retrieval. Layer-2 deployment reduces gas usage by 10--13x, though data availability charges dominate actual costs. We discuss metadata privacy, key registry requirements, and regulatory considerations (HIPAA/GDPR), demonstrating a practical route to restoring patient control while preserving security properties required for sensitive clinical data.

</details>
