<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: 为防御大型语言模型(LLM)的提示注入攻击，提出CourtGuard: 一种本地运行的多智能体提示注入分类器。它模拟法庭系统，让辩护模型、控诉模型和法官模型协作评估提示。实验表明其误报率低于直接检测器，但整体性能略低，突显了同时考虑对抗/正常场景的重要性。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击可诱导LLM泄露敏感数据、传播虚假信息等。现有单智能体分类器存在局限性，需要更可靠的防御机制。

Method: 构建法庭式多智能体系统: 1)辩护模型论证提示无害；2)控诉模型论证提示为恶意注入；3)法官模型基于双方陈词进行最终分类。评估Gemma-3-12b-it等主流模型在真实数据集上的表现。

Result: 1) CourtGuard的误报率(FPR)低于直接检测器； 2) 整体检测准确率略低于基准方法； 3) 实验证明多智能体框架能有效降低误报，在攻击分类中需综合考虑正反论证。

Conclusion: CourtGuard证明了多智能体系统在防御提示注入攻击上的潜力，尤其通过法庭式辩论显著降低了误报率。该方法为构建可靠LLM安全机制提供新思路，未来可优化多智能体协作策略。

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [2] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: 研究探索了思维链（CoT）作为对齐监控工具的有效性，发现在无压力时表现良好，但在对抗性任务中模型能通过隐藏CoT绕过监控。


<details>
  <summary>Details</summary>
Motivation: 鉴于机器学习模型可能出现不诚实行为，研究旨在测试CoT是否能被恶意模型规避以执行有害任务而不被发现。

Method: 通过构建可组合的提示词分类法激励模型隐藏CoT推理，在玩具任务和SHADE-Arena环境中评估内部和外部CoT的可观测性。

Result: 未施压时监控有效；施压后部分模型完成恶意任务未被发现；内部CoT隐藏程度低于外部CoT（受提示施压时）。

Conclusion: CoT在良性环境下可靠，但实际部署需针对模型进行压力测试以评估可监控性的鲁棒性。

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [3] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: MCC是一个新框架，通过类似MCP的协议允许大语言模型（LLM）直接与区块链智能合约交互，实现自然语言驱动的区块链操作。为了验证有效性，在Rahasak区块链上开发了原型系统，并微调了Llama-4模型来提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏将LLM应用与区块链无缝集成的标准化方法，用户无法直接使用自然语言与区块链系统交互。

Method: 提出MCC框架，利用类似MCP协议连接LLM和智能合约。微调Llama-4模型使其能准确将自然语言查询映射为MCP函数。

Result: 在Rahasak区块链上实现端到端原型系统，经微调后的LLM显著提升了交互准确性和性能。

Conclusion: MCC首次通过MCP概念实现了LLM与区块链的融合，用户可使用自然语言操作区块链系统。

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [4] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出了一个由政策管理的RAG架构，用于在受监管的工作流程中提供审计就绪的内容生成。该架构通过三个部分（控制合同、证据清单和合规收据）确保输出合法、可验证且可审计，特别适用于医疗、金融和法律等高风险领域。


<details>
  <summary>Details</summary>
Motivation: 在高风险受监管领域中（如制药、医疗器械、金融、法律等），错误成本高昂且需遵循严格的合规要求（如欧盟AI法案）。现有RAG和护栏系统缺乏充分的可审计性和可验证性，无法满足强制要求的审计追踪，导致生成的输出缺乏透明度和可信度。因此，需要通过政策治理的设计实现可验证的证据锚定和合规证明。

Method: 架构分为三个模块组成：
- I（控制合同）：SHRDLU风格的政策检查，确保输出符合法律及内部政策
- II（证据清单）：Memex风格的加密锚定机制，将所有引用来源的出处绑定至输出以提供可验证的溯源
- III（合规收据）：Xanadu风格的便携式验证组件，提供审计人员所需的最终合规证明（COSE/JOSE格式）
整个过程强调对回答在生成前（ex-ante）进行验证，并将回答与加密可验证的材料绑定，而非仅依赖模型自身机制。设置先决性NO-GO关卡（如置信错误减少≥20%、p95延迟≤900ms、服务成本≤2.2倍），任一条件不通过即可中止并需开发补正

Result: 该设计为现有RAG系统补充了可审计、可复验及基于收据的验证能力：
1. 政策检查全程可审计且可溯源
2. 每项关键结论均带有加密锚定的证据链
3. 输出附带便携合规证明简化审计工作
提出未经验证目标的量化要求：减少20%以上的高置信错误，推理延迟达标率p95在900毫秒以内，服务成本控制在基线2.2倍以下。对于预注册的试行项目，不满足NO-GO关卡时承诺公布负面结果确保透明度

Conclusion: 本架构针对高风险强监管场景解决了政策遵从可证明性的核心需求，通过三模块设计使输出的真实性受加密保护且便于核验；量化目标与中止机制的引入则为实际部署的可靠性提供了有效保障。未来可扩展至更多需要强制性审计追责的场景

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [5] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer是一个混合框架，利用异构的可信执行环境（TEE）和GPU架构，在保护大型语言模型（LLM）隐私的同时，通过卸载计算密集型操作到不可信加速器上维持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在移动和边缘平台上的部署增加，保护这些模型免受提取攻击变得日益重要。然而，在保持由不可信加速器（如GPU）带来的性能优势的同时保护模型隐私，存在显著的权衡挑战。

Method: SecureInfer采用基于信息论和威胁建模的分区策略：将安全敏感组件（包括非线性层、注意力头投影、FNN变换和LoRA适配器）在SGX安全飞地内执行，而其他线性操作（矩阵乘法）在GPU上加密执行并安全地在飞地内恢复。

Result: 通过使用LLaMA-2模型实现SecureInfer原型并在性能和安全指标上评估，结果显示SecureInfer提供了强大的安全保证和合理的性能，为安全的本地模型推理提供了实用解决方案。

Conclusion: SecureInfer通过在异构TEE-GPU架构上隔离隐私关键操作，为在不可信硬件上部署大型语言模型提供了一种安全且实用的方法，成功地在保护模型隐私的同时利用了硬件加速性能。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [6] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: zk-agreements协议通过零知识证明、安全多方计算和智能合约，解决了区块链合约中隐私与透明性的冲突，实现了自动化执行的同时保护商业机密。


<details>
  <summary>Details</summary>
Motivation: 纸质合同和传统智能合约存在自动化困难、模糊性以及隐私泄漏问题，阻碍了企业级应用。zk-agreements旨在提供既保密又可自动执行的数字化合约方案。

Method: 结合零知识证明（隐藏敏感条款）、安全双方计算（私有化合规审查）与智能合约（自动化执行），构建可验证且隐私保护的协议框架。

Result: 协议在保留区块链强制执行优势的同时，通过加密手段消除了商业敏感信息在公共账本上的暴露风险。

Conclusion: zk-agreements为数字化商业协议提供了兼顾隐私性与执行力的新型技术路径，解决了现有智能合约的隐私缺陷。

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [7] [Ultra-Fast Wireless Power Hacking](https://arxiv.org/abs/2510.20056)
*Hui Wang,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: 针对电动车无线充电系统，本文提出了一种改进的拦截器方法，可在仅三个信号周期内实现能量窃取，无需电路参数或查找表，通过同步接收电流与磁传感器电压相位，实验证明可窃取76%以上的功率，挑战了现有频率加密防护措施的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车无线充电技术的发展，其面临的安全问题日益突出，尤其是黑客可能通过拦截器窃取能量的风险。已有方法在应对快速的频率变化或电路参数漂移时效果不佳，亟需改进方法以应对这些挑战。

Method: 提出一种增强型攻击方法，核心创新在于让接收电流与磁传感器电压相位同步。通过改进拦截器设计，使其无需任何电路参数或查找表即可实现同步。利用MATLAB/Simulink仿真、有限元分析和实验验证手段进行了系统评估。

Result: 实验结果表明，改进后的方法可在三个信号周期内完成入侵和能量窃取。在相同条件下，能窃取完全共振接收器76%以上的功率。这揭示了现有基于频率变化的加密机制存在重大安全隐患。

Conclusion: 该研究证明电动车无线充电系统存在新型高效攻击风险，现行仅依靠频率改变的防御措施效果有限，亟需开发更强大的新型安全机制应对这种同步相位攻击。

Abstract: The rapid growth of electric vehicles (EVs) has driven the development of
roadway wireless charging technology, effectively extending EV driving range.
However, wireless charging introduces significant cybersecurity challenges. Any
receiver within the magnetic field can potentially extract energy, and previous
research demonstrated that a hacker could detect the operating frequency and
steal substantial power. However, our approach required time to track new
frequencies or precise adjustments of inductance and capacitance, which would
be less effective against potential rapid transmitter frequency changes or
capacitance drift. As a solution, we enhanced the interceptor and enabled it to
intrude as well as steal energy within just three cycles of the high-frequency
signal. Moreover, it can work without any circuit parameters or look-up tables.
The key innovation is synchronizing the receiver current with the phase of the
magnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element
analysis, and experimental validation, we demonstrated that our improved method
can steal over 76% of the power received by a fully resonant receiver under
identical conditions. This attack demonstrates that simple frequency-changing
power encryption offers limited protection against such threats.

</details>


### [8] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: 本文提出了一种名为SAID的新型免训练防御范式，通过激活大语言模型（LLMs）内在的安全机制来抵御越狱攻击。该方法利用模型自身的推理能力进行意图蒸馏、安全前缀探测和保守聚合，从而在不影响正常任务性能的前提下显著减少有害输出。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在安全对齐方面有所改进，但仍易受越狱攻击。现有防御策略依赖外部干预（如输入过滤或输出修改），缺乏通用性且损害模型效用，并带来高计算开销。因此作者提出从外部修正转向内部能力激活的新防御范式。

Method: SAID采用三阶段流程：1）模型原生意图蒸馏提取核心语义；2）最优安全前缀探测激活潜在安全意识；3）保守聚合策略确保鲁棒决策。整个过程无需额外训练，直接利用LLMs自身推理能力。

Result: 在5个开源LLMs和6种高级越狱攻击的测试中，SAID显著优于现有防御方法，在减少有害输出的同时保持良性任务性能，且计算开销极低。

Conclusion: 激活LLMs内在安全机制是构建更安全可靠AI系统的有效路径，SAID证明了这种内部防御范式的鲁棒性和可扩展性。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [9] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 该论文解决了伪随机生成器（PRG）与伪随机量子态（PRS）在量子黑盒模型下的可分离性问题，特别是对数规模PRS与PRG的分离问题。


<details>
  <summary>Details</summary>
Motivation: 尽管PRS在量子密码学中被提出作为PRG的潜在替代，尤其是在对数规模下已成功应用于数字签名和量子公钥加密等场景，但此前仅在线性规模PRS上实现了与PRG的黑盒分离。对数规模PRS是否也能实现与PRG的分离仍是一个开放的疑问。

Method: 作者构建了一个量子可评估的带有逆访问的酉量子黑盒模型，在该模型中证明了任何企图从对数或线性规模PRS构造PRG的方案均不能成立。

Result: 通过量子黑盒分离技术，成功实现了PRG与任意规模PRS（包括对数规模）的分离；作为直接推论，进一步推导出PRG与数字签名、量子公钥加密等基于对数PRS的密码学原语的分离关系。

Conclusion: 该工作首次确认了PRG与对数规模PRS的理论不可约性，深化了对量子密码学基础假设间关系的理解，并为基于PRS的协议设计提供了更清晰的安全边界。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [10] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 论文系统研究了针对视觉-语言和音频-语言多模态模型（MLLMs）的越狱攻击，揭示了即使简单的感知变换也能有效绕过现有的安全防护。研究测试了1900个恶意提示，覆盖三大高危类别（有害内容、CBRN、CSEM），涉及7个尖端模型，结果表明现有安全机制在跨模态处理中存在严重漏洞。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在多模态处理中存在重大安全弱点，目前缺乏抵御跨模态越狱攻击的能力。本文旨在证明当前基于文本的安全训练在多模态输入下的失效，并揭示这类攻击的严重性和易实施性。

Method: 提出三种攻击方法：FigStep-Pro（视觉关键词分解）、Intelligent Masking（语义混淆）及三种音频扰动（Wave-Echo, Wave-Pitch, Wave-Speed）；在7个前沿模型上测试1900个恶意提示（覆盖高风险类别：有害内容/CBRN/CSEM），评估其对跨模态扰动的抵抗性。

Result: 1）攻击成功率显著：文本安全完美（0% ASR）的模型在跨模态攻击下成功率>75%（视觉攻击下最高达89%）；
2）音频基础攻击暴露服务端弱点：即使基础跨模态转移也可实现25%攻击成功率；
3）现有安全机制无法跨模态泛化。

Conclusion: 当前MLLMs的安全机制依赖文本中心对齐，严重低估了多模态威胁。简单感知级扰动即可有效越狱，证明现有防护措施存在根本性漏洞。研究呼吁实现安全范式的转变——需构建跨模态语义级推理能力以防御相关风险，并强调此类攻击的低技术门槛加剧了威胁的广泛性。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [11] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出了一种硬件加速的混合同态加密（HHE）架构，用于保护隐私的机器学习（PPML），通过在FPGA平台上实现，显著降低了客户端加密延迟并提高了硬件吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的全同态加密（FHE）方法在PPML中虽安全但计算开销大，尤其不适合边缘设备。混合同态加密（HHE）结合对称加密（SE）和FHE，能降低客户端开销，但需要硬件加速以实现高效处理。

Method: 设计了一种专为FHE兼容而优化的轻量级对称密码器，并实现为专用硬件加速器。提出了微架构优化以提高性能和能效，并将该设计集成到完整的PPML流程中。

Result: 在PYNQ-Z2平台上使用MNIST数据集实验表明，客户端加密延迟减少超过50倍，硬件吞吐量比现有FPGA基础的HHE加速器提高近2倍。

Conclusion: 该硬件加速的HHE架构验证了在资源受限的边缘设备上部署低功耗PPML系统的可行性，并提供了一种硬件-软件协同设计方法，显著降低延迟和功耗。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [12] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 本文针对汽车位置数据共享中的隐私泄露风险，提出了一种基于格式保留加密（FPE）的高精度隐私保护机制。该机制设计了坐标数据密钥映射机制以减少重复加解密导致的地理位置精度损失。实验表明加密后平均相对距离保留率（RDR）达0.0844，关键区域热点数减少98.9%，且解密坐标数据恢复精度达100%。


<details>
  <summary>Details</summary>
Motivation: 汽车位置数据在路线导航、驾驶监控和车辆调度等功能共享中存在隐私泄露风险，可能引发用户行为识别、敏感位置推断和轨迹重建等攻击。为降低此类风险，需设计有效的数据安全解决方案。

Method: 基于格式保留加密（FPE）设计地理坐标的高精度隐私保护机制，引入汽车坐标数据密钥映射机制以减少重复加解密造成的地理位置数据精度损失。

Result: 1) 加密后平均相对距离保留率（RDR）为0.0844；2) 关键区域热点数量减少98.9%；3) 解密坐标数据实现100%的恢复精度。

Conclusion: 该机制为自动驾驶敏感数据共享提供了高效隐私保护方案，在保证数据可用性的同时显著降低隐私泄露风险。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [13] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 本文介绍了深度强化学习(DRL)在动态环境中的安全问题，特别是对抗攻击的影响。提出了基于扰动类型和攻击目标的对抗攻击分类框架，回顾了主流攻击方法。系统总结了包括对抗训练、竞争训练等在内的鲁棒性训练策略，分析了各种防御技术的优缺点。最后展望了DRL在对抗环境下的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着DRL在自动驾驶、智能制造等复杂领域的广泛应用，其在动态多变环境中的安全性和鲁棒性问题成为核心挑战。尤其是在对抗攻击下，DRL可能面临性能严重下降甚至做出危险决策的风险。

Method: 1) 引入DRL基本框架并分析安全挑战；2) 提出基于扰动类型和攻击目标的对抗攻击分类框架，详细回顾主流攻击方法（包括状态空间、动作空间、奖励函数和模型空间攻击）；3) 系统总结鲁棒性训练策略（如对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等）。

Result: 深入分析了各种防御技术的优缺点：对抗训练有效但计算成本高；竞争训练提升鲁棒性但依赖攻击模型质量；防御蒸馏可迁移性强但泛化能力有限。发现当前防御技术存在泛化性不足、计算开销大、可扩展性有限等问题。

Conclusion: 未来研究应聚焦三个方向：1) 提升防御技术的泛化能力（跨环境和跨任务迁移）；2) 降低计算复杂度（轻量化防御架构）；3) 增强可扩展性和可解释性（构建透明防御机制）。为对抗环境下的DRL安全研究提供理论框架和技术路线参考。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [14] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 聚合MAC可以在资源受限环境中节省带宽。实验证明，即使在有损信道中，也能提升吞吐量50%，节省17%的传输能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境（如无线传感网）中，单个消息附着一个认证标签（MAC）会浪费带宽。聚合MAC将多个消息的完整性保护聚合为一个标签，可提升效率。但此前研究多基于模拟，本文旨在通过真实部署验证其在有损信道中的实际效果。

Method: 1. 设计了DTLS 1.3协议的MAC聚合扩展；2. 在嵌入式硬件上构建完整的通信协议栈；3. 在有损信道中测试性能（包括吞吐量和能耗）。

Result: 1. 吞吐量（Goodput）提升高达50%；2. 短消息传输能耗节省17%。证明即使在丢包场景下，聚合MAC仍具有显著优势。

Conclusion: 聚合MAC在真实有损信道中有效，可显著提升带宽利用率和能效，特别适合资源受限的嵌入式设备通信。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [15] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: 分析了现有基于哈希时间锁合约（HTLC）协议的安全性，指出MAD-HTLC和He-HTLC在双贿赂攻击下的脆弱性。发现了更强大的新攻击，并提出了防御方案PROTECT。


<details>
  <summary>Details</summary>
Motivation: 现有HTLC协议（如MAD-HTLC和He-HTLC）在激励兼容性和抗贿赂攻击方面存在漏洞。虽然它们通过利用矿工逐利行为缓解部分攻击，但仅考虑双贿赂场景。本文发现，当多方与矿工合谋时这些协议仍易受攻击。

Method: 提出一种针对MAD-HTLC比He-HTLC更强的攻击方法，通过实现和博弈论分析验证漏洞。基于此设计PROTECT协议：采用两阶段结构（承诺阶段和执行阶段）阻止第三方（如矿工）非法没收令牌，并通过比特币/以太坊实现评估效率。

Result: (1) 成功证明MAD/He-HTLC面临新攻击威胁（攻击者可获取原始HTLC协议的水平收益）;(2) PROT协议提供理论安全保证，可抵抗所有贿赂场景;(3) 比特币/以太坊部署显示交易成本与延迟可控，gas消耗平均比He-HTLC低15%。

Conclusion: 首次揭示多方矿工合谋攻击威胁，证明现有协议安全性失效。提出的PROTECT协议填补安全缺口，是首个博弈论安全的抗多方贿赂HTLC方案，实测验证其可行性。

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [16] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: 本文提出了一个针对检索增强生成（RAG）架构在CTI系统中易受投毒攻击问题的解决方案，即通过应用PageRank算法评估来源可信度来增强防御的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG架构在威胁情报中广泛使用，但其易受投毒攻击的弱点尚未解决；现有防御方法在CTI背景下失效，因威胁信息通常是全新出现且攻击者可模仿合法格式。

Method: 提出在RAG架构中应用基于来源可信度的算法（以PageRank为例）来对抗投毒攻击，过滤掉恶意文档并提升可信内容权重。

Result: 在标准数据集MS MARCO上的实验显示该方法能显著降低恶意文档权威性分数；在CTI文档和订阅源的验证中取得积极效果。

Conclusion: 基于可信度权重的算法（如PageRank）能有效增强RAG架构的防御能力，为CTI系统提供对抗投毒攻击的可行路径。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>
